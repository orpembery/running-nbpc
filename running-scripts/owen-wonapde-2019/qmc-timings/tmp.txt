---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500 Tue Jan 15 14:18:33 2019
run-qmc.py on a , 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-qmc.py on a  named mapc-4034 with 1 processor, by orp20 Tue Jan 15 14:18:39 2019
Using Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500

                         Max       Max/Min     Avg       Total 
Time (sec):           6.351e+00     1.000   6.351e+00
Objects:              1.018e+03     1.000   1.018e+03
Flop:                 3.804e+09     1.000   3.804e+09  3.804e+09
Flop/sec:             5.989e+08     1.000   5.989e+08  5.989e+08
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 1.4796e+00  23.3%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 
 1: HelmholtzProblem solve: 4.8713e+00  76.7%  3.8038e+09 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSidedF         1 1.0 7.7009e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             1 1.0 3.8147e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                37 1.0 6.9380e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      67 1.0 9.6083e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd        67 1.0 2.2945e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexInterp           1 1.0 4.4651e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexStratify         2 1.0 2.7230e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateMesh             4 1.0 4.7953e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateFunctionSpace       1 1.0 3.3676e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   2  0  0  0  0     0
Mesh: reorder          1 1.0 4.8614e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        1 1.0 2.6910e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateSparsity         1 1.0 1.9720e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroInitial        66 1.0 4.4892e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   3  0  0  0  0     0

--- Event Stage 1: HelmholtzProblem solve

VecMDot              213 1.0 5.9757e-03 1.0 1.44e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2416
VecNorm              245 1.0 1.6043e-03 1.0 4.15e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2585
VecScale             245 1.0 5.3072e-04 1.0 2.07e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3907
VecCopy              224 1.0 4.1580e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               586 1.0 8.8954e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               64 1.0 2.3389e-04 1.0 1.08e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  4632
VecMAXPY             245 1.0 7.5839e-03 1.0 1.80e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2379
VecNormalize         245 1.0 2.2078e-03 1.0 6.22e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2817
MatMult              213 1.0 1.2610e-02 1.0 2.28e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  1809
MatSolve             245 1.0 8.6346e-02 1.0 1.69e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  4  0  0  0   2  4  0  0  0  1957
MatLUFactorSym        32 1.0 3.2117e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatLUFactorNum        32 1.0 2.0573e-01 1.0 3.80e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 10  0  0  0   4 10  0  0  0  1848
MatAssemblyBegin     128 1.0 7.1526e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd       128 1.0 2.1183e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ           32 1.0 1.2040e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering        32 1.0 1.2486e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        64 1.0 4.9663e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp               32 1.0 2.5103e-01 1.0 3.80e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4 10  0  0  0   5 10  0  0  0  1515
PCApply              245 1.0 8.6403e-02 1.0 1.69e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  4  0  0  0   2  4  0  0  0  1956
KSPSetUp              32 1.0 6.7902e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              32 1.0 3.6864e-01 1.0 6.11e+08 1.0 0.0e+00 0.0e+00 0.0e+00  6 16  0  0  0   8 16  0  0  0  1658
KSPGMRESOrthog       213 1.0 1.2175e-02 1.0 2.89e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  2372
SNESSolve             32 1.0 4.8678e+00 1.0 3.80e+09 1.0 0.0e+00 0.0e+00 0.0e+00 77100  0  0  0 100100  0  0  0   781
SNESFunctionEval      32 1.0 1.4637e+00 1.0 8.21e+08 1.0 0.0e+00 0.0e+00 0.0e+00 23 22  0  0  0  30 22  0  0  0   561
SNESJacobianEval      32 1.0 3.0348e+00 1.0 2.37e+09 1.0 0.0e+00 0.0e+00 0.0e+00 48 62  0  0  0  62 62  0  0  0   781
ParLoopExecute       224 1.0 4.4576e+00 1.0 3.19e+09 1.0 0.0e+00 0.0e+00 0.0e+00 70 84  0  0  0  92 84  0  0  0   716
ParLoopset_1          64 1.0 1.2014e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednBegin     224 1.0 5.5671e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd       224 1.0 2.4700e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells         192 1.0 4.4200e+00 1.0 3.19e+09 1.0 0.0e+00 0.0e+00 0.0e+00 70 84  0  0  0  91 84  0  0  0   721
ParLoopExtFacets     192 1.0 4.8671e-03 1.0 6.98e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1434
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
           Index Set    37            133      1166284     0.
   IS L to G Mapping     1              1         9144     0.
             Section    17             17        12376     0.
   Star Forest Graph    18             18        14816     0.
              Vector    70            562     18845264     0.
              Matrix    69            101     78038336     0.
      Preconditioner    33             33        33264     0.
       Krylov Solver    33             33      1121832     0.
     DMKSP interface     0              1          664     0.
                SNES    33             33        45672     0.
              DMSNES     1              1          680     0.
      SNESLineSearch    33             33        33000     0.
    Distributed Mesh     9              9        44160     0.
    GraphPartitioner     2              2         1240     0.
     Discrete System     9              9         8428     0.

--- Event Stage 1: HelmholtzProblem solve

           Index Set    96              0            0     0.
              Vector   523             31      1097400     0.
              Matrix    32              0            0     0.
     DMKSP interface     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --with-fortran-bindings=0 --download-chaco --download-metis --download-exodusii --with-zlib --with-scalar-type=complex --download-netcdf --download-mumps --download-parmetis --download-pnetcdf --download-scalapack --download-hdf5
-----------------------------------------
Libraries compiled on 2018-11-08 14:42:32 on mapc-4034 
Machine characteristics: Linux-4.4.0-138-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/openmpi/lib -L/usr/lib/openmpi/lib -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lX11 -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Tue Jan 15 14:18:39 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500 Tue Jan 15 14:19:20 2019
run-qmc.py on a , 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-qmc.py on a  named mapc-4034 with 1 processor, by orp20 Tue Jan 15 14:19:31 2019
Using Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500

                         Max       Max/Min     Avg       Total 
Time (sec):           1.152e+01     1.000   1.152e+01
Objects:              1.924e+03     1.000   1.924e+03
Flop:                 7.545e+09     1.000   7.545e+09  7.545e+09
Flop/sec:             6.550e+08     1.000   6.550e+08  6.550e+08
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 1.8780e+00  16.3%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 
 1: HelmholtzProblem solve: 9.6414e+00  83.7%  7.5451e+09 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSidedF         1 1.0 7.4148e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             1 1.0 4.0531e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                69 1.0 1.1539e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin     131 1.0 9.7036e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd       131 1.0 4.5340e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexInterp           1 1.0 4.4689e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexStratify         2 1.0 2.7280e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateMesh             4 1.0 4.7889e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateFunctionSpace       1 1.0 3.2645e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   2  0  0  0  0     0
Mesh: reorder          1 1.0 4.8995e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        1 1.0 2.6951e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateSparsity         1 1.0 1.9541e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroInitial       130 1.0 8.8026e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   5  0  0  0  0     0

--- Event Stage 1: HelmholtzProblem solve

VecMDot              367 1.0 8.8716e-03 1.0 2.24e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2526
VecNorm              431 1.0 2.6989e-03 1.0 7.29e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2703
VecScale             431 1.0 8.4686e-04 1.0 3.65e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  4308
VecCopy              448 1.0 6.9571e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1172 1.0 1.5073e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              128 1.0 4.0197e-04 1.0 2.17e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  5390
VecMAXPY             431 1.0 1.1570e-02 1.0 2.86e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2474
VecNormalize         431 1.0 3.6497e-03 1.0 1.09e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2998
MatMult              367 1.0 2.0761e-02 1.0 3.93e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  1893
MatSolve             431 1.0 1.4541e-01 1.0 2.97e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  4  0  0  0   2  4  0  0  0  2044
MatLUFactorSym        64 1.0 6.3581e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatLUFactorNum        64 1.0 4.0353e-01 1.0 7.60e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4 10  0  0  0   4 10  0  0  0  1884
MatAssemblyBegin     256 1.0 4.7684e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd       256 1.0 4.1425e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ           64 1.0 2.3708e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering        64 1.0 2.4846e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries       128 1.0 8.9335e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp               64 1.0 4.9318e-01 1.0 7.60e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4 10  0  0  0   5 10  0  0  0  1542
PCApply              431 1.0 1.4550e-01 1.0 2.97e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  4  0  0  0   2  4  0  0  0  2043
KSPSetUp              64 1.0 1.1082e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              64 1.0 6.8816e-01 1.0 1.16e+09 1.0 0.0e+00 0.0e+00 0.0e+00  6 15  0  0  0   7 15  0  0  0  1686
KSPGMRESOrthog       367 1.0 1.8158e-02 1.0 4.48e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  2468
SNESSolve             64 1.0 9.6351e+00 1.0 7.55e+09 1.0 0.0e+00 0.0e+00 0.0e+00 84100  0  0  0 100100  0  0  0   783
SNESFunctionEval      64 1.0 2.8924e+00 1.0 1.64e+09 1.0 0.0e+00 0.0e+00 0.0e+00 25 22  0  0  0  30 22  0  0  0   568
SNESJacobianEval      64 1.0 6.0535e+00 1.0 4.74e+09 1.0 0.0e+00 0.0e+00 0.0e+00 53 63  0  0  0  63 63  0  0  0   783
ParLoopExecute       448 1.0 8.8709e+00 1.0 6.38e+09 1.0 0.0e+00 0.0e+00 0.0e+00 77 85  0  0  0  92 85  0  0  0   720
ParLoopset_1         128 1.0 1.8187e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednBegin     448 1.0 1.0083e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd       448 1.0 4.6206e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells         384 1.0 8.8184e+00 1.0 6.37e+09 1.0 0.0e+00 0.0e+00 0.0e+00 77 84  0  0  0  91 84  0  0  0   722
ParLoopExtFacets     384 1.0 7.6773e-03 1.0 1.40e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1819
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
           Index Set    37            229      2055628     0.
   IS L to G Mapping     1              1         9144     0.
             Section    17             17        12376     0.
   Star Forest Graph    18             18        14816     0.
              Vector   134           1116     37373472     0.
              Matrix   133            197    155399232     0.
      Preconditioner    65             65        65520     0.
       Krylov Solver    65             65      2242088     0.
     DMKSP interface     0              1          664     0.
                SNES    65             65        89960     0.
              DMSNES     1              1          680     0.
      SNESLineSearch    65             65        65000     0.
    Distributed Mesh     9              9        44160     0.
    GraphPartitioner     2              2         1240     0.
     Discrete System     9              9         8428     0.

--- Event Stage 1: HelmholtzProblem solve

           Index Set   192              0            0     0.
              Vector  1045             63      2230200     0.
              Matrix    64              0            0     0.
     DMKSP interface     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --with-fortran-bindings=0 --download-chaco --download-metis --download-exodusii --with-zlib --with-scalar-type=complex --download-netcdf --download-mumps --download-parmetis --download-pnetcdf --download-scalapack --download-hdf5
-----------------------------------------
Libraries compiled on 2018-11-08 14:42:32 on mapc-4034 
Machine characteristics: Linux-4.4.0-138-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/openmpi/lib -L/usr/lib/openmpi/lib -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lX11 -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Tue Jan 15 14:19:31 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500 Tue Jan 15 14:19:44 2019
run-qmc.py on a , 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-qmc.py on a  named mapc-4034 with 1 processor, by orp20 Tue Jan 15 14:20:05 2019
Using Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500

                         Max       Max/Min     Avg       Total 
Time (sec):           2.179e+01     1.000   2.179e+01
Objects:              3.716e+03     1.000   3.716e+03
Flop:                 1.506e+10     1.000   1.506e+10  1.506e+10
Flop/sec:             6.911e+08     1.000   6.911e+08  6.911e+08
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 2.6773e+00  12.3%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 
 1: HelmholtzProblem solve: 1.9108e+01  87.7%  1.5055e+10 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSidedF         1 1.0 7.3910e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             1 1.0 4.0531e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               133 1.0 1.7333e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin     259 1.0 1.0800e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd       259 1.0 8.9271e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexInterp           1 1.0 4.4680e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexStratify         2 1.0 2.7521e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateMesh             4 1.0 4.8118e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateFunctionSpace       1 1.0 3.2785e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   1  0  0  0  0     0
Mesh: reorder          1 1.0 5.1403e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        1 1.0 2.6908e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateSparsity         1 1.0 1.9510e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroInitial       258 1.0 1.7477e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   7  0  0  0  0     0

--- Event Stage 1: HelmholtzProblem solve

VecMDot              700 1.0 1.6451e-02 1.0 4.15e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2521
VecNorm              828 1.0 5.2125e-03 1.0 1.40e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2688
VecScale             828 1.0 1.6179e-03 1.0 7.01e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  4332
VecCopy              896 1.0 1.3633e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              2324 1.0 2.8367e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              256 1.0 7.9560e-04 1.0 4.33e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  5447
VecMAXPY             828 1.0 2.1519e-02 1.0 5.33e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2479
VecNormalize         828 1.0 7.0770e-03 1.0 2.10e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2970
MatMult              700 1.0 3.9536e-02 1.0 7.50e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1896
MatSolve             828 1.0 2.7959e-01 1.0 5.71e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  4  0  0  0   1  4  0  0  0  2042
MatLUFactorSym       128 1.0 1.2684e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatLUFactorNum       128 1.0 8.0697e-01 1.0 1.52e+09 1.0 0.0e+00 0.0e+00 0.0e+00  4 10  0  0  0   4 10  0  0  0  1885
MatAssemblyBegin     512 1.0 2.5034e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd       512 1.0 8.2877e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ          128 1.0 4.7474e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering       128 1.0 4.9956e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries       256 1.0 1.7073e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp              128 1.0 9.8628e-01 1.0 1.52e+09 1.0 0.0e+00 0.0e+00 0.0e+00  5 10  0  0  0   5 10  0  0  0  1542
PCApply              828 1.0 2.7978e-01 1.0 5.71e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  4  0  0  0   1  4  0  0  0  2041
KSPSetUp             128 1.0 2.2333e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve             128 1.0 1.3597e+00 1.0 2.28e+09 1.0 0.0e+00 0.0e+00 0.0e+00  6 15  0  0  0   7 15  0  0  0  1680
KSPGMRESOrthog       700 1.0 3.3632e-02 1.0 8.30e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  2467
SNESSolve            128 1.0 1.9095e+01 1.0 1.51e+10 1.0 0.0e+00 0.0e+00 0.0e+00 88100  0  0  0 100100  0  0  0   788
SNESFunctionEval     128 1.0 5.7591e+00 1.0 3.28e+09 1.0 0.0e+00 0.0e+00 0.0e+00 26 22  0  0  0  30 22  0  0  0   570
SNESJacobianEval     128 1.0 1.1974e+01 1.0 9.48e+09 1.0 0.0e+00 0.0e+00 0.0e+00 55 63  0  0  0  63 63  0  0  0   792
ParLoopExecute       896 1.0 1.7583e+01 1.0 1.28e+10 1.0 0.0e+00 0.0e+00 0.0e+00 81 85  0  0  0  92 85  0  0  0   726
ParLoopset_1         256 1.0 3.1497e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednBegin     896 1.0 2.0535e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd       896 1.0 9.2316e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells         768 1.0 1.7496e+01 1.0 1.27e+10 1.0 0.0e+00 0.0e+00 0.0e+00 80 85  0  0  0  92 85  0  0  0   728
ParLoopExtFacets     768 1.0 1.3953e-02 1.0 2.79e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2001
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
           Index Set    37            421      3834316     0.
   IS L to G Mapping     1              1         9144     0.
             Section    17             17        12376     0.
   Star Forest Graph    18             18        14816     0.
              Vector   262           2204     73721888     0.
              Matrix   261            389    310121024     0.
      Preconditioner   129            129       130032     0.
       Krylov Solver   129            129      4482600     0.
     DMKSP interface     0              1          664     0.
                SNES   129            129       178536     0.
              DMSNES     1              1          680     0.
      SNESLineSearch   129            129       129000     0.
    Distributed Mesh     9              9        44160     0.
    GraphPartitioner     2              2         1240     0.
     Discrete System     9              9         8428     0.

--- Event Stage 1: HelmholtzProblem solve

           Index Set   384              0            0     0.
              Vector  2069            127      4495800     0.
              Matrix   128              0            0     0.
     DMKSP interface     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --with-fortran-bindings=0 --download-chaco --download-metis --download-exodusii --with-zlib --with-scalar-type=complex --download-netcdf --download-mumps --download-parmetis --download-pnetcdf --download-scalapack --download-hdf5
-----------------------------------------
Libraries compiled on 2018-11-08 14:42:32 on mapc-4034 
Machine characteristics: Linux-4.4.0-138-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/openmpi/lib -L/usr/lib/openmpi/lib -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lX11 -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Tue Jan 15 14:20:05 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500 Tue Jan 15 14:20:20 2019
run-qmc.py on a , 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-qmc.py on a  named mapc-4034 with 1 processor, by orp20 Tue Jan 15 14:21:03 2019
Using Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500

                         Max       Max/Min     Avg       Total 
Time (sec):           4.267e+01     1.000   4.267e+01
Objects:              7.290e+03     1.000   7.290e+03
Flop:                 3.003e+10     1.000   3.003e+10  3.003e+10
Flop/sec:             7.038e+08     1.000   7.038e+08  7.038e+08
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 4.2740e+00  10.0%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 
 1: HelmholtzProblem solve: 3.8397e+01  90.0%  3.0032e+10 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSidedF         1 1.0 7.7009e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             1 1.0 4.0531e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               261 1.0 3.1710e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin     515 1.0 1.3781e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd       515 1.0 1.7768e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexInterp           1 1.0 4.4739e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexStratify         2 1.0 2.7359e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateMesh             4 1.0 4.8330e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateFunctionSpace       1 1.0 3.2697e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   1  0  0  0  0     0
Mesh: reorder          1 1.0 4.9806e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        1 1.0 2.7070e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateSparsity         1 1.0 1.9550e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroInitial       514 1.0 3.5051e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   8  0  0  0  0     0

--- Event Stage 1: HelmholtzProblem solve

VecMDot             1334 1.0 2.8345e-02 1.0 7.14e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2520
VecNorm             1590 1.0 9.9802e-03 1.0 2.69e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2696
VecScale            1590 1.0 3.1307e-03 1.0 1.35e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  4299
VecCopy             1792 1.0 2.7215e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              4618 1.0 5.4891e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              512 1.0 1.5979e-03 1.0 8.67e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  5424
VecMAXPY            1590 1.0 3.7964e-02 1.0 9.40e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2477
VecNormalize        1590 1.0 1.3504e-02 1.0 4.04e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2989
MatMult             1334 1.0 7.5406e-02 1.0 1.43e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1895
MatSolve            1590 1.0 5.3624e-01 1.0 1.10e+09 1.0 0.0e+00 0.0e+00 0.0e+00  1  4  0  0  0   1  4  0  0  0  2045
MatLUFactorSym       256 1.0 2.5386e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatLUFactorNum       256 1.0 1.6119e+00 1.0 3.04e+09 1.0 0.0e+00 0.0e+00 0.0e+00  4 10  0  0  0   4 10  0  0  0  1887
MatAssemblyBegin    1024 1.0 5.7459e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd      1024 1.0 1.6653e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ          256 1.0 9.4409e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering       256 1.0 9.9582e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries       512 1.0 3.2158e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp              256 1.0 1.9703e+00 1.0 3.04e+09 1.0 0.0e+00 0.0e+00 0.0e+00  5 10  0  0  0   5 10  0  0  0  1544
PCApply             1590 1.0 5.3659e-01 1.0 1.10e+09 1.0 0.0e+00 0.0e+00 0.0e+00  1  4  0  0  0   1  4  0  0  0  2044
KSPSetUp             256 1.0 4.4415e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve             256 1.0 2.6800e+00 1.0 4.49e+09 1.0 0.0e+00 0.0e+00 0.0e+00  6 15  0  0  0   7 15  0  0  0  1676
KSPGMRESOrthog      1334 1.0 5.7977e-02 1.0 1.43e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2464
SNESSolve            256 1.0 3.8372e+01 1.0 3.00e+10 1.0 0.0e+00 0.0e+00 0.0e+00 90100  0  0  0 100100  0  0  0   783
SNESFunctionEval     256 1.0 1.1744e+01 1.0 6.57e+09 1.0 0.0e+00 0.0e+00 0.0e+00 28 22  0  0  0  31 22  0  0  0   559
SNESJacobianEval     256 1.0 2.3943e+01 1.0 1.90e+10 1.0 0.0e+00 0.0e+00 0.0e+00 56 63  0  0  0  62 63  0  0  0   792
ParLoopExecute      1792 1.0 3.5391e+01 1.0 2.55e+10 1.0 0.0e+00 0.0e+00 0.0e+00 83 85  0  0  0  92 85  0  0  0   722
ParLoopset_1         512 1.0 5.9726e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednBegin    1792 1.0 3.9594e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd      1792 1.0 1.9264e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells        1536 1.0 3.5237e+01 1.0 2.55e+10 1.0 0.0e+00 0.0e+00 0.0e+00 83 85  0  0  0  92 85  0  0  0   723
ParLoopExtFacets    1536 1.0 2.6520e-02 1.0 5.58e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2106
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
           Index Set    37            805      7391692     0.
   IS L to G Mapping     1              1         9144     0.
             Section    17             17        12376     0.
   Star Forest Graph    18             18        14816     0.
              Vector   518           4370    146064720     0.
              Matrix   517            773    619564608     0.
      Preconditioner   257            257       259056     0.
       Krylov Solver   257            257      8963624     0.
     DMKSP interface     0              1          664     0.
                SNES   257            257       355688     0.
              DMSNES     1              1          680     0.
      SNESLineSearch   257            257       257000     0.
    Distributed Mesh     9              9        44160     0.
    GraphPartitioner     2              2         1240     0.
     Discrete System     9              9         8428     0.

--- Event Stage 1: HelmholtzProblem solve

           Index Set   768              0            0     0.
              Vector  4107            255      9027000     0.
              Matrix   256              0            0     0.
     DMKSP interface     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --with-fortran-bindings=0 --download-chaco --download-metis --download-exodusii --with-zlib --with-scalar-type=complex --download-netcdf --download-mumps --download-parmetis --download-pnetcdf --download-scalapack --download-hdf5
-----------------------------------------
Libraries compiled on 2018-11-08 14:42:32 on mapc-4034 
Machine characteristics: Linux-4.4.0-138-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/openmpi/lib -L/usr/lib/openmpi/lib -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lX11 -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Tue Jan 15 14:21:03 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500 Tue Jan 15 14:21:21 2019
run-qmc.py on a , 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-qmc.py on a  named mapc-4034 with 1 processor, by orp20 Tue Jan 15 14:21:27 2019
Using Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500

                         Max       Max/Min     Avg       Total 
Time (sec):           6.185e+00     1.000   6.185e+00
Objects:              2.360e+02     1.000   2.360e+02
Flop:                 3.833e+09     1.000   3.833e+09  3.833e+09
Flop/sec:             6.197e+08     1.000   6.197e+08  6.197e+08
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 1.1471e+00  18.5%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 
 1: HelmholtzProblem solve: 5.0376e+00  81.5%  3.8329e+09 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSidedF         1 1.0 7.6056e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             1 1.0 4.0531e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                 8 1.0 3.9577e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       9 1.0 8.5831e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         9 1.0 3.0947e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexInterp           1 1.0 4.4608e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexStratify         2 1.0 2.7189e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateMesh             4 1.0 4.8223e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateFunctionSpace       1 1.0 3.2805e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   3  0  0  0  0     0
Mesh: reorder          1 1.0 4.8900e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        1 1.0 2.7049e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateSparsity         1 1.0 1.9870e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroInitial         8 1.0 5.7411e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   1  0  0  0  0     0

--- Event Stage 1: HelmholtzProblem solve

VecMDot              267 1.0 8.7183e-03 1.0 2.19e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  2516
VecNorm              302 1.0 1.8871e-03 1.0 5.11e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2708
VecScale             302 1.0 5.5170e-04 1.0 2.56e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  4633
VecCopy              245 1.0 3.3140e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               154 1.0 3.9935e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               70 1.0 2.1148e-04 1.0 1.18e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  5603
VecMAXPY             302 1.0 1.0657e-02 1.0 2.65e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  2483
VecNormalize         302 1.0 2.5299e-03 1.0 7.67e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3031
MatMult              267 1.0 1.5071e-02 1.0 2.86e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  1898
MatSolve             302 1.0 1.0216e-01 1.0 2.08e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  5  0  0  0   2  5  0  0  0  2039
MatLUFactorSym         4 1.0 4.1544e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum         4 1.0 2.5423e-02 1.0 4.75e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   1  1  0  0  0  1869
MatAssemblyBegin     140 1.0 1.1206e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd       140 1.0 2.2802e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            4 1.0 1.6880e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         4 1.0 1.5919e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        70 1.0 4.4179e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp                4 1.0 3.1263e-02 1.0 4.75e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0  1520
PCApply              302 1.0 1.0222e-01 1.0 2.08e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  5  0  0  0   2  5  0  0  0  2038
KSPSetUp              35 1.0 1.3065e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              35 1.0 1.7182e-01 1.0 3.41e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  9  0  0  0   3  9  0  0  0  1985
KSPGMRESOrthog       267 1.0 1.7719e-02 1.0 4.39e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  2476
SNESSolve             35 1.0 5.0346e+00 1.0 3.83e+09 1.0 0.0e+00 0.0e+00 0.0e+00 81100  0  0  0 100100  0  0  0   761
SNESFunctionEval      35 1.0 1.6132e+00 1.0 8.98e+08 1.0 0.0e+00 0.0e+00 0.0e+00 26 23  0  0  0  32 23  0  0  0   557
SNESJacobianEval      35 1.0 3.2490e+00 1.0 2.59e+09 1.0 0.0e+00 0.0e+00 0.0e+00 53 68  0  0  0  64 68  0  0  0   798
ParLoopExecute       245 1.0 4.8481e+00 1.0 3.49e+09 1.0 0.0e+00 0.0e+00 0.0e+00 78 91  0  0  0  96 91  0  0  0   720
ParLoopset_1          70 1.0 1.0653e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednBegin     245 1.0 2.6250e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd       245 1.0 2.4033e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells         210 1.0 4.8174e+00 1.0 3.48e+09 1.0 0.0e+00 0.0e+00 0.0e+00 78 91  0  0  0  96 91  0  0  0   723
ParLoopExtFacets     210 1.0 4.6995e-03 1.0 7.64e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1625
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
           Index Set    37             49       388108     0.
   IS L to G Mapping     1              1         9144     0.
             Section    17             17        12376     0.
   Star Forest Graph    18             18        14816     0.
              Vector    12             94      3259888     0.
              Matrix    11             15      9678584     0.
      Preconditioner     4              4         4032     0.
       Krylov Solver     4              4       140032     0.
     DMKSP interface     0              1          664     0.
                SNES     4              4         5536     0.
              DMSNES     1              1          680     0.
      SNESLineSearch     4              4         4000     0.
    Distributed Mesh     9              9        44160     0.
    GraphPartitioner     2              2         1240     0.
     Discrete System     9              9         8428     0.

--- Event Stage 1: HelmholtzProblem solve

           Index Set    12              0            0     0.
              Vector    85              3       106200     0.
              Matrix     4              0            0     0.
     DMKSP interface     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --with-fortran-bindings=0 --download-chaco --download-metis --download-exodusii --with-zlib --with-scalar-type=complex --download-netcdf --download-mumps --download-parmetis --download-pnetcdf --download-scalapack --download-hdf5
-----------------------------------------
Libraries compiled on 2018-11-08 14:42:32 on mapc-4034 
Machine characteristics: Linux-4.4.0-138-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/openmpi/lib -L/usr/lib/openmpi/lib -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lX11 -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Tue Jan 15 14:21:27 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500 Tue Jan 15 14:21:43 2019
run-qmc.py on a , 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-qmc.py on a  named mapc-4034 with 1 processor, by orp20 Tue Jan 15 14:21:54 2019
Using Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500

                         Max       Max/Min     Avg       Total 
Time (sec):           1.071e+01     1.000   1.071e+01
Objects:              2.360e+02     1.000   2.360e+02
Flop:                 7.292e+09     1.000   7.292e+09  7.292e+09
Flop/sec:             6.812e+08     1.000   6.812e+08  6.812e+08
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 1.1865e+00  11.1%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 
 1: HelmholtzProblem solve: 9.5194e+00  88.9%  7.2924e+09 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSidedF         1 1.0 7.7009e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             1 1.0 4.0531e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                 8 1.0 3.4571e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       9 1.0 8.7023e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         9 1.0 3.0780e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexInterp           1 1.0 4.4620e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexStratify         2 1.0 2.7211e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateMesh             4 1.0 4.8342e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateFunctionSpace       1 1.0 3.2662e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   3  0  0  0  0     0
Mesh: reorder          1 1.0 5.0497e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        1 1.0 2.7080e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateSparsity         1 1.0 1.9431e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroInitial         8 1.0 5.6756e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0

--- Event Stage 1: HelmholtzProblem solve

VecMDot              513 1.0 1.6088e-02 1.0 4.04e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  2514
VecNorm              580 1.0 3.6442e-03 1.0 9.82e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2694
VecScale             580 1.0 1.0350e-03 1.0 4.91e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  4743
VecCopy              469 1.0 6.2037e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               218 1.0 4.8041e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              134 1.0 3.7909e-04 1.0 2.27e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  5984
VecMAXPY             580 1.0 1.9824e-02 1.0 4.91e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  2479
VecNormalize         580 1.0 4.8289e-03 1.0 1.47e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3049
MatMult              513 1.0 3.0146e-02 1.0 5.49e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  1823
MatSolve             580 1.0 1.9618e-01 1.0 4.00e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  5  0  0  0   2  5  0  0  0  2039
MatLUFactorSym         4 1.0 4.1528e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum         4 1.0 2.5423e-02 1.0 4.75e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  1869
MatAssemblyBegin     268 1.0 1.8358e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd       268 1.0 4.3252e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            4 1.0 1.6737e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         4 1.0 1.5864e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries       134 1.0 7.8058e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp                4 1.0 3.1250e-02 1.0 4.75e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  1521
PCApply              580 1.0 1.9629e-01 1.0 4.00e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  5  0  0  0   2  5  0  0  0  2038
KSPSetUp              67 1.0 1.3494e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              67 1.0 3.0056e-01 1.0 6.08e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  8  0  0  0   3  8  0  0  0  2023
KSPGMRESOrthog       513 1.0 3.2703e-02 1.0 8.09e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  2474
SNESSolve             67 1.0 9.5137e+00 1.0 7.29e+09 1.0 0.0e+00 0.0e+00 0.0e+00 89100  0  0  0 100100  0  0  0   767
SNESFunctionEval      67 1.0 3.0027e+00 1.0 1.72e+09 1.0 0.0e+00 0.0e+00 0.0e+00 28 24  0  0  0  32 24  0  0  0   573
SNESJacobianEval      67 1.0 6.2094e+00 1.0 4.96e+09 1.0 0.0e+00 0.0e+00 0.0e+00 58 68  0  0  0  65 68  0  0  0   799
ParLoopExecute       469 1.0 9.1909e+00 1.0 6.68e+09 1.0 0.0e+00 0.0e+00 0.0e+00 86 92  0  0  0  97 92  0  0  0   727
ParLoopset_1         134 1.0 1.4920e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednBegin     469 1.0 3.9005e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd       469 1.0 4.3321e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells         402 1.0 9.1520e+00 1.0 6.67e+09 1.0 0.0e+00 0.0e+00 0.0e+00 85 91  0  0  0  96 91  0  0  0   729
ParLoopExtFacets     402 1.0 7.4840e-03 1.0 1.46e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1953
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
           Index Set    37             49       388108     0.
   IS L to G Mapping     1              1         9144     0.
             Section    17             17        12376     0.
   Star Forest Graph    18             18        14816     0.
              Vector    12             94      3259888     0.
              Matrix    11             15      9678584     0.
      Preconditioner     4              4         4032     0.
       Krylov Solver     4              4       140032     0.
     DMKSP interface     0              1          664     0.
                SNES     4              4         5536     0.
              DMSNES     1              1          680     0.
      SNESLineSearch     4              4         4000     0.
    Distributed Mesh     9              9        44160     0.
    GraphPartitioner     2              2         1240     0.
     Discrete System     9              9         8428     0.

--- Event Stage 1: HelmholtzProblem solve

           Index Set    12              0            0     0.
              Vector    85              3       106200     0.
              Matrix     4              0            0     0.
     DMKSP interface     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --with-fortran-bindings=0 --download-chaco --download-metis --download-exodusii --with-zlib --with-scalar-type=complex --download-netcdf --download-mumps --download-parmetis --download-pnetcdf --download-scalapack --download-hdf5
-----------------------------------------
Libraries compiled on 2018-11-08 14:42:32 on mapc-4034 
Machine characteristics: Linux-4.4.0-138-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/openmpi/lib -L/usr/lib/openmpi/lib -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lX11 -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Tue Jan 15 14:21:54 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500 Tue Jan 15 14:22:06 2019
run-qmc.py on a , 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-qmc.py on a  named mapc-4034 with 1 processor, by orp20 Tue Jan 15 14:22:26 2019
Using Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500

                         Max       Max/Min     Avg       Total 
Time (sec):           1.991e+01     1.000   1.991e+01
Objects:              2.360e+02     1.000   2.360e+02
Flop:                 1.423e+10     1.000   1.423e+10  1.423e+10
Flop/sec:             7.148e+08     1.000   7.148e+08  7.148e+08
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 1.2729e+00   6.4%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 
 1: HelmholtzProblem solve: 1.8632e+01  93.6%  1.4229e+10 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSidedF         1 1.0 7.4148e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             1 1.0 4.0531e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                 8 1.0 3.6716e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       9 1.0 8.7023e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         9 1.0 2.9993e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexInterp           1 1.0 4.4711e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexStratify         2 1.0 2.7380e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateMesh             4 1.0 4.8082e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateFunctionSpace       1 1.0 3.2858e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   3  0  0  0  0     0
Mesh: reorder          1 1.0 4.9400e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        1 1.0 2.7041e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateSparsity         1 1.0 1.9610e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroInitial         8 1.0 5.7709e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0

--- Event Stage 1: HelmholtzProblem solve

VecMDot             1020 1.0 3.1979e-02 1.0 7.99e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  2500
VecNorm             1151 1.0 7.2117e-03 1.0 1.95e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2701
VecScale            1151 1.0 2.0740e-03 1.0 9.74e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  4697
VecCopy              917 1.0 1.2071e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               346 1.0 6.2394e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              262 1.0 7.5340e-04 1.0 4.44e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  5887
VecMAXPY            1151 1.0 3.9287e-02 1.0 9.72e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  2475
VecNormalize        1151 1.0 9.5835e-03 1.0 2.92e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3049
MatMult             1020 1.0 5.7808e-02 1.0 1.09e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  1890
MatSolve            1151 1.0 3.9057e-01 1.0 7.94e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  6  0  0  0   2  6  0  0  0  2032
MatLUFactorSym         4 1.0 4.2338e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum         4 1.0 2.5499e-02 1.0 4.75e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1864
MatAssemblyBegin     524 1.0 3.2663e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd       524 1.0 8.4805e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            4 1.0 1.7285e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         4 1.0 1.5850e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries       262 1.0 1.4162e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp                4 1.0 3.1412e-02 1.0 4.75e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1513
PCApply             1151 1.0 3.9077e-01 1.0 7.94e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  6  0  0  0   2  6  0  0  0  2031
KSPSetUp             131 1.0 1.4400e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve             131 1.0 5.6446e-01 1.0 1.16e+09 1.0 0.0e+00 0.0e+00 0.0e+00  3  8  0  0  0   3  8  0  0  0  2054
KSPGMRESOrthog      1020 1.0 6.4904e-02 1.0 1.60e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  2464
SNESSolve            131 1.0 1.8621e+01 1.0 1.42e+10 1.0 0.0e+00 0.0e+00 0.0e+00 94100  0  0  0 100100  0  0  0   764
SNESFunctionEval     131 1.0 5.8840e+00 1.0 3.36e+09 1.0 0.0e+00 0.0e+00 0.0e+00 30 24  0  0  0  32 24  0  0  0   571
SNESJacobianEval     131 1.0 1.2170e+01 1.0 9.71e+09 1.0 0.0e+00 0.0e+00 0.0e+00 61 68  0  0  0  65 68  0  0  0   798
ParLoopExecute       917 1.0 1.8018e+01 1.0 1.31e+10 1.0 0.0e+00 0.0e+00 0.0e+00 91 92  0  0  0  97 92  0  0  0   725
ParLoopset_1         262 1.0 2.4166e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednBegin     917 1.0 7.5388e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd       917 1.0 8.4066e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells         786 1.0 1.7960e+01 1.0 1.30e+10 1.0 0.0e+00 0.0e+00 0.0e+00 90 92  0  0  0  96 92  0  0  0   726
ParLoopExtFacets     786 1.0 1.3181e-02 1.0 2.86e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2168
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
           Index Set    37             49       388108     0.
   IS L to G Mapping     1              1         9144     0.
             Section    17             17        12376     0.
   Star Forest Graph    18             18        14816     0.
              Vector    12             94      3259888     0.
              Matrix    11             15      9678584     0.
      Preconditioner     4              4         4032     0.
       Krylov Solver     4              4       140032     0.
     DMKSP interface     0              1          664     0.
                SNES     4              4         5536     0.
              DMSNES     1              1          680     0.
      SNESLineSearch     4              4         4000     0.
    Distributed Mesh     9              9        44160     0.
    GraphPartitioner     2              2         1240     0.
     Discrete System     9              9         8428     0.

--- Event Stage 1: HelmholtzProblem solve

           Index Set    12              0            0     0.
              Vector    85              3       106200     0.
              Matrix     4              0            0     0.
     DMKSP interface     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --with-fortran-bindings=0 --download-chaco --download-metis --download-exodusii --with-zlib --with-scalar-type=complex --download-netcdf --download-mumps --download-parmetis --download-pnetcdf --download-scalapack --download-hdf5
-----------------------------------------
Libraries compiled on 2018-11-08 14:42:32 on mapc-4034 
Machine characteristics: Linux-4.4.0-138-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/openmpi/lib -L/usr/lib/openmpi/lib -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lX11 -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Tue Jan 15 14:22:26 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500 Tue Jan 15 14:22:39 2019
run-qmc.py on a , 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-qmc.py on a  named mapc-4034 with 1 processor, by orp20 Tue Jan 15 14:23:17 2019
Using Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500

                         Max       Max/Min     Avg       Total 
Time (sec):           3.830e+01     1.000   3.830e+01
Objects:              2.360e+02     1.000   2.360e+02
Flop:                 2.807e+10     1.000   2.807e+10  2.807e+10
Flop/sec:             7.329e+08     1.000   7.329e+08  7.329e+08
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 1.4492e+00   3.8%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 
 1: HelmholtzProblem solve: 3.6847e+01  96.2%  2.8067e+10 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSidedF         1 1.0 7.6056e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             1 1.0 4.0531e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                 8 1.0 4.8637e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       9 1.0 8.7976e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         9 1.0 3.0661e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexInterp           1 1.0 4.4658e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexStratify         2 1.0 2.7452e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateMesh             4 1.0 4.8268e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateFunctionSpace       1 1.0 3.2677e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   2  0  0  0  0     0
Mesh: reorder          1 1.0 4.9710e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        1 1.0 2.7020e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateSparsity         1 1.0 1.9650e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroInitial         8 1.0 5.7092e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0

--- Event Stage 1: HelmholtzProblem solve

VecMDot             2005 1.0 6.1671e-02 1.0 1.54e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  2496
VecNorm             2264 1.0 1.4351e-02 1.0 3.83e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2670
VecScale            2264 1.0 4.0646e-03 1.0 1.92e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  4715
VecCopy             1813 1.0 2.6348e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               602 1.0 1.0030e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              518 1.0 1.5786e-03 1.0 8.77e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  5555
VecMAXPY            2264 1.0 7.6144e-02 1.0 1.88e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  2468
VecNormalize        2264 1.0 1.9034e-02 1.0 5.75e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3020
MatMult             2005 1.0 1.1363e-01 1.0 2.15e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  1890
MatSolve            2264 1.0 7.6977e-01 1.0 1.56e+09 1.0 0.0e+00 0.0e+00 0.0e+00  2  6  0  0  0   2  6  0  0  0  2028
MatLUFactorSym         4 1.0 4.1678e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum         4 1.0 2.5336e-02 1.0 4.75e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1876
MatAssemblyBegin    1036 1.0 5.5313e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd      1036 1.0 1.6899e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            4 1.0 1.6618e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         4 1.0 1.5831e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries       518 1.0 2.8350e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp                4 1.0 3.1173e-02 1.0 4.75e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1525
PCApply             2264 1.0 7.7023e-01 1.0 1.56e+09 1.0 0.0e+00 0.0e+00 0.0e+00  2  6  0  0  0   2  6  0  0  0  2027
KSPSetUp             259 1.0 1.9932e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve             259 1.0 1.0795e+00 1.0 2.23e+09 1.0 0.0e+00 0.0e+00 0.0e+00  3  8  0  0  0   3  8  0  0  0  2063
KSPGMRESOrthog      2005 1.0 1.2528e-01 1.0 3.08e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  2458
SNESSolve            259 1.0 3.6823e+01 1.0 2.81e+10 1.0 0.0e+00 0.0e+00 0.0e+00 96100  0  0  0 100100  0  0  0   762
SNESFunctionEval     259 1.0 1.1607e+01 1.0 6.65e+09 1.0 0.0e+00 0.0e+00 0.0e+00 30 24  0  0  0  32 24  0  0  0   573
SNESJacobianEval     259 1.0 2.4132e+01 1.0 1.92e+10 1.0 0.0e+00 0.0e+00 0.0e+00 63 68  0  0  0  65 68  0  0  0   795
ParLoopExecute      1813 1.0 3.5670e+01 1.0 2.58e+10 1.0 0.0e+00 0.0e+00 0.0e+00 93 92  0  0  0  97 92  0  0  0   724
ParLoopset_1         518 1.0 4.3409e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednBegin    1813 1.0 1.3902e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd      1813 1.0 1.6811e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells        1554 1.0 3.5573e+01 1.0 2.58e+10 1.0 0.0e+00 0.0e+00 0.0e+00 93 92  0  0  0  97 92  0  0  0   725
ParLoopExtFacets    1554 1.0 2.5877e-02 1.0 5.65e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2183
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
           Index Set    37             49       388108     0.
   IS L to G Mapping     1              1         9144     0.
             Section    17             17        12376     0.
   Star Forest Graph    18             18        14816     0.
              Vector    12             94      3259888     0.
              Matrix    11             15      9678584     0.
      Preconditioner     4              4         4032     0.
       Krylov Solver     4              4       140032     0.
     DMKSP interface     0              1          664     0.
                SNES     4              4         5536     0.
              DMSNES     1              1          680     0.
      SNESLineSearch     4              4         4000     0.
    Distributed Mesh     9              9        44160     0.
    GraphPartitioner     2              2         1240     0.
     Discrete System     9              9         8428     0.

--- Event Stage 1: HelmholtzProblem solve

           Index Set    12              0            0     0.
              Vector    85              3       106200     0.
              Matrix     4              0            0     0.
     DMKSP interface     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --with-fortran-bindings=0 --download-chaco --download-metis --download-exodusii --with-zlib --with-scalar-type=complex --download-netcdf --download-mumps --download-parmetis --download-pnetcdf --download-scalapack --download-hdf5
-----------------------------------------
Libraries compiled on 2018-11-08 14:42:32 on mapc-4034 
Machine characteristics: Linux-4.4.0-138-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/openmpi/lib -L/usr/lib/openmpi/lib -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lX11 -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Tue Jan 15 14:23:17 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500 Thu Jan 17 15:03:53 2019
run-qmc.py on a , 1 proc. with options:
---------------------------------------------------------
[0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: ---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500 Thu Jan 17 16:08:24 2019
run-qmc.py on a , 1 proc. with options:
---------------------------------------------------------
[0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: ---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500 Thu Jan 17 16:29:59 2019
run-qmc.py on a , 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-qmc.py on a  named mapc-4034 with 1 processor, by orp20 Thu Jan 17 19:46:44 2019
Using Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500

                         Max       Max/Min     Avg       Total 
Time (sec):           1.181e+04     1.000   1.181e+04
Objects:              3.764e+04     1.000   3.764e+04
Flop:                 5.340e+12     1.000   5.340e+12  5.340e+12
Flop/sec:             4.523e+08     1.000   4.523e+08  4.523e+08
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 9.9657e-01   0.0%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 
 1:         No nbpc: 6.0373e+03  51.1%  2.9006e+12  54.3%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 
 2:            nbpc: 5.7674e+03  48.9%  2.4391e+12  45.7%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage


--- Event Stage 1: No nbpc

BuildTwoSidedF         1 1.0 7.7009e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             1 1.0 2.9087e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             2048 1.0 1.1100e-01 1.0 2.68e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2418
VecNorm             4096 1.0 1.9726e-01 1.0 5.37e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2722
VecScale            4096 1.0 5.5012e-02 1.0 2.68e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  4880
VecCopy            14336 1.0 2.4699e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet             18436 1.0 1.5410e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY             4096 1.0 9.1859e-02 1.0 5.37e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  5845
VecMAXPY            4096 1.0 2.2145e-01 1.0 5.37e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2424
VecNormalize        4096 1.0 2.5604e-01 1.0 8.05e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3145
MatMult             2048 1.0 9.3204e-01 1.0 1.73e+09 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1854
MatSolve            4096 1.0 1.6200e+01 1.0 3.30e+10 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  2037
MatLUFactorSym      2048 1.0 2.2274e+01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum      2048 1.0 2.9878e+02 1.0 6.39e+11 1.0 0.0e+00 0.0e+00 0.0e+00  3 12  0  0  0   5 22  0  0  0  2140
MatAssemblyBegin   12289 1.0 9.5105e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd     12289 1.0 2.0887e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ         2048 1.0 5.5954e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering      2048 1.0 7.2983e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries      4096 1.0 2.1094e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp             2048 1.0 3.2842e+02 1.0 6.39e+11 1.0 0.0e+00 0.0e+00 0.0e+00  3 12  0  0  0   5 22  0  0  0  1947
PCApply             4096 1.0 1.6202e+01 1.0 3.30e+10 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  2037
KSPSetUp            2048 1.0 1.2528e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve            2048 1.0 3.4651e+02 1.0 6.76e+11 1.0 0.0e+00 0.0e+00 0.0e+00  3 13  0  0  0   6 23  0  0  0  1951
KSPGMRESOrthog      2048 1.0 2.2621e-01 1.0 5.37e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2373
SNESSolve           2048 1.0 5.9744e+03 1.0 2.90e+12 1.0 0.0e+00 0.0e+00 0.0e+00 51 54  0  0  0  99100  0  0  0   485
SNESFunctionEval    2048 1.0 1.8673e+03 1.0 6.18e+11 1.0 0.0e+00 0.0e+00 0.0e+00 16 12  0  0  0  31 21  0  0  0   331
SNESJacobianEval    2048 1.0 3.7605e+03 1.0 1.61e+12 1.0 0.0e+00 0.0e+00 0.0e+00 32 30  0  0  0  62 55  0  0  0   427
DMPlexInterp           1 1.0 3.5895e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexStratify         2 1.0 2.1969e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateMesh             4 1.0 3.3783e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateFunctionSpace       1 1.0 8.5801e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: reorder          1 1.0 3.8111e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        1 1.0 2.0733e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateSparsity         1 1.0 1.3982e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroInitial      4096 1.0 2.4795e+01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopExecute     14336 1.0 5.6235e+03 1.0 2.22e+12 1.0 0.0e+00 0.0e+00 0.0e+00 48 42  0  0  0  93 77  0  0  0   396
ParLoopset_1        4096 1.0 6.4810e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednBegin   14336 1.0 6.4241e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd     14336 1.0 2.4613e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells       12288 1.0 5.6206e+03 1.0 2.22e+12 1.0 0.0e+00 0.0e+00 0.0e+00 48 42  0  0  0  93 77  0  0  0   395
ParLoopExtFacets   12288 1.0 7.4550e-01 1.0 1.26e+09 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1691

--- Event Stage 2: nbpc

BuildTwoSidedF         1 1.0 3.1948e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             1 1.0 2.7895e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot            16346 1.0 3.9372e+00 1.0 9.79e+09 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2485
VecNorm            18413 1.0 8.8296e-01 1.0 2.41e+09 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2733
VecScale           18413 1.0 2.3118e-01 1.0 1.21e+09 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  5220
VecCopy            14469 1.0 2.1755e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              4488 1.0 4.0085e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY             4134 1.0 9.3018e-02 1.0 5.42e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  5825
VecMAXPY           18413 1.0 4.8472e+00 1.0 1.19e+10 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2461
VecNormalize       18413 1.0 1.1226e+00 1.0 3.62e+09 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3225
MatMult            16346 1.0 7.3263e+00 1.0 1.38e+10 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  1  0  0  0  1883
MatSolve           18413 1.0 7.1915e+01 1.0 1.48e+11 1.0 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  6  0  0  0  2063
MatLUFactorSym        20 1.0 2.1723e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum        20 1.0 2.9112e+00 1.0 6.25e+09 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2145
MatAssemblyBegin    8309 1.0 5.4812e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd      8309 1.0 9.8726e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ           20 1.0 5.4123e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering        20 1.0 7.1502e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries      4134 1.0 2.1017e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp               20 1.0 3.2006e+00 1.0 6.25e+09 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1951
PCApply            18413 1.0 7.1923e+01 1.0 1.48e+11 1.0 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  6  0  0  0  2063
KSPSetUp            2067 1.0 1.8644e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve            2067 1.0 9.2608e+01 1.0 1.94e+11 1.0 0.0e+00 0.0e+00 0.0e+00  1  4  0  0  0   2  8  0  0  0  2095
KSPGMRESOrthog     16346 1.0 7.9532e+00 1.0 1.96e+10 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  1  0  0  0  2461
SNESSolve           2067 1.0 5.7630e+03 1.0 2.44e+12 1.0 0.0e+00 0.0e+00 0.0e+00 49 46  0  0  0 100100  0  0  0   423
SNESFunctionEval    2067 1.0 1.8781e+03 1.0 6.23e+11 1.0 0.0e+00 0.0e+00 0.0e+00 16 12  0  0  0  33 26  0  0  0   332
SNESJacobianEval    2067 1.0 3.7922e+03 1.0 1.62e+12 1.0 0.0e+00 0.0e+00 0.0e+00 32 30  0  0  0  66 66  0  0  0   428
DMPlexInterp           1 1.0 3.5565e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexStratify         2 1.0 2.1720e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateMesh             4 1.0 3.2518e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateFunctionSpace       1 1.0 5.1947e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: reorder          1 1.0 3.6669e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        1 1.0 1.9890e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateSparsity         1 1.0 1.3830e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroInitial        40 1.0 2.0645e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopExecute     14469 1.0 5.6685e+03 1.0 2.24e+12 1.0 0.0e+00 0.0e+00 0.0e+00 48 42  0  0  0  98 92  0  0  0   396
ParLoopRednBegin   14469 1.0 1.7759e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd     14469 1.0 2.2868e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells       12402 1.0 5.6670e+03 1.0 2.24e+12 1.0 0.0e+00 0.0e+00 0.0e+00 48 42  0  0  0  98 92  0  0  0   396
ParLoopExtFacets   12402 1.0 7.2009e-01 1.0 1.27e+09 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1767
ParLoopset_5        4134 1.0 5.4321e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
           Index Set     0              9       982060     0.
   IS L to G Mapping     0              1        66216     0.
             Section     0             10         7280     0.
   Star Forest Graph     0              7         5840     0.
              Vector     0             14      2118768     0.
              Matrix     0             15     36462232     0.
     DMKSP interface     0              1          664     0.
              DMSNES     0              1          680     0.
    Distributed Mesh     0              6        29352     0.
    GraphPartitioner     0              1          620     0.
     Discrete System     0              6         5620     0.

--- Event Stage 1: No nbpc

           Index Set  6181           6172    408563616     0.
   IS L to G Mapping     1              0            0     0.
             Section    17              7         5096     0.
   Star Forest Graph    18             11         8976     0.
              Vector 16389          16345   3783070920     0.
              Matrix  6147           6104  52156832592     0.
      Preconditioner  2048           2048      2064384     0.
       Krylov Solver  2048           2048     71696384     0.
     DMKSP interface     1              0            0     0.
                SNES  2048           2048      2834432     0.
              DMSNES     1              0            0     0.
      SNESLineSearch  2048           2048      2048000     0.
    Distributed Mesh     9              3        14808     0.
    GraphPartitioner     2              1          620     0.
     Discrete System     9              3         2808     0.

--- Event Stage 2: nbpc

           Index Set    97             97      5957452     0.
   IS L to G Mapping     1              1        66216     0.
             Section    17             17        12376     0.
   Star Forest Graph    18             18        14816     0.
              Vector   375            405     94210728     0.
              Matrix    63             91    583339464     0.
      Preconditioner    20             20        20160     0.
       Krylov Solver    20             20       700160     0.
     DMKSP interface     1              1          664     0.
                SNES    20             20        27680     0.
              DMSNES     1              1          680     0.
      SNESLineSearch    20             20        20000     0.
    Distributed Mesh     9              9        44160     0.
    GraphPartitioner     2              2         1240     0.
     Discrete System     9              9         8428     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --with-fortran-bindings=0 --download-chaco --download-metis --download-exodusii --with-zlib --with-scalar-type=complex --download-netcdf --download-mumps --download-parmetis --download-pnetcdf --download-scalapack --download-hdf5
-----------------------------------------
Libraries compiled on 2018-11-08 14:42:32 on mapc-4034 
Machine characteristics: Linux-4.4.0-138-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/orp20/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/openmpi/lib -L/usr/lib/openmpi/lib -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lX11 -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Thu Jan 17 19:46:44 2019
---------------------------------------------------------
