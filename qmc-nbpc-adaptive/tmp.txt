---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000 Thu May 23 10:39:51 2019
run-nbpc-qmc.py on a , 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-nbpc-qmc.py on a  named cranmer with 1 processor, by owen Thu May 23 10:39:52 2019
Using Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000

                         Max       Max/Min     Avg       Total 
Time (sec):           1.323e+00     1.000   1.323e+00
Objects:              1.000e+00     1.000   1.000e+00
Flop:                 0.000e+00     0.000   0.000e+00  0.000e+00
Flop/sec:             0.000e+00     0.000   0.000e+00  0.000e+00
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 1.3233e+00 100.0%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 7.15256e-08
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --download-parmetis --download-pnetcdf --download-scalapack --with-zlib --download-netcdf --with-scalar-type=complex --download-hdf5 --download-mumps --download-exodusii --with-fortran-bindings=0 --download-chaco --download-metis
-----------------------------------------
Libraries compiled on 2019-03-15 16:36:05 on cranmer 
Machine characteristics: Linux-4.15.0-46-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lpthread -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lpthread -lstdc++ -ldl -lmpichfort -lmpich -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Thu May 23 10:39:52 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000 Thu May 23 10:40:09 2019
run-nbpc-qmc.py on a , 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-nbpc-qmc.py on a  named cranmer with 1 processor, by owen Thu May 23 10:40:10 2019
Using Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000

                         Max       Max/Min     Avg       Total 
Time (sec):           1.331e+00     1.000   1.331e+00
Objects:              1.000e+00     1.000   1.000e+00
Flop:                 0.000e+00     0.000   0.000e+00  0.000e+00
Flop/sec:             0.000e+00     0.000   0.000e+00  0.000e+00
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 1.3305e+00 100.0%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 7.15256e-08
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --download-parmetis --download-pnetcdf --download-scalapack --with-zlib --download-netcdf --with-scalar-type=complex --download-hdf5 --download-mumps --download-exodusii --with-fortran-bindings=0 --download-chaco --download-metis
-----------------------------------------
Libraries compiled on 2019-03-15 16:36:05 on cranmer 
Machine characteristics: Linux-4.15.0-46-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lpthread -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lpthread -lstdc++ -ldl -lmpichfort -lmpich -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Thu May 23 10:40:10 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000 Thu May 23 12:17:38 2019
run-nbpc-qmc.py on a , 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-nbpc-qmc.py on a  named cranmer with 1 processor, by owen Thu May 23 12:17:40 2019
Using Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000

                         Max       Max/Min     Avg       Total 
Time (sec):           1.329e+00     1.000   1.329e+00
Objects:              1.000e+00     1.000   1.000e+00
Flop:                 0.000e+00     0.000   0.000e+00  0.000e+00
Flop/sec:             0.000e+00     0.000   0.000e+00  0.000e+00
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 1.3290e+00 100.0%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 7.15256e-08
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --download-parmetis --download-pnetcdf --download-scalapack --with-zlib --download-netcdf --with-scalar-type=complex --download-hdf5 --download-mumps --download-exodusii --with-fortran-bindings=0 --download-chaco --download-metis
-----------------------------------------
Libraries compiled on 2019-03-15 16:36:05 on cranmer 
Machine characteristics: Linux-4.15.0-46-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lpthread -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lpthread -lstdc++ -ldl -lmpichfort -lmpich -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Thu May 23 12:17:40 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000 Thu May 23 12:17:54 2019
run-nbpc-qmc.py on a , 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-nbpc-qmc.py on a  named cranmer with 1 processor, by owen Thu May 23 12:17:55 2019
Using Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000

                         Max       Max/Min     Avg       Total 
Time (sec):           1.332e+00     1.000   1.332e+00
Objects:              1.000e+00     1.000   1.000e+00
Flop:                 0.000e+00     0.000   0.000e+00  0.000e+00
Flop/sec:             0.000e+00     0.000   0.000e+00  0.000e+00
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 1.3316e+00 100.0%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 4.76837e-08
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --download-parmetis --download-pnetcdf --download-scalapack --with-zlib --download-netcdf --with-scalar-type=complex --download-hdf5 --download-mumps --download-exodusii --with-fortran-bindings=0 --download-chaco --download-metis
-----------------------------------------
Libraries compiled on 2019-03-15 16:36:05 on cranmer 
Machine characteristics: Linux-4.15.0-46-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lpthread -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lpthread -lstdc++ -ldl -lmpichfort -lmpich -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Thu May 23 12:17:55 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000 Thu May 23 12:18:10 2019
run-nbpc-qmc.py on a , 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-nbpc-qmc.py on a  named cranmer with 1 processor, by owen Thu May 23 12:18:12 2019
Using Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000

                         Max       Max/Min     Avg       Total 
Time (sec):           1.335e+00     1.000   1.335e+00
Objects:              1.000e+00     1.000   1.000e+00
Flop:                 0.000e+00     0.000   0.000e+00  0.000e+00
Flop/sec:             0.000e+00     0.000   0.000e+00  0.000e+00
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 1.3347e+00 100.0%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 4.76837e-08
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --download-parmetis --download-pnetcdf --download-scalapack --with-zlib --download-netcdf --with-scalar-type=complex --download-hdf5 --download-mumps --download-exodusii --with-fortran-bindings=0 --download-chaco --download-metis
-----------------------------------------
Libraries compiled on 2019-03-15 16:36:05 on cranmer 
Machine characteristics: Linux-4.15.0-46-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lpthread -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lpthread -lstdc++ -ldl -lmpichfort -lmpich -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Thu May 23 12:18:12 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000 Thu May 23 12:18:29 2019
run-nbpc-qmc.py on a , 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-nbpc-qmc.py on a  named cranmer with 1 processor, by owen Thu May 23 12:19:14 2019
Using Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000

                         Max       Max/Min     Avg       Total 
Time (sec):           4.493e+01     1.000   4.493e+01
Objects:              3.910e+02     1.000   3.910e+02
Flop:                 1.544e+10     1.000   1.544e+10  1.544e+10
Flop/sec:             3.435e+08     1.000   3.435e+08  3.435e+08
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 4.4935e+01 100.0%  1.5435e+10 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSidedF         3 1.0 4.6492e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             3 1.0 3.0994e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             3638 1.0 2.6497e-02 1.0 4.15e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1566
VecNorm             4214 1.0 7.3817e-03 1.0 1.24e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1685
VecScale            4214 1.0 3.4158e-03 1.0 6.23e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1823
VecCopy             4032 1.0 3.2637e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1215 1.0 9.1004e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY             1152 1.0 1.5159e-03 1.0 3.34e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2203
VecMAXPY            4214 1.0 3.3734e-02 1.0 5.23e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1552
VecNormalize        4214 1.0 1.2416e-02 1.0 1.87e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1503
MatMult             3638 1.0 5.4816e-02 1.0 6.57e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1199
MatSolve            4214 1.0 2.5733e-01 1.0 3.23e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0  1254
MatLUFactorSym         3 1.0 5.6171e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum         3 1.0 1.7002e-03 1.0 1.59e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   935
MatAssemblyBegin    2313 1.0 3.2449e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd      2313 1.0 1.2114e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            3 1.0 4.6968e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         3 1.0 2.9731e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries      1152 1.0 3.5968e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp                3 1.0 2.6708e-03 1.0 1.59e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   595
PCApply             4214 1.0 2.5871e-01 1.0 3.23e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0  1248
KSPSetUp             576 1.0 2.9945e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve             576 1.0 4.0390e-01 1.0 5.04e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0  1249
KSPGMRESOrthog      3638 1.0 5.5869e-02 1.0 8.31e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  1487
SNESSolve            576 1.0 4.0315e+01 1.0 1.54e+10 1.0 0.0e+00 0.0e+00 0.0e+00 90100  0  0  0  90100  0  0  0   383
SNESFunctionEval     576 1.0 1.3799e+01 1.0 4.27e+09 1.0 0.0e+00 0.0e+00 0.0e+00 31 28  0  0  0  31 28  0  0  0   310
SNESJacobianEval     576 1.0 2.6099e+01 1.0 1.07e+10 1.0 0.0e+00 0.0e+00 0.0e+00 58 69  0  0  0  58 69  0  0  0   408
DMPlexInterp           3 1.0 2.6021e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexStratify         6 1.0 1.6508e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateMesh            12 1.0 5.4271e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateFunctionSpace       3 1.0 7.7218e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: reorder          3 1.0 4.9901e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        3 1.0 2.3396e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateSparsity         3 1.0 1.9054e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroInitial         6 1.0 1.1046e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopExecute      4032 1.0 3.9719e+01 1.0 1.49e+10 1.0 0.0e+00 0.0e+00 0.0e+00 88 97  0  0  0  88 97  0  0  0   376
ParLoopset_1         128 1.0 2.2922e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednBegin    4032 1.0 4.9613e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd      4032 1.0 6.0322e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells        3456 1.0 3.6021e+01 1.0 1.49e+10 1.0 0.0e+00 0.0e+00 0.0e+00 80 96  0  0  0  80 96  0  0  0   413
ParLoopExtFacets    3456 1.0 5.5977e-02 1.0 4.96e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   887
ParLoopset_5         512 1.0 5.9803e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopset_9         512 1.0 6.3486e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
           Index Set   120            120       212028     0.
   IS L to G Mapping     3              3         5784     0.
             Section    45             45        32400     0.
   Star Forest Graph    42             42        33984     0.
              Vector    69             69       467712     0.
              Matrix    18             18       819496     0.
      Preconditioner     3              3         3000     0.
       Krylov Solver     3              3       105048     0.
     DMKSP interface     3              3         1968     0.
                SNES     3              3         4212     0.
              DMSNES     3              3         2016     0.
      SNESLineSearch     3              3         2976     0.
    Distributed Mesh    21             21       104112     0.
            DM Label    27             27        16848     0.
    GraphPartitioner     6              6         3720     0.
     Discrete System    21             21        19488     0.
========================================================================================================================
Average time to get PetscTime(): 4.76837e-08
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --download-parmetis --download-pnetcdf --download-scalapack --with-zlib --download-netcdf --with-scalar-type=complex --download-hdf5 --download-mumps --download-exodusii --with-fortran-bindings=0 --download-chaco --download-metis
-----------------------------------------
Libraries compiled on 2019-03-15 16:36:05 on cranmer 
Machine characteristics: Linux-4.15.0-46-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lpthread -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lpthread -lstdc++ -ldl -lmpichfort -lmpich -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Thu May 23 12:19:14 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000 Thu May 23 12:27:35 2019
run-nbpc-qmc.py on a , 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-nbpc-qmc.py on a  named cranmer with 1 processor, by owen Thu May 23 12:27:39 2019
Using Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000

                         Max       Max/Min     Avg       Total 
Time (sec):           3.623e+00     1.000   3.623e+00
Objects:              1.310e+02     1.000   1.310e+02
Flop:                 7.530e+08     1.000   7.530e+08  7.530e+08
Flop/sec:             2.078e+08     1.000   2.078e+08  2.078e+08
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 3.6231e+00 100.0%  7.5300e+08 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSidedF         1 1.0 2.1458e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             1 1.0 7.1526e-07 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              347 1.0 1.0629e-03 1.0 1.55e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1457
VecNorm              411 1.0 3.6693e-04 1.0 5.54e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1510
VecScale             411 1.0 2.1291e-04 1.0 2.78e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1305
VecCopy              448 1.0 2.3150e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               149 1.0 7.7963e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              128 1.0 1.1754e-04 1.0 1.73e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1472
VecMAXPY             411 1.0 1.3549e-03 1.0 2.02e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1493
VecNormalize         411 1.0 7.3934e-04 1.0 8.32e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1125
MatMult              347 1.0 2.3389e-03 1.0 2.77e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1183
MatSolve             411 1.0 9.0942e-03 1.0 1.10e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  1208
MatLUFactorSym         1 1.0 1.0920e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum         1 1.0 2.1386e-04 1.0 1.62e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   757
MatAssemblyBegin     259 1.0 5.7459e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd       259 1.0 7.4124e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            1 1.0 1.6928e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         1 1.0 6.5804e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries       128 1.0 1.9836e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp                1 1.0 4.2152e-04 1.0 1.62e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   384
PCApply              411 1.0 9.2342e-03 1.0 1.10e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  1189
KSPSetUp              64 1.0 5.7936e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              64 1.0 1.6562e-02 1.0 1.84e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  1111
KSPGMRESOrthog       347 1.0 2.3022e-03 1.0 3.10e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1348
SNESSolve             64 1.0 1.9096e+00 1.0 7.53e+08 1.0 0.0e+00 0.0e+00 0.0e+00 53100  0  0  0  53100  0  0  0   394
SNESFunctionEval      64 1.0 6.6150e-01 1.0 2.10e+08 1.0 0.0e+00 0.0e+00 0.0e+00 18 28  0  0  0  18 28  0  0  0   318
SNESJacobianEval      64 1.0 1.2302e+00 1.0 5.24e+08 1.0 0.0e+00 0.0e+00 0.0e+00 34 70  0  0  0  34 70  0  0  0   426
DMPlexInterp           1 1.0 4.9853e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexStratify         2 1.0 3.1614e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateMesh             4 1.0 1.4944e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateFunctionSpace       1 1.0 8.2550e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0
Mesh: reorder          1 1.0 1.0991e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        1 1.0 5.7673e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateSparsity         1 1.0 4.6587e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroInitial         2 1.0 2.3699e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopExecute       448 1.0 1.8694e+00 1.0 7.35e+08 1.0 0.0e+00 0.0e+00 0.0e+00 52 98  0  0  0  52 98  0  0  0   393
ParLoopset_1         128 1.0 2.3177e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednBegin     448 1.0 5.3120e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd       448 1.0 6.3586e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells         384 1.0 1.7938e+00 1.0 7.31e+08 1.0 0.0e+00 0.0e+00 0.0e+00 50 97  0  0  0  50 97  0  0  0   407
ParLoopExtFacets     384 1.0 6.7368e-03 1.0 3.72e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   553
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
           Index Set    40             40        52160     0.
   IS L to G Mapping     1              1         1348     0.
             Section    15             15        10800     0.
   Star Forest Graph    14             14        11328     0.
              Vector    23             23       100224     0.
              Matrix     6              6       136652     0.
      Preconditioner     1              1         1000     0.
       Krylov Solver     1              1        35016     0.
     DMKSP interface     1              1          656     0.
                SNES     1              1         1404     0.
              DMSNES     1              1          672     0.
      SNESLineSearch     1              1          992     0.
    Distributed Mesh     7              7        34704     0.
            DM Label     9              9         5616     0.
    GraphPartitioner     2              2         1240     0.
     Discrete System     7              7         6496     0.
========================================================================================================================
Average time to get PetscTime(): 4.76837e-08
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --download-parmetis --download-pnetcdf --download-scalapack --with-zlib --download-netcdf --with-scalar-type=complex --download-hdf5 --download-mumps --download-exodusii --with-fortran-bindings=0 --download-chaco --download-metis
-----------------------------------------
Libraries compiled on 2019-03-15 16:36:05 on cranmer 
Machine characteristics: Linux-4.15.0-46-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lpthread -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lpthread -lstdc++ -ldl -lmpichfort -lmpich -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Thu May 23 12:27:39 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000 Thu May 23 12:27:56 2019
run-nbpc-qmc.py on a , 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-nbpc-qmc.py on a  named cranmer with 1 processor, by owen Thu May 23 12:28:00 2019
Using Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000

                         Max       Max/Min     Avg       Total 
Time (sec):           3.639e+00     1.000   3.639e+00
Objects:              1.310e+02     1.000   1.310e+02
Flop:                 7.530e+08     1.000   7.530e+08  7.530e+08
Flop/sec:             2.069e+08     1.000   2.069e+08  2.069e+08
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 3.6391e+00 100.0%  7.5300e+08 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSidedF         1 1.0 2.1935e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             1 1.0 7.1526e-07 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              347 1.0 1.0984e-03 1.0 1.55e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1410
VecNorm              411 1.0 3.7026e-04 1.0 5.54e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1496
VecScale             411 1.0 2.1052e-04 1.0 2.78e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1320
VecCopy              448 1.0 2.6441e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               149 1.0 7.1526e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              128 1.0 1.2231e-04 1.0 1.73e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1415
VecMAXPY             411 1.0 1.3683e-03 1.0 2.02e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1478
VecNormalize         411 1.0 7.4220e-04 1.0 8.32e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1121
MatMult              347 1.0 2.3887e-03 1.0 2.77e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1158
MatSolve             411 1.0 9.2921e-03 1.0 1.10e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  1182
MatLUFactorSym         1 1.0 1.1301e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum         1 1.0 2.1720e-04 1.0 1.62e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   746
MatAssemblyBegin     259 1.0 5.9128e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd       259 1.0 7.5293e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            1 1.0 1.5259e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         1 1.0 6.3658e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries       128 1.0 1.9526e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp                1 1.0 4.2701e-04 1.0 1.62e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   379
PCApply              411 1.0 9.4237e-03 1.0 1.10e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  1166
KSPSetUp              64 1.0 6.0558e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              64 1.0 1.6900e-02 1.0 1.84e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  1089
KSPGMRESOrthog       347 1.0 2.3627e-03 1.0 3.10e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1313
SNESSolve             64 1.0 1.9164e+00 1.0 7.53e+08 1.0 0.0e+00 0.0e+00 0.0e+00 53100  0  0  0  53100  0  0  0   393
SNESFunctionEval      64 1.0 6.6224e-01 1.0 2.10e+08 1.0 0.0e+00 0.0e+00 0.0e+00 18 28  0  0  0  18 28  0  0  0   317
SNESJacobianEval      64 1.0 1.2359e+00 1.0 5.24e+08 1.0 0.0e+00 0.0e+00 0.0e+00 34 70  0  0  0  34 70  0  0  0   424
DMPlexInterp           1 1.0 5.3525e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexStratify         2 1.0 3.1781e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateMesh             4 1.0 1.4999e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateFunctionSpace       1 1.0 8.5507e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0
Mesh: reorder          1 1.0 1.0443e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        1 1.0 5.9247e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateSparsity         1 1.0 4.7135e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroInitial         2 1.0 2.3770e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopExecute       448 1.0 1.8755e+00 1.0 7.35e+08 1.0 0.0e+00 0.0e+00 0.0e+00 52 98  0  0  0  52 98  0  0  0   392
ParLoopset_1         128 1.0 2.8183e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednBegin     448 1.0 5.7626e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd       448 1.0 6.7043e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells         384 1.0 1.7975e+00 1.0 7.31e+08 1.0 0.0e+00 0.0e+00 0.0e+00 49 97  0  0  0  49 97  0  0  0   407
ParLoopExtFacets     384 1.0 6.7699e-03 1.0 3.72e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   550
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
           Index Set    40             40        52160     0.
   IS L to G Mapping     1              1         1348     0.
             Section    15             15        10800     0.
   Star Forest Graph    14             14        11328     0.
              Vector    23             23       100224     0.
              Matrix     6              6       136652     0.
      Preconditioner     1              1         1000     0.
       Krylov Solver     1              1        35016     0.
     DMKSP interface     1              1          656     0.
                SNES     1              1         1404     0.
              DMSNES     1              1          672     0.
      SNESLineSearch     1              1          992     0.
    Distributed Mesh     7              7        34704     0.
            DM Label     9              9         5616     0.
    GraphPartitioner     2              2         1240     0.
     Discrete System     7              7         6496     0.
========================================================================================================================
Average time to get PetscTime(): 4.76837e-08
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --download-parmetis --download-pnetcdf --download-scalapack --with-zlib --download-netcdf --with-scalar-type=complex --download-hdf5 --download-mumps --download-exodusii --with-fortran-bindings=0 --download-chaco --download-metis
-----------------------------------------
Libraries compiled on 2019-03-15 16:36:05 on cranmer 
Machine characteristics: Linux-4.15.0-46-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lpthread -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lpthread -lstdc++ -ldl -lmpichfort -lmpich -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Thu May 23 12:28:00 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000 Thu May 23 12:28:13 2019
run-nbpc-qmc.py on a , 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-nbpc-qmc.py on a  named cranmer with 1 processor, by owen Thu May 23 12:28:54 2019
Using Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000

                         Max       Max/Min     Avg       Total 
Time (sec):           4.065e+01     1.000   4.065e+01
Objects:              3.910e+02     1.000   3.910e+02
Flop:                 1.544e+10     1.000   1.544e+10  1.544e+10
Flop/sec:             3.797e+08     1.000   3.797e+08  3.797e+08
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 4.0653e+01 100.0%  1.5435e+10 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSidedF         3 1.0 6.3181e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             3 1.0 3.0994e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             3638 1.0 2.6601e-02 1.0 4.15e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1560
VecNorm             4214 1.0 7.3919e-03 1.0 1.24e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1683
VecScale            4214 1.0 3.3472e-03 1.0 6.23e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1860
VecCopy             4032 1.0 3.5055e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1215 1.0 9.1577e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY             1152 1.0 1.5395e-03 1.0 3.34e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2169
VecMAXPY            4214 1.0 3.3754e-02 1.0 5.23e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1551
VecNormalize        4214 1.0 1.2427e-02 1.0 1.87e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1502
MatMult             3638 1.0 5.5022e-02 1.0 6.57e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1195
MatSolve            4214 1.0 2.5757e-01 1.0 3.23e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0  1253
MatLUFactorSym         3 1.0 6.0654e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum         3 1.0 1.7254e-03 1.0 1.59e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   922
MatAssemblyBegin    2313 1.0 3.2020e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd      2313 1.0 1.2160e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            3 1.0 4.4823e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         3 1.0 2.9182e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries      1152 1.0 3.4068e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp                3 1.0 2.7127e-03 1.0 1.59e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   586
PCApply             4214 1.0 2.5902e-01 1.0 3.23e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0  1246
KSPSetUp             576 1.0 2.9778e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve             576 1.0 4.0531e-01 1.0 5.04e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0  1244
KSPGMRESOrthog      3638 1.0 5.6005e-02 1.0 8.31e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  1483
SNESSolve            576 1.0 3.7109e+01 1.0 1.54e+10 1.0 0.0e+00 0.0e+00 0.0e+00 91100  0  0  0  91100  0  0  0   416
SNESFunctionEval     576 1.0 1.1976e+01 1.0 4.27e+09 1.0 0.0e+00 0.0e+00 0.0e+00 29 28  0  0  0  29 28  0  0  0   357
SNESJacobianEval     576 1.0 2.4714e+01 1.0 1.07e+10 1.0 0.0e+00 0.0e+00 0.0e+00 61 69  0  0  0  61 69  0  0  0   431
DMPlexInterp           3 1.0 2.7022e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexStratify         6 1.0 1.6766e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateMesh            12 1.0 5.7008e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateFunctionSpace       3 1.0 9.1759e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: reorder          3 1.0 5.5027e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        3 1.0 2.4216e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateSparsity         3 1.0 1.8930e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroInitial         6 1.0 1.1215e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopExecute      4032 1.0 3.6503e+01 1.0 1.49e+10 1.0 0.0e+00 0.0e+00 0.0e+00 90 97  0  0  0  90 97  0  0  0   409
ParLoopset_1         128 1.0 2.3170e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednBegin    4032 1.0 5.0199e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd      4032 1.0 6.3920e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells        3456 1.0 3.6193e+01 1.0 1.49e+10 1.0 0.0e+00 0.0e+00 0.0e+00 89 96  0  0  0  89 96  0  0  0   411
ParLoopExtFacets    3456 1.0 5.4116e-02 1.0 4.96e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   917
ParLoopset_5         512 1.0 5.6953e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopset_9         512 1.0 6.1343e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
           Index Set   120            120       212028     0.
   IS L to G Mapping     3              3         5784     0.
             Section    45             45        32400     0.
   Star Forest Graph    42             42        33984     0.
              Vector    69             69       467712     0.
              Matrix    18             18       819496     0.
      Preconditioner     3              3         3000     0.
       Krylov Solver     3              3       105048     0.
     DMKSP interface     3              3         1968     0.
                SNES     3              3         4212     0.
              DMSNES     3              3         2016     0.
      SNESLineSearch     3              3         2976     0.
    Distributed Mesh    21             21       104112     0.
            DM Label    27             27        16848     0.
    GraphPartitioner     6              6         3720     0.
     Discrete System    21             21        19488     0.
========================================================================================================================
Average time to get PetscTime(): 4.76837e-08
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --download-parmetis --download-pnetcdf --download-scalapack --with-zlib --download-netcdf --with-scalar-type=complex --download-hdf5 --download-mumps --download-exodusii --with-fortran-bindings=0 --download-chaco --download-metis
-----------------------------------------
Libraries compiled on 2019-03-15 16:36:05 on cranmer 
Machine characteristics: Linux-4.15.0-46-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lpthread -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lpthread -lstdc++ -ldl -lmpichfort -lmpich -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Thu May 23 12:28:54 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000 Thu May 23 12:46:41 2019
run-nbpc-qmc.py on a , 2 proc. with options:
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000 Thu May 23 12:46:56 2019
run-nbpc-qmc.py on a , 2 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-nbpc-qmc.py on a  named cranmer with 2 processors, by owen Thu May 23 12:47:09 2019
Using Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000

                         Max       Max/Min     Avg       Total 
Time (sec):           1.253e+01     1.000   1.253e+01
Objects:              7.250e+02     1.114   6.880e+02
Flop:                 4.113e+09     1.298   3.640e+09  7.280e+09
Flop/sec:             3.282e+08     1.298   2.905e+08  5.811e+08
MPI Messages:         5.055e+03     1.066   4.898e+03  9.797e+03
MPI Message Lengths:  5.767e+06     1.053   1.148e+03  1.124e+07
MPI Reductions:       6.636e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 1.2529e+01 100.0%  7.2804e+09 100.0%  9.797e+03 100.0%  1.148e+03      100.0%  6.629e+03  99.9% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided         82 1.0 1.9894e-03 2.4 0.00e+00 0.0 6.2e+01 4.0e+00 0.0e+00  0  0  1  0  0   0  0  1  0  0     0
BuildTwoSidedF      1286 1.0 6.4223e-0141.8 0.00e+00 0.0 9.7e+02 1.2e+03 0.0e+00  3  0 10 11  0   3  0 10 11  0     0
SFSetGraph            86 1.0 5.5790e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp               82 1.0 3.0158e-03 1.6 0.00e+00 0.0 1.9e+02 3.9e+02 0.0e+00  0  0  2  1  0   0  0  2  1  0     0
SFBcastBegin         486 1.0 2.9142e-03 1.3 0.00e+00 0.0 8.9e+02 4.9e+02 0.0e+00  0  0  9  4  0   0  0  9  4  0     0
SFBcastEnd           486 1.0 1.6365e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceBegin        648 1.0 1.3766e-03 1.2 0.00e+00 0.0 1.3e+03 3.8e+02 0.0e+00  0  0 13  4  0   0  0 13  4  0     0
SFReduceEnd          648 1.0 4.1152e-01304.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0
SFFetchOpBegin         2 1.0 1.4305e-06 1.0 0.00e+00 0.0 2.0e+00 1.2e+02 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFFetchOpEnd           2 1.0 5.0068e-06 1.4 0.00e+00 0.0 2.0e+00 1.2e+02 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             1903 1.0 1.1089e-02 1.1 7.72e+06 1.1 0.0e+00 0.0e+00 1.9e+03  0  0  0  0 29   0  0  0  0 29  1313
VecNorm             2223 1.0 7.1721e-03 1.1 2.52e+06 1.1 0.0e+00 0.0e+00 2.2e+03  0  0  0  0 33   0  0  0  0 34   662
VecScale            2223 1.0 1.3268e-03 1.1 1.26e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1788
VecCopy             2240 1.0 1.4791e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               652 1.0 4.0030e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              640 1.0 5.2977e-04 1.0 7.20e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2561
VecMAXPY            2223 1.0 7.4496e-03 1.1 9.91e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2508
VecScatterBegin     6349 1.0 8.6792e-03 1.2 0.00e+00 0.0 6.4e+03 1.4e+03 2.2e+03  0  0 66 80 33   0  0 66 80 34     0
VecScatterEnd       4126 1.0 4.1604e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize        2223 1.0 9.4869e-03 1.0 3.78e+06 1.1 0.0e+00 0.0e+00 2.2e+03  0  0  0  0 33   0  0  0  0 34   750
MatMult             1903 1.0 1.6541e-02 1.1 1.30e+07 1.1 3.8e+03 2.6e+02 0.0e+00  0  0 39  9  0   0  0 39  9  0  1480
MatSolve            2223 1.0 8.5348e-01 1.0 1.10e+09 6.8 2.6e+03 3.0e+03 2.2e+03  7 17 27 72 34   7 17 27 72 34  1480
MatLUFactorSym         2 1.0 1.1518e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum         2 1.0 2.2357e-03 1.0 3.85e+04 3.2 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0    23
MatAssemblyBegin    1286 1.0 6.6056e-0128.1 0.00e+00 0.0 9.7e+02 1.2e+03 0.0e+00  3  0 10 11  0   3  0 10 11  0     0
MatAssemblyEnd      1286 1.0 1.3357e-02 1.5 0.00e+00 0.0 1.6e+01 3.2e+01 3.2e+01  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            2 1.0 7.1526e-07 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 3.0041e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries       640 1.0 1.3955e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp                2 1.0 3.6469e-03 1.0 3.85e+04 3.2 0.0e+00 0.0e+00 2.2e+01  0  0  0  0  0   0  0  0  0  0    14
PCApply             2223 1.0 8.5447e-01 1.0 1.10e+09 6.8 2.6e+03 3.0e+03 2.2e+03  7 17 27 72 34   7 17 27 72 34  1478
KSPSetUp             320 1.0 1.7405e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve             320 1.0 9.1013e-01 1.0 1.14e+09 5.9 6.4e+03 1.4e+03 6.4e+03  7 18 66 80 96   7 18 66 80 96  1460
KSPGMRESOrthog      1903 1.0 1.8026e-02 1.0 1.55e+07 1.1 0.0e+00 0.0e+00 1.9e+03  0  0  0  0 29   0  0  0  0 29  1618
SNESSolve            320 1.0 9.6013e+00 1.0 4.11e+09 1.3 9.3e+03 1.2e+03 6.4e+03 77100 95 97 96  77100 95 97 96   758
SNESFunctionEval     320 1.0 2.9865e+00 1.0 8.51e+08 1.0 1.9e+03 3.7e+02 4.0e+00 24 23 20  6  0  24 23 20  6  0   570
SNESJacobianEval     320 1.0 5.6970e+00 1.0 2.12e+09 1.0 9.6e+02 1.2e+03 0.0e+00 45 58 10 11  0  45 58 10 11  0   746
Mesh Partition         4 1.0 4.1661e-03 1.1 0.00e+00 0.0 1.1e+02 3.2e+02 2.0e+01  0  0  1  0  0   0  0  1  0  0     0
Mesh Migration         4 1.0 4.3700e-03 1.0 0.00e+00 0.0 3.0e+02 7.2e+02 1.3e+02  0  0  3  2  2   0  0  3  2  2     0
DMPlexInterp           2 1.0 1.4930e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexDistribute       2 1.0 5.1024e-03 1.0 0.00e+00 0.0 1.3e+02 1.6e+03 5.8e+01  0  0  1  2  1   0  0  1  2  1     0
DMPlexDistCones        4 1.0 7.9918e-04 1.0 0.00e+00 0.0 4.8e+01 1.3e+03 0.0e+00  0  0  0  1  0   0  0  0  1  0     0
DMPlexDistLabels       4 1.0 2.0797e-03 1.0 0.00e+00 0.0 1.7e+02 6.4e+02 9.2e+01  0  0  2  1  1   0  0  2  1  1     0
DMPlexDistOvrlp        2 1.0 3.5629e-03 1.1 0.00e+00 0.0 2.8e+02 1.8e+02 8.8e+01  0  0  3  0  1   0  0  3  0  1     0
DMPlexDistField        6 1.0 4.0698e-04 1.0 0.00e+00 0.0 6.2e+01 4.2e+02 4.0e+00  0  0  1  0  0   0  0  1  0  0     0
DMPlexDistData         4 1.0 1.1115e-03 3.3 0.00e+00 0.0 6.3e+01 3.1e+02 0.0e+00  0  0  1  0  0   0  0  1  0  0     0
DMPlexStratify         8 1.0 2.4877e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 8.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateMesh             8 1.0 1.3176e-02 1.1 0.00e+00 0.0 4.2e+02 6.5e+02 1.5e+02  0  0  4  2  2   0  0  4  2  2     0
CreateFunctionSpace       2 1.0 9.8011e-02 1.0 0.00e+00 0.0 3.0e+02 1.9e+02 1.0e+02  1  0  3  0  2   1  0  3  0  2     0
Mesh: reorder          2 1.0 2.3103e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        2 1.0 1.3900e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateSparsity         2 1.0 1.3347e-03 1.1 0.00e+00 0.0 1.1e+01 5.2e+02 1.2e+01  0  0  0  0  0   0  0  0  0  0     0
MatZeroInitial         4 1.0 6.1369e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopExecute      2240 1.0 8.5347e+00 1.1 2.98e+09 1.0 1.9e+03 3.7e+02 4.0e+00 66 82 20  6  0  66 82 20  6  0   697
ParLoopset_1         128 1.0 3.1855e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednBegin    2240 1.0 2.9933e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd      2240 1.0 3.5014e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells        1920 1.0 8.3002e+00 1.1 2.96e+09 1.0 0.0e+00 0.0e+00 0.0e+00 62 81  0  0  0  62 81  0  0  0   714
ParLoopExtFacets    1920 1.0 2.9254e-02 1.0 1.18e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   806
ParLoopset_5         512 1.0 7.2086e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
           Index Set   270            270       317856     0.
   IS L to G Mapping     8              8        42128     0.
             Section   116            116        83520     0.
   Star Forest Graph   124            124       104576     0.
              Vector    64             64       230320     0.
         Vec Scatter     8              8        10000     0.
              Matrix    24             24       160256     0.
      Preconditioner     2              2         2000     0.
       Krylov Solver     2              2        70032     0.
     DMKSP interface     2              2         1312     0.
                SNES     2              2         2808     0.
              DMSNES     2              2         1344     0.
      SNESLineSearch     2              2         1984     0.
    Distributed Mesh    22             22       109728     0.
            DM Label    46             46        28704     0.
    GraphPartitioner     8              8         4960     0.
     Discrete System    22             22        20416     0.
========================================================================================================================
Average time to get PetscTime(): 4.76837e-08
Average time for MPI_Barrier(): 1.43051e-06
Average time for zero size MPI_Send(): 1.3113e-06
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --download-parmetis --download-pnetcdf --download-scalapack --with-zlib --download-netcdf --with-scalar-type=complex --download-hdf5 --download-mumps --download-exodusii --with-fortran-bindings=0 --download-chaco --download-metis
-----------------------------------------
Libraries compiled on 2019-03-15 16:36:05 on cranmer 
Machine characteristics: Linux-4.15.0-46-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lpthread -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lpthread -lstdc++ -ldl -lmpichfort -lmpich -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Thu May 23 12:47:09 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000 Thu May 23 12:50:19 2019
run-nbpc-qmc.py on a , 2 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-nbpc-qmc.py on a  named cranmer with 2 processors, by owen Thu May 23 12:50:32 2019
Using Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000

                         Max       Max/Min     Avg       Total 
Time (sec):           1.245e+01     1.000   1.245e+01
Objects:              7.250e+02     1.114   6.880e+02
Flop:                 4.113e+09     1.298   3.640e+09  7.280e+09
Flop/sec:             3.304e+08     1.298   2.924e+08  5.848e+08
MPI Messages:         5.055e+03     1.066   4.898e+03  9.797e+03
MPI Message Lengths:  5.767e+06     1.053   1.148e+03  1.124e+07
MPI Reductions:       6.636e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 1.2448e+01 100.0%  7.2804e+09 100.0%  9.797e+03 100.0%  1.148e+03      100.0%  6.629e+03  99.9% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided         82 1.0 2.0981e-03 2.6 0.00e+00 0.0 6.2e+01 4.0e+00 0.0e+00  0  0  1  0  0   0  0  1  0  0     0
BuildTwoSidedF      1286 1.0 6.3793e-0141.2 0.00e+00 0.0 9.7e+02 1.2e+03 0.0e+00  3  0 10 11  0   3  0 10 11  0     0
SFSetGraph            86 1.0 5.5313e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp               82 1.0 3.1166e-03 1.7 0.00e+00 0.0 1.9e+02 3.9e+02 0.0e+00  0  0  2  1  0   0  0  2  1  0     0
SFBcastBegin         486 1.0 2.7223e-03 1.1 0.00e+00 0.0 8.9e+02 4.9e+02 0.0e+00  0  0  9  4  0   0  0  9  4  0     0
SFBcastEnd           486 1.0 2.7163e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceBegin        648 1.0 1.3595e-03 1.1 0.00e+00 0.0 1.3e+03 3.8e+02 0.0e+00  0  0 13  4  0   0  0 13  4  0     0
SFReduceEnd          648 1.0 3.0026e-01238.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
SFFetchOpBegin         2 1.0 1.4305e-06 1.2 0.00e+00 0.0 2.0e+00 1.2e+02 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFFetchOpEnd           2 1.0 4.0531e-06 1.1 0.00e+00 0.0 2.0e+00 1.2e+02 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             1903 1.0 1.2056e-02 1.2 7.72e+06 1.1 0.0e+00 0.0e+00 1.9e+03  0  0  0  0 29   0  0  0  0 29  1208
VecNorm             2223 1.0 6.8965e-03 1.1 2.52e+06 1.1 0.0e+00 0.0e+00 2.2e+03  0  0  0  0 33   0  0  0  0 34   688
VecScale            2223 1.0 1.3218e-03 1.1 1.26e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1795
VecCopy             2240 1.0 1.3969e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               652 1.0 3.7575e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              640 1.0 5.4812e-04 1.1 7.20e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2475
VecMAXPY            2223 1.0 7.3640e-03 1.1 9.91e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2537
VecScatterBegin     6349 1.0 9.0258e-03 1.2 0.00e+00 0.0 6.4e+03 1.4e+03 2.2e+03  0  0 66 80 33   0  0 66 80 34     0
VecScatterEnd       4126 1.0 4.5118e-03 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize        2223 1.0 9.0570e-03 1.0 3.78e+06 1.1 0.0e+00 0.0e+00 2.2e+03  0  0  0  0 33   0  0  0  0 34   786
MatMult             1903 1.0 1.6832e-02 1.1 1.30e+07 1.1 3.8e+03 2.6e+02 0.0e+00  0  0 39  9  0   0  0 39  9  0  1454
MatSolve            2223 1.0 8.6070e-01 1.0 1.10e+09 6.8 2.6e+03 3.0e+03 2.2e+03  7 17 27 72 34   7 17 27 72 34  1467
MatLUFactorSym         2 1.0 1.0059e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum         2 1.0 1.9314e-03 1.0 3.85e+04 3.2 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0    26
MatAssemblyBegin    1286 1.0 6.5635e-0127.8 0.00e+00 0.0 9.7e+02 1.2e+03 0.0e+00  3  0 10 11  0   3  0 10 11  0     0
MatAssemblyEnd      1286 1.0 1.3324e-02 1.5 0.00e+00 0.0 1.6e+01 3.2e+01 3.2e+01  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            2 1.0 5.0068e-0610.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.8372e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries       640 1.0 1.3950e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp                2 1.0 3.1781e-03 1.0 3.85e+04 3.2 0.0e+00 0.0e+00 2.2e+01  0  0  0  0  0   0  0  0  0  0    16
PCApply             2223 1.0 8.6166e-01 1.0 1.10e+09 6.8 2.6e+03 3.0e+03 2.2e+03  7 17 27 72 34   7 17 27 72 34  1466
KSPSetUp             320 1.0 1.4567e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve             320 1.0 9.1710e-01 1.0 1.14e+09 5.9 6.4e+03 1.4e+03 6.4e+03  7 18 66 80 96   7 18 66 80 96  1449
KSPGMRESOrthog      1903 1.0 1.8919e-02 1.1 1.55e+07 1.1 0.0e+00 0.0e+00 1.9e+03  0  0  0  0 29   0  0  0  0 29  1542
SNESSolve            320 1.0 9.4799e+00 1.0 4.11e+09 1.3 9.3e+03 1.2e+03 6.4e+03 76100 95 97 96  76100 95 97 96   768
SNESFunctionEval     320 1.0 2.8727e+00 1.0 8.51e+08 1.0 1.9e+03 3.7e+02 4.0e+00 23 23 20  6  0  23 23 20  6  0   593
SNESJacobianEval     320 1.0 5.6854e+00 1.0 2.12e+09 1.0 9.6e+02 1.2e+03 0.0e+00 46 58 10 11  0  46 58 10 11  0   747
Mesh Partition         4 1.0 4.3051e-03 1.2 0.00e+00 0.0 1.1e+02 3.2e+02 2.0e+01  0  0  1  0  0   0  0  1  0  0     0
Mesh Migration         4 1.0 4.3731e-03 1.0 0.00e+00 0.0 3.0e+02 7.2e+02 1.3e+02  0  0  3  2  2   0  0  3  2  2     0
DMPlexInterp           2 1.0 1.4653e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexDistribute       2 1.0 5.0492e-03 1.0 0.00e+00 0.0 1.3e+02 1.6e+03 5.8e+01  0  0  1  2  1   0  0  1  2  1     0
DMPlexDistCones        4 1.0 7.9918e-04 1.0 0.00e+00 0.0 4.8e+01 1.3e+03 0.0e+00  0  0  0  1  0   0  0  0  1  0     0
DMPlexDistLabels       4 1.0 2.0401e-03 1.0 0.00e+00 0.0 1.7e+02 6.4e+02 9.2e+01  0  0  2  1  1   0  0  2  1  1     0
DMPlexDistOvrlp        2 1.0 3.7584e-03 1.2 0.00e+00 0.0 2.8e+02 1.8e+02 8.8e+01  0  0  3  0  1   0  0  3  0  1     0
DMPlexDistField        6 1.0 4.1413e-04 1.0 0.00e+00 0.0 6.2e+01 4.2e+02 4.0e+00  0  0  1  0  0   0  0  1  0  0     0
DMPlexDistData         4 1.0 1.0724e-03 3.2 0.00e+00 0.0 6.3e+01 3.1e+02 0.0e+00  0  0  1  0  0   0  0  1  0  0     0
DMPlexStratify         8 1.0 2.4617e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 8.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateMesh             8 1.0 1.3266e-02 1.1 0.00e+00 0.0 4.2e+02 6.5e+02 1.5e+02  0  0  4  2  2   0  0  4  2  2     0
CreateFunctionSpace       2 1.0 1.5317e-01 1.0 0.00e+00 0.0 3.0e+02 1.9e+02 1.0e+02  1  0  3  0  2   1  0  3  0  2     0
Mesh: reorder          2 1.0 2.2793e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        2 1.0 1.3785e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateSparsity         2 1.0 2.2078e-03 2.1 0.00e+00 0.0 1.1e+01 5.2e+02 1.2e+01  0  0  0  0  0   0  0  0  0  0     0
MatZeroInitial         4 1.0 4.4179e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopExecute      2240 1.0 8.4197e+00 1.1 2.98e+09 1.0 1.9e+03 3.7e+02 4.0e+00 65 82 20  6  0  65 82 20  6  0   707
ParLoopset_1         128 1.0 2.6712e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednBegin    2240 1.0 3.0661e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd      2240 1.0 3.5250e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells        1920 1.0 8.2033e+00 1.1 2.96e+09 1.0 0.0e+00 0.0e+00 0.0e+00 62 81  0  0  0  62 81  0  0  0   723
ParLoopExtFacets    1920 1.0 2.7349e-02 1.1 1.18e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   862
ParLoopset_5         512 1.0 6.8219e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
           Index Set   270            270       317856     0.
   IS L to G Mapping     8              8        42128     0.
             Section   116            116        83520     0.
   Star Forest Graph   124            124       104576     0.
              Vector    64             64       230320     0.
         Vec Scatter     8              8        10000     0.
              Matrix    24             24       160256     0.
      Preconditioner     2              2         2000     0.
       Krylov Solver     2              2        70032     0.
     DMKSP interface     2              2         1312     0.
                SNES     2              2         2808     0.
              DMSNES     2              2         1344     0.
      SNESLineSearch     2              2         1984     0.
    Distributed Mesh    22             22       109728     0.
            DM Label    46             46        28704     0.
    GraphPartitioner     8              8         4960     0.
     Discrete System    22             22        20416     0.
========================================================================================================================
Average time to get PetscTime(): 2.38419e-08
Average time for MPI_Barrier(): 1.57356e-06
Average time for zero size MPI_Send(): 1.3113e-06
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --download-parmetis --download-pnetcdf --download-scalapack --with-zlib --download-netcdf --with-scalar-type=complex --download-hdf5 --download-mumps --download-exodusii --with-fortran-bindings=0 --download-chaco --download-metis
-----------------------------------------
Libraries compiled on 2019-03-15 16:36:05 on cranmer 
Machine characteristics: Linux-4.15.0-46-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lpthread -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lpthread -lstdc++ -ldl -lmpichfort -lmpich -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Thu May 23 12:50:32 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000 Thu May 23 12:52:21 2019
run-nbpc-qmc.py on a , 2 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-nbpc-qmc.py on a  named cranmer with 2 processors, by owen Thu May 23 12:52:33 2019
Using Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000

                         Max       Max/Min     Avg       Total 
Time (sec):           1.250e+01     1.000   1.250e+01
Objects:              7.250e+02     1.114   6.880e+02
Flop:                 4.113e+09     1.298   3.640e+09  7.280e+09
Flop/sec:             3.290e+08     1.298   2.912e+08  5.824e+08
MPI Messages:         5.055e+03     1.066   4.898e+03  9.797e+03
MPI Message Lengths:  5.767e+06     1.053   1.148e+03  1.124e+07
MPI Reductions:       6.636e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 1.2501e+01 100.0%  7.2804e+09 100.0%  9.797e+03 100.0%  1.148e+03      100.0%  6.629e+03  99.9% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided         82 1.0 1.8694e-03 2.3 0.00e+00 0.0 6.2e+01 4.0e+00 0.0e+00  0  0  1  0  0   0  0  1  0  0     0
BuildTwoSidedF      1286 1.0 6.3701e-0142.3 0.00e+00 0.0 9.7e+02 1.2e+03 0.0e+00  3  0 10 11  0   3  0 10 11  0     0
SFSetGraph            86 1.0 5.4836e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp               82 1.0 2.8884e-03 1.6 0.00e+00 0.0 1.9e+02 3.9e+02 0.0e+00  0  0  2  1  0   0  0  2  1  0     0
SFBcastBegin         486 1.0 2.7523e-03 1.1 0.00e+00 0.0 8.9e+02 4.9e+02 0.0e+00  0  0  9  4  0   0  0  9  4  0     0
SFBcastEnd           486 1.0 1.6932e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceBegin        648 1.0 1.3885e-03 1.2 0.00e+00 0.0 1.3e+03 3.8e+02 0.0e+00  0  0 13  4  0   0  0 13  4  0     0
SFReduceEnd          648 1.0 3.2517e-01235.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
SFFetchOpBegin         2 1.0 1.9073e-06 1.6 0.00e+00 0.0 2.0e+00 1.2e+02 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFFetchOpEnd           2 1.0 4.0531e-06 1.2 0.00e+00 0.0 2.0e+00 1.2e+02 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             1903 1.0 1.0775e-02 1.1 7.72e+06 1.1 0.0e+00 0.0e+00 1.9e+03  0  0  0  0 29   0  0  0  0 29  1351
VecNorm             2223 1.0 6.8152e-03 1.1 2.52e+06 1.1 0.0e+00 0.0e+00 2.2e+03  0  0  0  0 33   0  0  0  0 34   696
VecScale            2223 1.0 1.2712e-03 1.0 1.26e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1866
VecCopy             2240 1.0 1.4627e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               652 1.0 3.8719e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              640 1.0 6.2919e-04 1.2 7.20e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2156
VecMAXPY            2223 1.0 7.3946e-03 1.1 9.91e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2526
VecScatterBegin     6349 1.0 8.8434e-03 1.2 0.00e+00 0.0 6.4e+03 1.4e+03 2.2e+03  0  0 66 80 33   0  0 66 80 34     0
VecScatterEnd       4126 1.0 3.8579e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize        2223 1.0 9.0134e-03 1.0 3.78e+06 1.1 0.0e+00 0.0e+00 2.2e+03  0  0  0  0 33   0  0  0  0 34   790
MatMult             1903 1.0 1.6332e-02 1.1 1.30e+07 1.1 3.8e+03 2.6e+02 0.0e+00  0  0 39  9  0   0  0 39  9  0  1499
MatSolve            2223 1.0 8.5225e-01 1.0 1.10e+09 6.8 2.6e+03 3.0e+03 2.2e+03  7 17 27 72 34   7 17 27 72 34  1482
MatLUFactorSym         2 1.0 1.0123e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum         2 1.0 1.9639e-03 1.0 3.85e+04 3.2 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0    26
MatAssemblyBegin    1286 1.0 6.5543e-0128.4 0.00e+00 0.0 9.7e+02 1.2e+03 0.0e+00  3  0 10 11  0   3  0 10 11  0     0
MatAssemblyEnd      1286 1.0 1.3051e-02 1.5 0.00e+00 0.0 1.6e+01 3.2e+01 3.2e+01  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            2 1.0 7.1526e-07 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 2.4796e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries       640 1.0 1.3900e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp                2 1.0 3.1970e-03 1.0 3.85e+04 3.2 0.0e+00 0.0e+00 2.2e+01  0  0  0  0  0   0  0  0  0  0    16
PCApply             2223 1.0 8.5322e-01 1.0 1.10e+09 6.8 2.6e+03 3.0e+03 2.2e+03  7 17 27 72 34   7 17 27 72 34  1480
KSPSetUp             320 1.0 1.4710e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve             320 1.0 9.0719e-01 1.0 1.14e+09 5.9 6.4e+03 1.4e+03 6.4e+03  7 18 66 80 96   7 18 66 80 96  1465
KSPGMRESOrthog      1903 1.0 1.7747e-02 1.0 1.55e+07 1.1 0.0e+00 0.0e+00 1.9e+03  0  0  0  0 29   0  0  0  0 29  1644
SNESSolve            320 1.0 9.4830e+00 1.0 4.11e+09 1.3 9.3e+03 1.2e+03 6.4e+03 76100 95 97 96  76100 95 97 96   768
SNESFunctionEval     320 1.0 2.8860e+00 1.0 8.51e+08 1.0 1.9e+03 3.7e+02 4.0e+00 23 23 20  6  0  23 23 20  6  0   590
SNESJacobianEval     320 1.0 5.6851e+00 1.0 2.12e+09 1.0 9.6e+02 1.2e+03 0.0e+00 45 58 10 11  0  45 58 10 11  0   747
Mesh Partition         4 1.0 4.0863e-03 1.1 0.00e+00 0.0 1.1e+02 3.2e+02 2.0e+01  0  0  1  0  0   0  0  1  0  0     0
Mesh Migration         4 1.0 4.3724e-03 1.0 0.00e+00 0.0 3.0e+02 7.2e+02 1.3e+02  0  0  3  2  2   0  0  3  2  2     0
DMPlexInterp           2 1.0 1.4842e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexDistribute       2 1.0 5.0809e-03 1.0 0.00e+00 0.0 1.3e+02 1.6e+03 5.8e+01  0  0  1  2  1   0  0  1  2  1     0
DMPlexDistCones        4 1.0 7.8630e-04 1.0 0.00e+00 0.0 4.8e+01 1.3e+03 0.0e+00  0  0  0  1  0   0  0  0  1  0     0
DMPlexDistLabels       4 1.0 2.0952e-03 1.0 0.00e+00 0.0 1.7e+02 6.4e+02 9.2e+01  0  0  2  1  1   0  0  2  1  1     0
DMPlexDistOvrlp        2 1.0 3.5081e-03 1.1 0.00e+00 0.0 2.8e+02 1.8e+02 8.8e+01  0  0  3  0  1   0  0  3  0  1     0
DMPlexDistField        6 1.0 4.0770e-04 1.0 0.00e+00 0.0 6.2e+01 4.2e+02 4.0e+00  0  0  1  0  0   0  0  1  0  0     0
DMPlexDistData         4 1.0 1.0788e-03 3.2 0.00e+00 0.0 6.3e+01 3.1e+02 0.0e+00  0  0  1  0  0   0  0  1  0  0     0
DMPlexStratify         8 1.0 2.4559e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 8.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateMesh             8 1.0 1.3068e-02 1.1 0.00e+00 0.0 4.2e+02 6.5e+02 1.5e+02  0  0  4  2  2   0  0  4  2  2     0
CreateFunctionSpace       2 1.0 1.5285e-01 1.0 0.00e+00 0.0 3.0e+02 1.9e+02 1.0e+02  1  0  3  0  2   1  0  3  0  2     0
Mesh: reorder          2 1.0 2.2149e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        2 1.0 1.3449e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateSparsity         2 1.0 1.0889e-03 1.1 0.00e+00 0.0 1.1e+01 5.2e+02 1.2e+01  0  0  0  0  0   0  0  0  0  0     0
MatZeroInitial         4 1.0 4.4465e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopExecute      2240 1.0 8.4316e+00 1.1 2.98e+09 1.0 1.9e+03 3.7e+02 4.0e+00 65 82 20  6  0  65 82 20  6  0   706
ParLoopset_1         128 1.0 3.0775e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednBegin    2240 1.0 3.0975e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd      2240 1.0 3.5706e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells        1920 1.0 8.2165e+00 1.1 2.96e+09 1.0 0.0e+00 0.0e+00 0.0e+00 62 81  0  0  0  62 81  0  0  0   721
ParLoopExtFacets    1920 1.0 2.8539e-02 1.1 1.18e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   826
ParLoopset_5         512 1.0 6.8996e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
           Index Set   270            270       317856     0.
   IS L to G Mapping     8              8        42128     0.
             Section   116            116        83520     0.
   Star Forest Graph   124            124       104576     0.
              Vector    64             64       230320     0.
         Vec Scatter     8              8        10000     0.
              Matrix    24             24       160256     0.
      Preconditioner     2              2         2000     0.
       Krylov Solver     2              2        70032     0.
     DMKSP interface     2              2         1312     0.
                SNES     2              2         2808     0.
              DMSNES     2              2         1344     0.
      SNESLineSearch     2              2         1984     0.
    Distributed Mesh    22             22       109728     0.
            DM Label    46             46        28704     0.
    GraphPartitioner     8              8         4960     0.
     Discrete System    22             22        20416     0.
========================================================================================================================
Average time to get PetscTime(): 7.15256e-08
Average time for MPI_Barrier(): 1.19209e-06
Average time for zero size MPI_Send(): 1.07288e-06
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --download-parmetis --download-pnetcdf --download-scalapack --with-zlib --download-netcdf --with-scalar-type=complex --download-hdf5 --download-mumps --download-exodusii --with-fortran-bindings=0 --download-chaco --download-metis
-----------------------------------------
Libraries compiled on 2019-03-15 16:36:05 on cranmer 
Machine characteristics: Linux-4.15.0-46-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lpthread -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lpthread -lstdc++ -ldl -lmpichfort -lmpich -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Thu May 23 12:52:33 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000 Thu May 23 12:55:23 2019
run-nbpc-qmc.py on a , 4 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-nbpc-qmc.py on a  named cranmer with 4 processors, by owen Thu May 23 12:56:01 2019
Using Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000

                         Max       Max/Min     Avg       Total 
Time (sec):           3.819e+01     1.000   3.819e+01
Objects:              1.694e+04     1.005   1.688e+04
Flop:                 3.311e+09     1.011   3.296e+09  1.319e+10
Flop/sec:             8.672e+07     1.011   8.632e+07  3.453e+08
MPI Messages:         2.821e+04     1.673   2.212e+04  8.847e+04
MPI Message Lengths:  7.354e+06     1.514   2.658e+02  2.352e+07
MPI Reductions:       1.864e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 3.8187e+01 100.0%  1.3185e+10 100.0%  8.847e+04 100.0%  2.658e+02      100.0%  1.863e+04 100.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided         82 1.0 5.0340e-03 2.4 0.00e+00 0.0 2.7e+02 4.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
BuildTwoSidedF      3194 1.0 8.6619e-01 8.3 0.00e+00 0.0 1.4e+04 5.0e+02 0.0e+00  2  0 16 31  0   2  0 16 31  0     0
SFSetGraph            86 1.0 6.3896e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp               82 1.0 6.1903e-03 1.9 0.00e+00 0.0 8.1e+02 1.5e+02 0.0e+00  0  0  1  1  0   0  0  1  1  0     0
SFBcastBegin         804 1.0 7.4902e-03 1.1 0.00e+00 0.0 7.5e+03 1.6e+02 0.0e+00  0  0  8  5  0   0  0  8  5  0     0
SFBcastEnd           804 1.0 6.7871e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceBegin       1284 1.0 5.5976e-03 1.2 0.00e+00 0.0 1.3e+04 1.4e+02 0.0e+00  0  0 14  8  0   0  0 14  8  0     0
SFReduceEnd         1284 1.0 6.5653e-0153.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
SFFetchOpBegin         2 1.0 2.8610e-06 1.2 0.00e+00 0.0 1.0e+01 4.7e+01 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFFetchOpEnd           2 1.0 8.1062e-06 1.4 0.00e+00 0.0 1.0e+01 4.7e+01 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             1664 1.0 1.5209e-02 1.5 2.30e+06 1.3 0.0e+00 0.0e+00 1.7e+03  0  0  0  0  9   0  0  0  0  9   536
VecNorm             2302 1.0 1.0751e-02 1.0 1.38e+06 1.3 0.0e+00 0.0e+00 2.3e+03  0  0  0  0 12   0  0  0  0 12   454
VecScale            2302 1.0 1.9975e-03 1.1 6.88e+05 1.3 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1223
VecCopy             4466 1.0 4.2808e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              2242 1.0 1.4801e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY             1276 1.0 1.2631e-03 1.0 7.62e+05 1.3 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2142
VecMAXPY            2302 1.0 4.9243e-03 1.1 3.31e+06 1.3 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2384
VecScatterBegin     6268 1.0 1.9361e-02 1.4 0.00e+00 0.0 3.5e+04 3.6e+02 2.3e+03  0  0 40 54 12   0  0 40 54 12     0
VecScatterEnd       3966 1.0 9.1302e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize        2302 1.0 1.5233e-02 1.0 2.06e+06 1.3 0.0e+00 0.0e+00 2.3e+03  0  0  0  0 12   0  0  0  0 12   481
MatMult             1664 1.0 1.9839e-02 1.2 6.03e+06 1.3 1.7e+04 1.1e+02 0.0e+00  0  0 19  8  0   0  0 19  8  0  1072
MatSolve            2302 1.0 9.9475e-01 1.0 3.29e+08 1.1 2.4e+04 4.7e+02 2.9e+03  3 10 27 47 16   3 10 27 47 16  1260
MatLUFactorSym       320 1.0 2.5666e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.6e+03  1  0  0  0  9   1  0  0  0  9     0
MatLUFactorNum       320 1.0 3.5524e-01 1.0 2.94e+06 1.3 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0    30
MatAssemblyBegin    3194 1.0 9.4535e-01 5.6 0.00e+00 0.0 1.4e+04 5.0e+02 0.0e+00  2  0 16 31  0   2  0 16 31  0     0
MatAssemblyEnd      3194 1.0 1.0662e-01 1.2 0.00e+00 0.0 1.3e+04 1.5e+01 5.1e+03  0  0 14  1 27   0  0 14  1 27     0
MatGetRowIJ          320 1.0 8.7976e-05 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering       320 1.0 4.9422e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries      1276 1.0 3.7560e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp              320 1.0 6.4895e-01 1.0 2.94e+06 1.3 0.0e+00 0.0e+00 3.5e+03  2  0  0  0 19   2  0  0  0 19    16
PCApply             2302 1.0 9.9641e-01 1.0 3.29e+08 1.1 2.4e+04 4.7e+02 2.9e+03  3 10 27 47 16   3 10 27 47 16  1258
KSPSetUp             638 1.0 9.8801e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve             638 1.0 1.7336e+00 1.0 3.43e+08 1.1 4.0e+04 3.2e+02 1.0e+04  5 10 45 55 56   5 10 45 55 56   758
KSPGMRESOrthog      1664 1.0 2.0901e-02 1.4 4.61e+06 1.3 0.0e+00 0.0e+00 1.7e+03  0  0  0  0  9   0  0  0  0  9   783
SNESSolve            638 1.0 1.8482e+01 1.0 3.31e+09 1.0 6.9e+04 3.0e+02 1.0e+04 48100 78 87 56  48100 78 87 56   713
SNESFunctionEval     638 1.0 5.2895e+00 1.0 8.49e+08 1.0 1.9e+04 1.4e+02 4.0e+00 14 26 22 12  0  14 26 22 12  0   642
SNESJacobianEval     638 1.0 1.1449e+01 1.0 2.12e+09 1.0 9.6e+03 5.0e+02 0.0e+00 30 64 11 20  0  30 64 11 20  0   740
Mesh Partition         4 1.0 8.7211e-03 1.2 0.00e+00 0.0 5.9e+02 1.0e+02 2.0e+01  0  0  1  0  0   0  0  1  0  0     0
Mesh Migration         4 1.0 6.0236e-03 1.0 0.00e+00 0.0 1.2e+03 2.8e+02 1.3e+02  0  0  1  1  1   0  0  1  1  1     0
DMPlexInterp           2 1.0 2.3055e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexDistribute       2 1.0 8.8108e-03 1.0 0.00e+00 0.0 4.7e+02 7.2e+02 5.8e+01  0  0  1  1  0   0  0  1  1  0     0
DMPlexDistCones        4 1.0 1.0858e-03 1.0 0.00e+00 0.0 2.1e+02 4.6e+02 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexDistLabels       4 1.0 3.0336e-03 1.0 0.00e+00 0.0 6.4e+02 2.7e+02 9.2e+01  0  0  1  1  0   0  0  1  1  0     0
DMPlexDistOvrlp        2 1.0 6.2032e-03 1.3 0.00e+00 0.0 1.4e+03 7.5e+01 8.8e+01  0  0  2  0  0   0  0  2  0  0     0
DMPlexDistField        6 1.0 7.4196e-04 1.0 0.00e+00 0.0 2.8e+02 1.6e+02 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexDistData         4 1.0 2.0833e-03 2.9 0.00e+00 0.0 3.5e+02 9.3e+01 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexStratify         8 1.0 3.4370e-03 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 8.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateMesh             8 1.0 2.1219e-02 1.1 0.00e+00 0.0 1.8e+03 2.4e+02 1.5e+02  0  0  2  2  1   0  0  2  2  1     0
CreateFunctionSpace       2 1.0 2.1604e-01 1.0 0.00e+00 0.0 1.4e+03 7.6e+01 1.0e+02  1  0  2  0  1   1  0  2  0  1     0
Mesh: reorder          2 1.0 2.8825e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        2 1.0 1.6787e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateSparsity         2 1.0 3.6933e-03 2.9 0.00e+00 0.0 5.5e+01 2.1e+02 1.2e+01  0  0  0  0  0   0  0  0  0  0     0
MatZeroInitial       640 1.0 9.6705e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopExecute      4466 1.0 1.4715e+01 1.1 2.97e+09 1.0 1.9e+04 1.4e+02 4.0e+00 37 90 22 12  0  37 90 22 12  0   807
ParLoopset_1         254 1.0 6.4485e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednBegin    4466 1.0 2.8692e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd      4466 1.0 1.4415e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells        3828 1.0 1.3842e+01 1.1 2.96e+09 1.0 0.0e+00 0.0e+00 0.0e+00 34 90  0  0  0  34 90  0  0  0   854
ParLoopExtFacets    3828 1.0 8.6324e-02 1.2 1.27e+07 1.2 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   545
ParLoopset_5        1022 1.0 1.9795e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
           Index Set  3788           3788      3482408     0.
   IS L to G Mapping     8              8        31736     0.
             Section   116            116        83520     0.
   Star Forest Graph   124            124       104576     0.
              Vector  7994           7994     20105456     0.
         Vec Scatter  1280           1280      1500160     0.
              Matrix  2250           2250     16015840     0.
      Preconditioner   320            320       320000     0.
       Krylov Solver   320            320     11205120     0.
     DMKSP interface     2              2         1312     0.
                SNES   320            320       449280     0.
              DMSNES     2              2         1344     0.
      SNESLineSearch   320            320       317440     0.
    Distributed Mesh    22             22       109728     0.
            DM Label    46             46        28704     0.
    GraphPartitioner     8              8         4960     0.
     Discrete System    22             22        20416     0.
========================================================================================================================
Average time to get PetscTime(): 7.15256e-08
Average time for MPI_Barrier(): 5.00679e-06
Average time for zero size MPI_Send(): 2.08616e-06
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --download-parmetis --download-pnetcdf --download-scalapack --with-zlib --download-netcdf --with-scalar-type=complex --download-hdf5 --download-mumps --download-exodusii --with-fortran-bindings=0 --download-chaco --download-metis
-----------------------------------------
Libraries compiled on 2019-03-15 16:36:05 on cranmer 
Machine characteristics: Linux-4.15.0-46-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lpthread -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lpthread -lstdc++ -ldl -lmpichfort -lmpich -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Thu May 23 12:56:01 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000 Thu May 23 12:56:53 2019
run-nbpc-qmc.py on a , 4 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-nbpc-qmc.py on a  named cranmer with 4 processors, by owen Thu May 23 12:57:09 2019
Using Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000

                         Max       Max/Min     Avg       Total 
Time (sec):           1.574e+01     1.000   1.574e+01
Objects:              2.224e+03     1.042   2.160e+03
Flop:                 1.906e+09     1.015   1.894e+09  7.576e+09
Flop/sec:             1.211e+08     1.015   1.203e+08  4.813e+08
MPI Messages:         1.591e+04     1.610   1.267e+04  5.067e+04
MPI Message Lengths:  4.972e+06     1.603   3.066e+02  1.554e+07
MPI Reductions:       6.752e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 1.5740e+01 100.0%  7.5765e+09 100.0%  5.067e+04 100.0%  3.066e+02      100.0%  6.745e+03  99.9% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided         82 1.0 9.1834e-03 1.8 0.00e+00 0.0 2.7e+02 4.0e+00 0.0e+00  0  0  1  0  0   0  0  1  0  0     0
BuildTwoSidedF      1460 1.0 9.1281e-0117.5 0.00e+00 0.0 5.7e+03 5.0e+02 0.0e+00  3  0 11 18  0   3  0 11 18  0     0
SFSetGraph            86 1.0 6.1989e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp               82 1.0 1.0307e-02 1.6 0.00e+00 0.0 8.1e+02 1.5e+02 0.0e+00  0  0  2  1  0   0  0  2  1  0     0
SFBcastBegin         515 1.0 4.5638e-03 1.1 0.00e+00 0.0 4.6e+03 1.8e+02 0.0e+00  0  0  9  5  0   0  0  9  5  0     0
SFBcastEnd           515 1.0 3.2890e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceBegin        706 1.0 3.0038e-03 1.3 0.00e+00 0.0 7.0e+03 1.4e+02 0.0e+00  0  0 14  7  0   0  0 14  7  0     0
SFReduceEnd          706 1.0 3.4777e-0129.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
SFFetchOpBegin         2 1.0 3.0994e-06 1.2 0.00e+00 0.0 1.0e+01 4.7e+01 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFFetchOpEnd           2 1.0 8.5831e-06 1.4 0.00e+00 0.0 1.0e+01 4.7e+01 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             1555 1.0 1.5434e-02 1.3 2.68e+06 1.3 0.0e+00 0.0e+00 1.6e+03  0  0  0  0 23   0  0  0  0 23   616
VecNorm             1904 1.0 1.1775e-02 1.4 1.14e+06 1.3 0.0e+00 0.0e+00 1.9e+03  0  0  0  0 28   0  0  0  0 28   343
VecScale            1904 1.0 1.5712e-03 1.2 5.69e+05 1.3 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1286
VecCopy             2443 1.0 2.3158e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               797 1.0 5.8389e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              698 1.0 6.7663e-04 1.0 4.16e+05 1.3 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2186
VecMAXPY            1904 1.0 5.1301e-03 1.1 3.63e+06 1.3 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2512
VecScatterBegin     5363 1.0 1.5572e-02 1.4 0.00e+00 0.0 3.1e+04 3.5e+02 1.9e+03  0  0 61 69 28   0  0 61 69 28     0
VecScatterEnd       3459 1.0 7.8263e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize        1904 1.0 1.4552e-02 1.3 1.71e+06 1.3 0.0e+00 0.0e+00 1.9e+03  0  0  0  0 28   0  0  0  0 28   417
MatMult             1555 1.0 1.7596e-02 1.1 5.64e+06 1.3 1.6e+04 1.1e+02 0.0e+00  0  0 31 11  0   0  0 31 11  0  1130
MatSolve            1904 1.0 8.1437e-01 1.0 2.72e+08 1.1 1.6e+04 5.7e+02 2.0e+03  5 14 31 58 29   5 14 31 58 29  1273
MatLUFactorSym        31 1.0 2.3977e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.6e+02  0  0  0  0  2   0  0  0  0  2     0
MatLUFactorNum        31 1.0 3.4736e-02 1.0 2.80e+05 1.3 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0    29
MatAssemblyBegin    1460 1.0 9.4476e-0111.9 0.00e+00 0.0 5.7e+03 5.0e+02 0.0e+00  3  0 11 18  0   3  0 11 18  0     0
MatAssemblyEnd      1460 1.0 2.6187e-02 1.5 0.00e+00 0.0 1.2e+03 1.5e+01 5.0e+02  0  0  2  0  7   0  0  2  0  7     0
MatGetRowIJ           31 1.0 9.2983e-06 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering        31 1.0 4.6992e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries       698 1.0 1.8175e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp               31 1.0 6.2356e-02 1.0 2.80e+05 1.3 0.0e+00 0.0e+00 3.4e+02  0  0  0  0  5   0  0  0  0  5    16
PCApply             1904 1.0 8.1565e-01 1.0 2.72e+08 1.1 1.6e+04 5.7e+02 2.0e+03  5 14 31 58 29   5 14 31 58 29  1271
KSPSetUp             349 1.0 1.0686e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve             349 1.0 9.4000e-01 1.0 2.83e+08 1.1 3.1e+04 3.4e+02 5.8e+03  6 14 62 69 85   6 14 62 69 85  1156
KSPGMRESOrthog      1555 1.0 2.0873e-02 1.2 5.38e+06 1.3 0.0e+00 0.0e+00 1.6e+03  0  0  0  0 23   0  0  0  0 23   915
SNESSolve            349 1.0 9.4910e+00 1.0 1.91e+09 1.0 4.7e+04 3.2e+02 5.8e+03 60100 93 95 85  60100 93 95 86   798
SNESFunctionEval     349 1.0 2.8508e+00 1.0 4.64e+08 1.0 1.0e+04 1.4e+02 4.0e+00 18 25 21 10  0  18 25 21 10  0   651
SNESJacobianEval     349 1.0 5.7023e+00 1.0 1.16e+09 1.0 5.2e+03 5.0e+02 0.0e+00 36 61 10 17  0  36 61 10 17  0   812
Mesh Partition         4 1.0 9.7196e-03 1.4 0.00e+00 0.0 5.9e+02 1.0e+02 2.0e+01  0  0  1  0  0   0  0  1  0  0     0
Mesh Migration         4 1.0 5.9133e-03 1.0 0.00e+00 0.0 1.2e+03 2.8e+02 1.3e+02  0  0  2  2  2   0  0  2  2  2     0
DMPlexInterp           2 1.0 2.2948e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexDistribute       2 1.0 8.7066e-03 1.0 0.00e+00 0.0 4.7e+02 7.2e+02 5.8e+01  0  0  1  2  1   0  0  1  2  1     0
DMPlexDistCones        4 1.0 1.0283e-03 1.0 0.00e+00 0.0 2.1e+02 4.6e+02 0.0e+00  0  0  0  1  0   0  0  0  1  0     0
DMPlexDistLabels       4 1.0 2.9793e-03 1.0 0.00e+00 0.0 6.4e+02 2.7e+02 9.2e+01  0  0  1  1  1   0  0  1  1  1     0
DMPlexDistOvrlp        2 1.0 7.1578e-03 1.5 0.00e+00 0.0 1.4e+03 7.5e+01 8.8e+01  0  0  3  1  1   0  0  3  1  1     0
DMPlexDistField        6 1.0 7.3695e-04 1.0 0.00e+00 0.0 2.8e+02 1.6e+02 4.0e+00  0  0  1  0  0   0  0  1  0  0     0
DMPlexDistData         4 1.0 2.0680e-03 3.0 0.00e+00 0.0 3.5e+02 9.3e+01 0.0e+00  0  0  1  0  0   0  0  1  0  0     0
DMPlexStratify         8 1.0 3.3996e-03 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 8.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateMesh             8 1.0 2.2176e-02 1.2 0.00e+00 0.0 1.8e+03 2.4e+02 1.5e+02  0  0  4  3  2   0  0  4  3  2     0
CreateFunctionSpace       2 1.0 2.9545e-01 1.0 0.00e+00 0.0 1.4e+03 7.6e+01 1.0e+02  2  0  3  1  1   2  0  3  1  1     0
Mesh: reorder          2 1.0 2.8586e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        2 1.0 1.6432e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateSparsity         2 1.0 1.6248e-03 1.3 0.00e+00 0.0 5.5e+01 2.1e+02 1.2e+01  0  0  0  0  0   0  0  0  0  0     0
MatZeroInitial        62 1.0 9.7415e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopExecute      2443 1.0 8.1422e+00 1.1 1.62e+09 1.0 1.0e+04 1.4e+02 4.0e+00 49 86 21 10  0  49 86 21 10  0   797
ParLoopset_1         140 1.0 4.0147e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednBegin    2443 1.0 7.6027e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd      2443 1.0 7.6613e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells        2094 1.0 7.6886e+00 1.2 1.62e+09 1.0 0.0e+00 0.0e+00 0.0e+00 46 85  0  0  0  46 85  0  0  0   841
ParLoopExtFacets    2094 1.0 4.7536e-02 1.2 6.94e+06 1.2 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   541
ParLoopset_5         558 1.0 1.0028e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
           Index Set   609            609       595476     0.
   IS L to G Mapping     8              8        31736     0.
             Section   116            116        83520     0.
   Star Forest Graph   124            124       104576     0.
              Vector   789            789      2004008     0.
         Vec Scatter   124            124       145328     0.
              Matrix   227            227      1569328     0.
      Preconditioner    31             31        31000     0.
       Krylov Solver    31             31      1085496     0.
     DMKSP interface     2              2         1312     0.
                SNES    31             31        43524     0.
              DMSNES     2              2         1344     0.
      SNESLineSearch    31             31        30752     0.
    Distributed Mesh    22             22       109728     0.
            DM Label    46             46        28704     0.
    GraphPartitioner     8              8         4960     0.
     Discrete System    22             22        20416     0.
========================================================================================================================
Average time to get PetscTime(): 7.15256e-08
Average time for MPI_Barrier(): 5.14984e-06
Average time for zero size MPI_Send(): 3.30806e-05
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --download-parmetis --download-pnetcdf --download-scalapack --with-zlib --download-netcdf --with-scalar-type=complex --download-hdf5 --download-mumps --download-exodusii --with-fortran-bindings=0 --download-chaco --download-metis
-----------------------------------------
Libraries compiled on 2019-03-15 16:36:05 on cranmer 
Machine characteristics: Linux-4.15.0-46-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lpthread -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lpthread -lstdc++ -ldl -lmpichfort -lmpich -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Thu May 23 12:57:09 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000 Thu May 23 12:59:07 2019
run-nbpc-qmc.py on a , 4 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-nbpc-qmc.py on a  named cranmer with 4 processors, by owen Thu May 23 12:59:19 2019
Using Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000

                         Max       Max/Min     Avg       Total 
Time (sec):           1.187e+01     1.000   1.187e+01
Objects:              3.730e+02     1.137   3.412e+02
Flop:                 1.612e+09     1.018   1.596e+09  6.385e+09
Flop/sec:             1.358e+08     1.018   1.345e+08  5.379e+08
MPI Messages:         1.333e+04     1.635   1.066e+04  4.264e+04
MPI Message Lengths:  4.704e+06     1.596   3.414e+02  1.456e+07
MPI Reductions:       5.328e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 1.1870e+01 100.0%  6.3852e+09 100.0%  4.264e+04 100.0%  3.414e+02      100.0%  5.321e+03  99.9% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided         41 1.0 4.0989e-02 1.9 0.00e+00 0.0 1.4e+02 4.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
BuildTwoSidedF      1027 1.0 7.1538e-0118.6 0.00e+00 0.0 3.9e+03 5.3e+02 0.0e+00  3  0  9 14  0   3  0  9 14  0     0
SFSetGraph            43 1.0 4.1008e-05 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp               41 1.0 4.1624e-02 1.2 0.00e+00 0.0 4.0e+02 1.8e+02 0.0e+00  0  0  1  1  0   0  0  1  1  0     0
SFBcastBegin         339 1.0 3.3057e-03 1.2 0.00e+00 0.0 3.1e+03 1.9e+02 0.0e+00  0  0  7  4  0   0  0  7  4  0     0
SFBcastEnd           339 1.0 1.5073e-02 7.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceBegin        516 1.0 2.1551e-03 1.2 0.00e+00 0.0 5.1e+03 1.5e+02 0.0e+00  0  0 12  5  0   0  0 12  5  0     0
SFReduceEnd          516 1.0 2.6525e-0118.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
SFFetchOpBegin         1 1.0 1.6689e-06 1.4 0.00e+00 0.0 5.0e+00 5.4e+01 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFFetchOpEnd           1 1.0 3.3379e-06 1.0 0.00e+00 0.0 5.0e+00 5.4e+01 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             1556 1.0 1.8440e-02 1.4 3.64e+06 1.2 0.0e+00 0.0e+00 1.6e+03  0  0  0  0 29   0  0  0  0 29   703
VecNorm             1812 1.0 8.4274e-03 1.0 1.17e+06 1.2 0.0e+00 0.0e+00 1.8e+03  0  0  0  0 34   0  0  0  0 34   497
VecScale            1812 1.0 1.4684e-03 1.2 5.87e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1426
VecCopy             1792 1.0 1.7087e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               518 1.0 3.8767e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              512 1.0 5.2452e-04 1.1 3.32e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2257
VecMAXPY            1812 1.0 6.3808e-03 1.2 4.67e+06 1.2 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2611
VecScatterBegin     5180 1.0 1.4227e-02 1.4 0.00e+00 0.0 3.0e+04 3.7e+02 1.8e+03  0  0 70 76 34   0  0 70 76 34     0
VecScatterEnd       3368 1.0 7.6389e-03 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize        1812 1.0 1.1010e-02 1.0 1.76e+06 1.2 0.0e+00 0.0e+00 1.8e+03  0  0  0  0 34   0  0  0  0 34   571
MatMult             1556 1.0 1.8456e-02 1.2 6.12e+06 1.3 1.6e+04 1.1e+02 0.0e+00  0  0 36 12  0   0  0 36 12  0  1177
MatSolve            1812 1.0 8.1559e-01 1.0 2.93e+08 1.1 1.5e+04 6.4e+02 1.8e+03  7 17 34 64 34   7 17 34 64 34  1361
MatLUFactorSym         1 1.0 1.0474e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 5.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum         1 1.0 1.4822e-03 1.0 1.05e+04 1.2 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0    25
MatAssemblyBegin    1027 1.0 7.3799e-0112.9 0.00e+00 0.0 3.9e+03 5.3e+02 0.0e+00  3  0  9 14  0   3  0  9 14  0     0
MatAssemblyEnd      1027 1.0 1.5024e-02 1.8 0.00e+00 0.0 4.0e+01 1.6e+01 1.6e+01  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            1 1.0 4.7684e-07 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         1 1.0 2.2650e-05 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries       512 1.0 1.3573e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp                1 1.0 2.7149e-03 1.0 1.05e+04 1.2 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  0   0  0  0  0  0    14
PCApply             1812 1.0 8.1675e-01 1.0 2.93e+08 1.1 1.5e+04 6.4e+02 1.8e+03  7 17 34 64 34   7 17 34 64 34  1359
KSPSetUp             256 1.0 1.2612e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve             256 1.0 8.7768e-01 1.0 3.08e+08 1.1 3.0e+04 3.7e+02 5.2e+03  7 18 71 76 97   7 18 71 76 98  1331
KSPGMRESOrthog      1556 1.0 2.4905e-02 1.2 7.30e+06 1.2 0.0e+00 0.0e+00 1.6e+03  0  0  0  0 29   0  0  0  0 29  1045
SNESSolve            256 1.0 7.5393e+00 1.0 1.61e+09 1.0 4.2e+04 3.4e+02 5.2e+03 63100 98 98 98  63100 98 98 98   847
SNESFunctionEval     256 1.0 2.2326e+00 1.0 3.73e+08 1.0 7.7e+03 1.5e+02 2.0e+00 19 23 18  8  0  19 23 18  8  0   669
SNESJacobianEval     256 1.0 4.4285e+00 1.0 9.31e+08 1.0 3.8e+03 5.3e+02 0.0e+00 37 58  9 14  0  37 58  9 14  0   841
Mesh Partition         2 1.0 4.1215e-03 1.0 0.00e+00 0.0 3.0e+02 1.2e+02 1.0e+01  0  0  1  0  0   0  0  1  0  0     0
Mesh Migration         2 1.0 3.4826e-03 1.0 0.00e+00 0.0 6.0e+02 3.5e+02 6.3e+01  0  0  1  1  1   0  0  1  1  1     0
DMPlexInterp           1 1.0 1.4362e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexDistribute       1 1.0 5.1751e-03 1.0 0.00e+00 0.0 2.4e+02 9.1e+02 2.9e+01  0  0  1  1  1   0  0  1  1  1     0
DMPlexDistCones        2 1.0 6.9404e-04 1.0 0.00e+00 0.0 1.0e+02 5.8e+02 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexDistLabels       2 1.0 1.7195e-03 1.0 0.00e+00 0.0 3.2e+02 3.4e+02 4.6e+01  0  0  1  1  1   0  0  1  1  1     0
DMPlexDistOvrlp        1 1.0 2.5499e-03 1.0 0.00e+00 0.0 6.8e+02 8.7e+01 4.4e+01  0  0  2  0  1   0  0  2  0  1     0
DMPlexDistField        3 1.0 4.0078e-04 1.0 0.00e+00 0.0 1.4e+02 1.9e+02 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexDistData         2 1.0 1.2698e-03 3.1 0.00e+00 0.0 1.8e+02 1.1e+02 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexStratify         4 1.0 2.1253e-03 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateMesh             4 1.0 1.1396e-02 1.1 0.00e+00 0.0 9.2e+02 3.0e+02 7.7e+01  0  0  2  2  1   0  0  2  2  1     0
CreateFunctionSpace       1 1.0 3.1901e-01 1.0 0.00e+00 0.0 7.1e+02 8.8e+01 5.0e+01  3  0  2  0  1   3  0  2  0  1     0
Mesh: reorder          1 1.0 1.5378e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        1 1.0 9.6250e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateSparsity         1 1.0 4.6108e-03 6.1 0.00e+00 0.0 2.8e+01 2.4e+02 6.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroInitial         2 1.0 3.3116e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopExecute      1792 1.0 6.4671e+00 1.1 1.30e+09 1.0 7.7e+03 1.5e+02 2.0e+00 51 82 18  8  0  51 82 18  8  0   807
ParLoopset_1         512 1.0 1.0849e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednBegin    1792 1.0 4.2591e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd      1792 1.0 5.2340e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells        1536 1.0 6.1618e+00 1.2 1.30e+09 1.0 0.0e+00 0.0e+00 0.0e+00 48 81  0  0  0  48 81  0  0  0   843
ParLoopExtFacets    1536 1.0 3.1629e-02 1.1 5.27e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   628
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
           Index Set   145            145       163076     0.
   IS L to G Mapping     4              4        19360     0.
             Section    58             58        41760     0.
   Star Forest Graph    62             62        52288     0.
              Vector    32             32       103192     0.
         Vec Scatter     4              4         4688     0.
              Matrix    12             12        65928     0.
      Preconditioner     1              1         1000     0.
       Krylov Solver     1              1        35016     0.
     DMKSP interface     1              1          656     0.
                SNES     1              1         1404     0.
              DMSNES     1              1          672     0.
      SNESLineSearch     1              1          992     0.
    Distributed Mesh    11             11        54864     0.
            DM Label    23             23        14352     0.
    GraphPartitioner     4              4         2480     0.
     Discrete System    11             11        10208     0.
========================================================================================================================
Average time to get PetscTime(): 7.15256e-08
Average time for MPI_Barrier(): 3.24249e-06
Average time for zero size MPI_Send(): 2.26498e-06
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --download-parmetis --download-pnetcdf --download-scalapack --with-zlib --download-netcdf --with-scalar-type=complex --download-hdf5 --download-mumps --download-exodusii --with-fortran-bindings=0 --download-chaco --download-metis
-----------------------------------------
Libraries compiled on 2019-03-15 16:36:05 on cranmer 
Machine characteristics: Linux-4.15.0-46-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lpthread -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lpthread -lstdc++ -ldl -lmpichfort -lmpich -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Thu May 23 12:59:19 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000 Thu May 23 12:59:23 2019
run-nbpc-qmc.py on a , 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-nbpc-qmc.py on a  named cranmer with 1 processor, by owen Thu May 23 12:59:38 2019
Using Petsc Development GIT revision: v3.4.2-24799-g53ccc99  GIT Date: 2019-03-06 16:46:02 +0000

                         Max       Max/Min     Avg       Total 
Time (sec):           1.518e+01     1.000   1.518e+01
Objects:              1.310e+02     1.000   1.310e+02
Flop:                 5.375e+09     1.000   5.375e+09  5.375e+09
Flop/sec:             3.540e+08     1.000   3.540e+08  3.540e+08
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 1.5183e+01 100.0%  5.3748e+09 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSidedF         1 1.0 2.3127e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             1 1.0 1.1921e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             1556 1.0 8.4283e-03 1.0 1.30e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1547
VecNorm             1812 1.0 2.5194e-03 1.0 4.18e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1660
VecScale            1812 1.0 1.2131e-03 1.0 2.09e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1727
VecCopy             1792 1.0 1.2126e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               533 1.0 3.4571e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              512 1.0 6.3777e-04 1.0 1.18e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1856
VecMAXPY            1812 1.0 1.0756e-02 1.0 1.67e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1549
VecNormalize        1812 1.0 4.4217e-03 1.0 6.28e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1420
MatMult             1556 1.0 1.8080e-02 1.0 2.17e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1201
MatSolve            1812 1.0 7.9315e-02 1.0 9.89e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0  1247
MatLUFactorSym         1 1.0 1.8835e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum         1 1.0 4.7493e-04 1.0 4.14e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   873
MatAssemblyBegin    1027 1.0 1.3685e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd      1027 1.0 4.4081e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            1 1.0 1.8597e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         1 1.0 9.6321e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries       512 1.0 1.1580e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp                1 1.0 7.9155e-04 1.0 4.14e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   524
PCApply             1812 1.0 7.9870e-02 1.0 9.89e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0  1239
KSPSetUp             256 1.0 8.4400e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve             256 1.0 1.2873e-01 1.0 1.58e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0  1224
KSPGMRESOrthog      1556 1.0 1.7913e-02 1.0 2.61e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1457
SNESSolve            256 1.0 1.2845e+01 1.0 5.37e+09 1.0 0.0e+00 0.0e+00 0.0e+00 85100  0  0  0  85100  0  0  0   418
SNESFunctionEval     256 1.0 4.2210e+00 1.0 1.49e+09 1.0 0.0e+00 0.0e+00 0.0e+00 28 28  0  0  0  28 28  0  0  0   354
SNESJacobianEval     256 1.0 8.4897e+00 1.0 3.72e+09 1.0 0.0e+00 0.0e+00 0.0e+00 56 69  0  0  0  56 69  0  0  0   439
DMPlexInterp           1 1.0 8.4066e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexStratify         2 1.0 5.2452e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateMesh             4 1.0 1.8661e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateFunctionSpace       1 1.0 8.5851e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
Mesh: reorder          1 1.0 1.4329e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        1 1.0 8.0013e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateSparsity         1 1.0 6.1417e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroInitial         2 1.0 3.6168e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopExecute      1792 1.0 1.2632e+01 1.0 5.22e+09 1.0 0.0e+00 0.0e+00 0.0e+00 83 97  0  0  0  83 97  0  0  0   413
ParLoopset_1         512 1.0 7.3872e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednBegin    1792 1.0 2.1544e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd      1792 1.0 2.6631e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells        1536 1.0 1.2472e+01 1.0 5.20e+09 1.0 0.0e+00 0.0e+00 0.0e+00 82 97  0  0  0  82 97  0  0  0   417
ParLoopExtFacets    1536 1.0 2.1336e-02 1.0 1.99e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   931
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
           Index Set    40             40        67424     0.
   IS L to G Mapping     1              1         1828     0.
             Section    15             15        10800     0.
   Star Forest Graph    14             14        11328     0.
              Vector    23             23       146304     0.
              Matrix     6              6       244652     0.
      Preconditioner     1              1         1000     0.
       Krylov Solver     1              1        35016     0.
     DMKSP interface     1              1          656     0.
                SNES     1              1         1404     0.
              DMSNES     1              1          672     0.
      SNESLineSearch     1              1          992     0.
    Distributed Mesh     7              7        34704     0.
            DM Label     9              9         5616     0.
    GraphPartitioner     2              2         1240     0.
     Discrete System     7              7         6496     0.
========================================================================================================================
Average time to get PetscTime(): 4.76837e-08
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --download-parmetis --download-pnetcdf --download-scalapack --with-zlib --download-netcdf --with-scalar-type=complex --download-hdf5 --download-mumps --download-exodusii --with-fortran-bindings=0 --download-chaco --download-metis
-----------------------------------------
Libraries compiled on 2019-03-15 16:36:05 on cranmer 
Machine characteristics: Linux-4.15.0-46-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lpthread -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lpthread -lstdc++ -ldl -lmpichfort -lmpich -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Thu May 23 12:59:38 2019
---------------------------------------------------------
