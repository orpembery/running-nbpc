---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500 Fri Jan 25 15:32:09 2019
run-qmc.py on a , 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-qmc.py on a  named wycliffe with 1 processor, by owen Fri Jan 25 15:32:23 2019
Using Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500

                         Max       Max/Min     Avg       Total 
Time (sec):           1.418e+01     1.000   1.418e+01
Objects:              1.000e+00     1.000   1.000e+00
Flop:                 0.000e+00     0.000   0.000e+00  0.000e+00
Flop/sec:             0.000e+00     0.000   0.000e+00  0.000e+00
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 1.4177e+01 100.0%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 
 1:         No nbpc: 6.9857e-05   0.0%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage


--- Event Stage 1: No nbpc

------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.

--- Event Stage 1: No nbpc

========================================================================================================================
Average time to get PetscTime(): 1.19209e-07
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --with-fortran-bindings=0 --download-netcdf --download-parmetis --download-hdf5 --download-chaco --download-scalapack --with-scalar-type=complex --download-pnetcdf --download-exodusii --with-zlib --download-mumps --download-metis
-----------------------------------------
Libraries compiled on 2018-11-05 11:29:19 on wycliffe 
Machine characteristics: Linux-4.15.0-36-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/openmpi/lib -L/usr/lib/openmpi/lib -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Fri Jan 25 15:32:23 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500 Fri Jan 25 15:33:07 2019
run-qmc.py on a , 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-qmc.py on a  named wycliffe with 1 processor, by owen Fri Jan 25 15:33:11 2019
Using Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500

                         Max       Max/Min     Avg       Total 
Time (sec):           3.852e+00     1.000   3.852e+00
Objects:              1.000e+00     1.000   1.000e+00
Flop:                 0.000e+00     0.000   0.000e+00  0.000e+00
Flop/sec:             0.000e+00     0.000   0.000e+00  0.000e+00
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 3.8516e+00 100.0%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 
 1:         No nbpc: 6.6996e-05   0.0%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage


--- Event Stage 1: No nbpc

------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.

--- Event Stage 1: No nbpc

========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --with-fortran-bindings=0 --download-netcdf --download-parmetis --download-hdf5 --download-chaco --download-scalapack --with-scalar-type=complex --download-pnetcdf --download-exodusii --with-zlib --download-mumps --download-metis
-----------------------------------------
Libraries compiled on 2018-11-05 11:29:19 on wycliffe 
Machine characteristics: Linux-4.15.0-36-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/openmpi/lib -L/usr/lib/openmpi/lib -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Fri Jan 25 15:33:11 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500 Fri Jan 25 15:34:54 2019
run-qmc.py on a , 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run-qmc.py on a  named wycliffe with 1 processor, by owen Fri Jan 25 15:35:43 2019
Using Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500

                         Max       Max/Min     Avg       Total 
Time (sec):           4.875e+01     1.000   4.875e+01
Objects:              8.950e+02     1.000   8.950e+02
Flop:                 7.459e+09     1.000   7.459e+09  7.459e+09
Flop/sec:             1.530e+08     1.000   1.530e+08  1.530e+08
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 3.8338e+00   7.9%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 
 1:         No nbpc: 2.2712e+01  46.6%  3.6241e+09  48.6%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 
 2:            nbpc: 2.2200e+01  45.5%  3.8345e+09  51.4%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage


--- Event Stage 1: No nbpc

BuildTwoSidedF         1 1.0 2.9922e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             1 1.0 3.4809e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot               32 1.0 1.6453e-03 1.0 5.42e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   329
VecNorm               64 1.0 3.8035e-03 1.0 1.08e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   285
VecScale              64 1.0 1.3376e-02 1.0 5.42e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0    40
VecCopy              224 1.0 1.3964e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               292 1.0 1.0879e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               64 1.0 1.1258e-03 1.0 1.08e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   962
VecMAXPY              64 1.0 3.7837e-03 1.0 1.08e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   286
VecNormalize          64 1.0 1.7289e-02 1.0 1.62e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0    94
MatMult               32 1.0 1.3700e-02 1.0 3.43e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   250
MatSolve              64 1.0 1.6717e-01 1.0 4.41e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   1  1  0  0  0   264
MatLUFactorSym        32 1.0 1.4775e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   1  0  0  0  0     0
MatLUFactorNum        32 1.0 1.6011e+00 1.0 3.80e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  5  0  0  0   7 10  0  0  0   237
MatAssemblyBegin     193 1.0 3.7932e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd       193 1.0 4.1237e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ           32 1.0 2.8026e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering        32 1.0 1.5814e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   1  0  0  0  0     0
MatZeroEntries        64 1.0 1.3406e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp               32 1.0 1.9101e+00 1.0 3.80e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4  5  0  0  0   8 10  0  0  0   199
PCApply               64 1.0 1.6730e-01 1.0 4.41e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   1  1  0  0  0   264
KSPSetUp              32 1.0 2.6054e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              32 1.0 2.1192e+00 1.0 4.32e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4  6  0  0  0   9 12  0  0  0   204
KSPGMRESOrthog        32 1.0 3.7720e-03 1.0 1.08e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   287
SNESSolve             32 1.0 1.9979e+01 1.0 3.62e+09 1.0 0.0e+00 0.0e+00 0.0e+00 41 49  0  0  0  88100  0  0  0   181
SNESFunctionEval      32 1.0 5.9390e+00 1.0 8.21e+08 1.0 0.0e+00 0.0e+00 0.0e+00 12 11  0  0  0  26 23  0  0  0   138
SNESJacobianEval      32 1.0 1.1918e+01 1.0 2.37e+09 1.0 0.0e+00 0.0e+00 0.0e+00 24 32  0  0  0  52 65  0  0  0   199
DMPlexInterp           1 1.0 3.3569e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexStratify         2 1.0 2.4020e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateMesh             4 1.0 5.5048e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateFunctionSpace       1 1.0 2.7584e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
Mesh: reorder          1 1.0 1.6103e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        1 1.0 1.8770e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateSparsity         1 1.0 2.4587e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroInitial        64 1.0 3.8432e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   2  0  0  0  0     0
ParLoopExecute       224 1.0 1.7666e+01 1.0 3.19e+09 1.0 0.0e+00 0.0e+00 0.0e+00 36 43  0  0  0  78 88  0  0  0   181
ParLoopset_1          64 1.0 4.4651e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednBegin     224 1.0 2.4269e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd       224 1.0 1.2043e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells         192 1.0 1.7333e+01 1.0 3.19e+09 1.0 0.0e+00 0.0e+00 0.0e+00 36 43  0  0  0  76 88  0  0  0   184
ParLoopExtFacets     192 1.0 2.9169e-02 1.0 6.98e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   239

--- Event Stage 2: nbpc

BuildTwoSidedF         1 1.0 7.7009e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             1 1.0 5.0068e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              268 1.0 7.8151e-02 1.0 2.23e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  1  0  0  0   286
VecNorm              303 1.0 2.2151e-02 1.0 5.13e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   232
VecScale             303 1.0 5.3577e-03 1.0 2.56e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   479
VecCopy              245 1.0 1.3523e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               142 1.0 5.8460e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               70 1.0 1.2321e-03 1.0 1.18e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   962
VecMAXPY             303 1.0 9.7571e-02 1.0 2.69e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  1  0  0  0   275
VecNormalize         303 1.0 2.8165e-02 1.0 7.69e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   273
MatMult              268 1.0 1.3492e-01 1.0 2.87e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   1  1  0  0  0   213
MatSolve             303 1.0 9.3718e-01 1.0 2.09e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  3  0  0  0   4  5  0  0  0   223
MatLUFactorSym         4 1.0 1.9903e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum         4 1.0 2.0384e-01 1.0 4.75e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   1  1  0  0  0   233
MatAssemblyBegin     149 1.0 1.6260e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd       149 1.0 2.4295e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            4 1.0 1.2300e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         4 1.0 1.8804e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        70 1.0 1.2841e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp                4 1.0 2.4296e-01 1.0 4.75e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   1  1  0  0  0   196
PCApply              303 1.0 9.3788e-01 1.0 2.09e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  3  0  0  0   4  5  0  0  0   223
KSPSetUp              35 1.0 2.8324e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              35 1.0 1.5274e+00 1.0 3.43e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  5  0  0  0   7  9  0  0  0   224
KSPGMRESOrthog       268 1.0 1.6263e-01 1.0 4.47e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   1  1  0  0  0   275
SNESSolve             35 1.0 2.1614e+01 1.0 3.83e+09 1.0 0.0e+00 0.0e+00 0.0e+00 44 51  0  0  0  97100  0  0  0   177
SNESFunctionEval      35 1.0 6.4181e+00 1.0 8.98e+08 1.0 0.0e+00 0.0e+00 0.0e+00 13 12  0  0  0  29 23  0  0  0   140
SNESJacobianEval      35 1.0 1.3666e+01 1.0 2.59e+09 1.0 0.0e+00 0.0e+00 0.0e+00 28 35  0  0  0  62 68  0  0  0   190
DMPlexInterp           1 1.0 2.7650e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexStratify         2 1.0 1.2327e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateMesh             4 1.0 3.6075e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateFunctionSpace       1 1.0 6.5568e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: reorder          1 1.0 5.2910e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        1 1.0 1.8432e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateSparsity         1 1.0 1.0967e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroInitial         8 1.0 5.3690e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopExecute       245 1.0 2.0019e+01 1.0 3.49e+09 1.0 0.0e+00 0.0e+00 0.0e+00 41 47  0  0  0  90 91  0  0  0   174
ParLoopRednBegin     245 1.0 1.2245e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd       245 1.0 1.3518e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells         210 1.0 1.9956e+01 1.0 3.48e+09 1.0 0.0e+00 0.0e+00 0.0e+00 41 47  0  0  0  90 91  0  0  0   175
ParLoopExtFacets     210 1.0 1.9753e-02 1.0 7.64e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   387
ParLoopset_5          70 1.0 1.9958e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
           Index Set     0             18       262456     0.
   IS L to G Mapping     0              2        18288     0.
             Section     0             20        14560     0.
   Star Forest Graph     0             14        11680     0.
              Vector     0             23       577208     0.
              Matrix     0             18      5357376     0.
     DMKSP interface     0              2         1328     0.
              DMSNES     0              2         1360     0.
    Distributed Mesh     0             12        58704     0.
    GraphPartitioner     0              2         1240     0.
     Discrete System     0             12        11240     0.

--- Event Stage 1: No nbpc

           Index Set   133            124      1035056     0.
   IS L to G Mapping     1              0            0     0.
             Section    17              7         5096     0.
   Star Forest Graph    18             11         8976     0.
              Vector   261            243      7925080     0.
              Matrix    99             79     71011356     0.
      Preconditioner    32             32        32256     0.
       Krylov Solver    32             32      1120256     0.
     DMKSP interface     1              0            0     0.
                SNES    32             32        44288     0.
              DMSNES     1              0            0     0.
      SNESLineSearch    32             32        32000     0.
    Distributed Mesh     9              3        14808     0.
    GraphPartitioner     2              1          620     0.
     Discrete System     9              3         2808     0.

--- Event Stage 2: nbpc

           Index Set    49             40       256880     0.
   IS L to G Mapping     1              0            0     0.
             Section    17              7         5096     0.
   Star Forest Graph    18             11         8976     0.
              Vector    77             72      2379520     0.
              Matrix    15             17     10679220     0.
      Preconditioner     4              4         4032     0.
       Krylov Solver     4              4       140032     0.
     DMKSP interface     1              0            0     0.
                SNES     4              4         5536     0.
              DMSNES     1              0            0     0.
      SNESLineSearch     4              4         4000     0.
    Distributed Mesh     9              3        14808     0.
    GraphPartitioner     2              1          620     0.
     Discrete System     9              3         2808     0.
========================================================================================================================
Average time to get PetscTime(): 1.19209e-07
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --with-fortran-bindings=0 --download-netcdf --download-parmetis --download-hdf5 --download-chaco --download-scalapack --with-scalar-type=complex --download-pnetcdf --download-exodusii --with-zlib --download-mumps --download-metis
-----------------------------------------
Libraries compiled on 2018-11-05 11:29:19 on wycliffe 
Machine characteristics: Linux-4.15.0-36-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/openmpi/lib -L/usr/lib/openmpi/lib -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Fri Jan 25 15:35:43 2019
---------------------------------------------------------
