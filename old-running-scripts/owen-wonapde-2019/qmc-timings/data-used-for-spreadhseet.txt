---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500 Wed Jan 16 09:28:01 2019
tmp.py on a , 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

tmp.py on a  named wycliffe with 1 processor, by owen Wed Jan 16 09:28:10 2019
Using Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500

                         Max       Max/Min     Avg       Total 
Time (sec):           8.694e+00     1.000   8.694e+00
Objects:              1.220e+02     1.000   1.220e+02
Flop:                 1.375e+09     1.000   1.375e+09  1.375e+09
Flop/sec:             1.582e+08     1.000   1.582e+08  1.582e+08
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 5.5600e+00  63.9%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 
 1: HelmholtzProblem solve: 3.1343e+00  36.1%  1.3752e+09 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSidedF         1 1.0 2.7609e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             1 1.0 1.4496e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                 5 1.0 6.0339e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       3 1.0 3.1590e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         3 1.0 3.8273e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexInterp           1 1.0 1.5354e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   3  0  0  0  0     0
DMPlexStratify         2 1.0 6.5705e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
CreateMesh             4 1.0 2.0567e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   4  0  0  0  0     0
CreateFunctionSpace       1 1.0 6.0273e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  7  0  0  0  0  11  0  0  0  0     0
Mesh: reorder          1 1.0 2.6630e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        1 1.0 1.1656e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   2  0  0  0  0     0
CreateSparsity         1 1.0 5.1928e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatZeroInitial         2 1.0 7.4788e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0

--- Event Stage 1: HelmholtzProblem solve

VecMDot               20 1.0 6.1021e-03 1.0 1.63e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   267
VecNorm               40 1.0 1.3252e-02 1.0 3.26e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   246
VecScale              40 1.0 2.9538e-03 1.0 1.63e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   553
VecCopy              140 1.0 3.5377e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                46 1.0 1.1952e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               40 1.0 3.0677e-03 1.0 3.26e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1064
VecMAXPY              40 1.0 1.3836e-02 1.0 3.26e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   236
VecNormalize          40 1.0 1.6301e-02 1.0 4.90e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   1  0  0  0  0   300
MatMult               20 1.0 4.8698e-02 1.0 1.05e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   2  1  0  0  0   215
MatSolve              40 1.0 8.2963e-01 1.0 1.85e+08 1.0 0.0e+00 0.0e+00 0.0e+00 10 13  0  0  0  26 13  0  0  0   223
MatLUFactorSym         1 1.0 4.7116e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   2  0  0  0  0     0
MatLUFactorNum         1 1.0 6.3705e-01 1.0 1.49e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7 11  0  0  0  20 11  0  0  0   234
MatAssemblyBegin      80 1.0 2.9087e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd        80 1.0 6.0055e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   2  0  0  0  0     0
MatGetRowIJ            1 1.0 1.9250e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         1 1.0 2.8013e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   1  0  0  0  0     0
MatZeroEntries        40 1.0 5.0325e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp                1 1.0 7.1233e-01 1.0 1.49e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8 11  0  0  0  23 11  0  0  0   209
PCApply               40 1.0 8.2976e-01 1.0 1.85e+08 1.0 0.0e+00 0.0e+00 0.0e+00 10 13  0  0  0  26 13  0  0  0   223
KSPSetUp              20 1.0 6.8188e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              20 1.0 1.6324e+00 1.0 3.56e+08 1.0 0.0e+00 0.0e+00 0.0e+00 19 26  0  0  0  52 26  0  0  0   218
KSPGMRESOrthog        20 1.0 1.3470e-02 1.0 3.26e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   242
SNESSolve             20 1.0 3.1215e+00 1.0 1.38e+09 1.0 0.0e+00 0.0e+00 0.0e+00 36100  0  0  0 100100  0  0  0   441
SNESFunctionEval      20 1.0 1.5291e-01 1.0 2.30e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2 17  0  0  0   5 17  0  0  0  1504
SNESJacobianEval      20 1.0 1.3330e+00 1.0 7.88e+08 1.0 0.0e+00 0.0e+00 0.0e+00 15 57  0  0  0  43 57  0  0  0   591
ParLoopExecute       140 1.0 1.3863e+00 1.0 1.02e+09 1.0 0.0e+00 0.0e+00 0.0e+00 16 74  0  0  0  44 74  0  0  0   734
ParLoopset_1          40 1.0 3.9184e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednBegin     140 1.0 6.1226e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd       140 1.0 8.4567e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells         120 1.0 1.2358e+00 1.0 1.01e+09 1.0 0.0e+00 0.0e+00 0.0e+00 14 73  0  0  0  39 73  0  0  0   816
ParLoopExtFacets     120 1.0 4.1880e-02 1.0 9.70e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   1  1  0  0  0   232
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
           Index Set    37             40      1364032     0.
   IS L to G Mapping     1              1        41484     0.
             Section    17             17        12376     0.
   Star Forest Graph    18             18        14816     0.
              Vector     6             13      2305096     0.
              Matrix     5              6     14903180     0.
      Preconditioner     1              1         1008     0.
       Krylov Solver     1              1        35008     0.
     DMKSP interface     0              1          664     0.
                SNES     1              1         1384     0.
              DMSNES     1              1          680     0.
      SNESLineSearch     1              1         1000     0.
    Distributed Mesh     9              9        44160     0.
    GraphPartitioner     2              2         1240     0.
     Discrete System     9              9         8428     0.

--- Event Stage 1: HelmholtzProblem solve

           Index Set     3              0            0     0.
              Vector     7              0            0     0.
              Matrix     1              0            0     0.
     DMKSP interface     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 1.19209e-07
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --with-fortran-bindings=0 --download-netcdf --download-parmetis --download-hdf5 --download-chaco --download-scalapack --with-scalar-type=complex --download-pnetcdf --download-exodusii --with-zlib --download-mumps --download-metis
-----------------------------------------
Libraries compiled on 2018-11-05 11:29:19 on wycliffe 
Machine characteristics: Linux-4.15.0-36-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/openmpi/lib -L/usr/lib/openmpi/lib -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Wed Jan 16 09:28:10 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500 Wed Jan 16 09:28:19 2019
tmp.py on a , 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

tmp.py on a  named wycliffe with 1 processor, by owen Wed Jan 16 09:28:42 2019
Using Petsc Development GIT revision: v3.4.2-23224-g603314f  GIT Date: 2018-10-14 16:43:13 -0500

                         Max       Max/Min     Avg       Total 
Time (sec):           2.295e+01     1.000   2.295e+01
Objects:              4.640e+02     1.000   4.640e+02
Flop:                 4.209e+09     1.000   4.209e+09  4.209e+09
Flop/sec:             1.834e+08     1.000   1.834e+08  1.834e+08
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 6.8260e+00  29.7%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 
 1: HelmholtzProblem solve: 1.6123e+01  70.3%  4.2095e+09 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSidedF         1 1.0 2.0790e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             1 1.0 2.2197e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                24 1.0 5.3883e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      41 1.0 2.6417e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd        41 1.0 7.9149e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   1  0  0  0  0     0
DMPlexInterp           1 1.0 1.4040e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   2  0  0  0  0     0
DMPlexStratify         2 1.0 6.8318e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   1  0  0  0  0     0
CreateMesh             4 1.0 1.6913e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   2  0  0  0  0     0
CreateFunctionSpace       1 1.0 4.0898e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   6  0  0  0  0     0
Mesh: reorder          1 1.0 2.4666e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        1 1.0 8.6711e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   1  0  0  0  0     0
CreateSparsity         1 1.0 4.1795e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   1  0  0  0  0     0
MatZeroInitial        40 1.0 1.3244e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  6  0  0  0  0  19  0  0  0  0     0

--- Event Stage 1: HelmholtzProblem solve

VecMDot               20 1.0 5.3813e-03 1.0 1.63e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   303
VecNorm               40 1.0 1.1914e-02 1.0 3.26e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   274
VecScale              40 1.0 2.6064e-03 1.0 1.63e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   626
VecCopy              140 1.0 3.4571e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               160 1.0 3.2556e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               40 1.0 2.8300e-03 1.0 3.26e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1153
VecMAXPY              40 1.0 1.2136e-02 1.0 3.26e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   269
VecNormalize          40 1.0 1.4602e-02 1.0 4.90e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   335
MatMult               20 1.0 4.1953e-02 1.0 1.05e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   250
MatSolve              40 1.0 7.3014e-01 1.0 1.85e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  4  0  0  0   5  4  0  0  0   253
MatLUFactorSym        20 1.0 6.7369e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   4  0  0  0  0     0
MatLUFactorNum        20 1.0 1.2654e+01 1.0 2.98e+09 1.0 0.0e+00 0.0e+00 0.0e+00 55 71  0  0  0  78 71  0  0  0   236
MatAssemblyBegin      80 1.0 3.0041e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd        80 1.0 5.8740e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ           20 1.0 3.7239e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering        20 1.0 5.8799e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   4  0  0  0  0     0
MatZeroEntries        40 1.0 5.1355e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp               20 1.0 1.3918e+01 1.0 2.98e+09 1.0 0.0e+00 0.0e+00 0.0e+00 61 71  0  0  0  86 71  0  0  0   214
PCApply               40 1.0 7.3025e-01 1.0 1.85e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  4  0  0  0   5  4  0  0  0   253
KSPSetUp              20 1.0 4.2574e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              20 1.0 1.4731e+01 1.0 3.19e+09 1.0 0.0e+00 0.0e+00 0.0e+00 64 76  0  0  0  91 76  0  0  0   217
KSPGMRESOrthog        20 1.0 1.1725e-02 1.0 3.26e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   278
SNESSolve             20 1.0 1.6113e+01 1.0 4.21e+09 1.0 0.0e+00 0.0e+00 0.0e+00 70100  0  0  0 100100  0  0  0   261
SNESFunctionEval      20 1.0 1.4596e-01 1.0 2.30e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  5  0  0  0   1  5  0  0  0  1576
SNESJacobianEval      20 1.0 1.2336e+00 1.0 7.88e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 19  0  0  0   8 19  0  0  0   638
ParLoopExecute       140 1.0 1.2040e+00 1.0 1.02e+09 1.0 0.0e+00 0.0e+00 0.0e+00  5 24  0  0  0   7 24  0  0  0   845
ParLoopset_1          40 1.0 3.8700e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednBegin     140 1.0 1.5442e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd       140 1.0 8.2636e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells         120 1.0 1.0639e+00 1.0 1.01e+09 1.0 0.0e+00 0.0e+00 0.0e+00  5 24  0  0  0   7 24  0  0  0   947
ParLoopExtFacets     120 1.0 3.3726e-02 1.0 9.70e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   287
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
           Index Set    37             97      3735460     0.
   IS L to G Mapping     1              1        41484     0.
             Section    17             17        12376     0.
   Star Forest Graph    18             18        14816     0.
              Vector    44            146     21117072     0.
              Matrix    43             63    297902632     0.
      Preconditioner    20             20        20160     0.
       Krylov Solver    20             20       700160     0.
     DMKSP interface     0              1          664     0.
                SNES    20             20        27680     0.
              DMSNES     1              1          680     0.
      SNESLineSearch    20             20        20000     0.
    Distributed Mesh     9              9        44160     0.
    GraphPartitioner     2              2         1240     0.
     Discrete System     9              9         8428     0.

--- Event Stage 1: HelmholtzProblem solve

           Index Set    60              0            0     0.
              Vector   121             19      3130440     0.
              Matrix    20              0            0     0.
     DMKSP interface     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 1.90735e-07
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --prefix=/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc PETSC_ARCH=arch-python-linux-x86_64 --with-shared-libraries=1 --with-debugging=0 --with-c2html=0 --with-cc=mpicc --with-cxx=mpicxx --with-fc=mpif90 --with-fortran-bindings=0 --download-netcdf --download-parmetis --download-hdf5 --download-chaco --download-scalapack --with-scalar-type=complex --download-pnetcdf --download-exodusii --with-zlib --download-mumps --download-metis
-----------------------------------------
Libraries compiled on 2018-11-05 11:29:19 on wycliffe 
Machine characteristics: Linux-4.15.0-36-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -lpetsc -Wl,-rpath,/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -L/home/owen/progs/firedrake-complex/firedrake/lib/python3.5/site-packages/petsc/lib -Wl,-rpath,/usr/lib/openmpi/lib -L/usr/lib/openmpi/lib -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lm -lz -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

---------------------------------------------------------
Finished at Wed Jan 16 09:28:42 2019
---------------------------------------------------------
