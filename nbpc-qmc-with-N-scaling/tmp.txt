---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23845-gd566698  GIT Date: 2018-12-05 06:21:01 +0100 Mon Jun  3 11:42:14 2019
run-nbpc-qmc.py on a arch-pyt, 1 proc. with options:
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23845-gd566698  GIT Date: 2018-12-05 06:21:01 +0100 Mon Jun  3 11:44:21 2019
run-nbpc-qmc.py on a arch-pyt, 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------



      ##########################################################
      #                                                        #
      #                       WARNING!!!                       #
      #                                                        #
      #   This code was compiled with a debugging option.      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


run-nbpc-qmc.py on a arch-python-linux-x86_64 named itd-ngpu-01 with 1 processor, by orp20 Mon Jun  3 11:45:10 2019
Using Petsc Development GIT revision: v3.4.2-23845-gd566698  GIT Date: 2018-12-05 06:21:01 +0100

                         Max       Max/Min     Avg       Total 
Time (sec):           4.880e+01     1.000   4.880e+01
Objects:              1.000e+00     1.000   1.000e+00
Flop:                 0.000e+00     0.000   0.000e+00  0.000e+00
Flop/sec:             0.000e+00     0.000   0.000e+00  0.000e+00
Memory:               1.352e+05     1.000   1.352e+05  1.352e+05
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 4.8801e+01 100.0%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------


      ##########################################################
      #                                                        #
      #                       WARNING!!!                       #
      #                                                        #
      #   This code was compiled with a debugging option.      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 1.19209e-07
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --with-fortran-bindings=0 --download-hdf5 --with-scalar-type=complex --download-exodusii --with-zlib-dir=/apps/zlib/1.2.8 --download-metis --download-netcdf --download-parmetis --download-chaco --download-pnetcdf --download-mumps --download-scalapack --with-cc=mpiicc --with-cxx=mpiicpc --with-fc=mpiifort --with-shared-libraries=1 --download-fblaslapack=1
-----------------------------------------
Libraries compiled on 2018-12-18 17:00:46 on itd-ngpu-02 
Machine characteristics: Linux-2.6.32-696.18.7.el6.x86_64-x86_64-with-redhat-6.9-Carbon
Using PETSc directory: /beegfs/scratch/user/s/orp20/firedrake-complex/petsc
Using PETSc arch: arch-python-linux-x86_64
-----------------------------------------

Using C compiler: mpiicc  -fPIC  -wd1572 -g  
Using Fortran compiler: mpiifort  -fPIC -g    
-----------------------------------------

Using include paths: -I/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/include -I/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/arch-python-linux-x86_64/include -I/apps/zlib/1.2.8/include
-----------------------------------------

Using C linker: mpiicc
Using Fortran linker: mpiifort
Using libraries: -Wl,-rpath,/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/arch-python-linux-x86_64/lib -L/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/arch-python-linux-x86_64/lib -lpetsc -Wl,-rpath,/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/arch-python-linux-x86_64/lib -L/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/arch-python-linux-x86_64/lib -Wl,-rpath,/apps/zlib/1.2.8/lib -L/apps/zlib/1.2.8/lib -Wl,-rpath,/apps/intel/parallel_studio_xe_2018/impi/2018.0.128/intel64/lib/debug_mt -L/apps/intel/parallel_studio_xe_2018/impi/2018.0.128/intel64/lib/debug_mt -Wl,-rpath,/apps/intel/parallel_studio_xe_2018/impi/2018.0.128/intel64/lib -L/apps/intel/parallel_studio_xe_2018/impi/2018.0.128/intel64/lib -Wl,-rpath,/apps/intel/parallel_studio_2015/composer_xe_2015.0.090/mkl/lib/intel64 -L/apps/intel/parallel_studio_2015/composer_xe_2015.0.090/mkl/lib/intel64 -Wl,-rpath,/apps/intel/parallel_studio_2015/composer_xe_2015.0.090/compiler/lib/intel64 -L/apps/intel/parallel_studio_2015/composer_xe_2015.0.090/compiler/lib/intel64 -Wl,-rpath,/cm/shared/apps/slurm/17.11.7/lib64/slurm -L/cm/shared/apps/slurm/17.11.7/lib64/slurm -Wl,-rpath,/cm/shared/apps/slurm/17.11.7/lib64 -L/cm/shared/apps/slurm/17.11.7/lib64 -Wl,-rpath,/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/compiler/lib/intel64 -L/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/compiler/lib/intel64 -Wl,-rpath,/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/ipp/lib/intel64 -L/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/ipp/lib/intel64 -Wl,-rpath,/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/tbb/lib/intel64/gcc4.4 -L/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/tbb/lib/intel64/gcc4.4 -Wl,-rpath,/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/tbb/lib/intel64/gcc4.4 -L/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/tbb/lib/intel64/gcc4.4 -Wl,-rpath,/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/compiler/lib/intel64 -L/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/compiler/lib/intel64 -Wl,-rpath,/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/mkl/lib/intel64 -L/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/mkl/lib/intel64 -Wl,-rpath,/usr/lib/gcc/x86_64-redhat-linux/4.4.7 -L/usr/lib/gcc/x86_64-redhat-linux/4.4.7 -Wl,-rpath,/opt/intel/mpi-rt/2017.0.0/intel64/lib/debug_mt -Wl,-rpath,/opt/intel/mpi-rt/2017.0.0/intel64/lib -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lflapack -lfblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lz -lX11 -lstdc++ -ldl -lmpifort -lmpi -lmpigi -lrt -lpthread -lifport -lifcore -limf -lsvml -lm -lipgo -lirc -lgcc_s -lirc_s -lstdc++ -ldl
-----------------------------------------



      ##########################################################
      #                                                        #
      #                       WARNING!!!                       #
      #                                                        #
      #   This code was compiled with a debugging option.      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


---------------------------------------------------------
Finished at Mon Jun  3 11:45:10 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23845-gd566698  GIT Date: 2018-12-05 06:21:01 +0100 Mon Jun  3 11:47:07 2019
run-nbpc-qmc.py on a arch-pyt, 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------



      ##########################################################
      #                                                        #
      #                       WARNING!!!                       #
      #                                                        #
      #   This code was compiled with a debugging option.      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


run-nbpc-qmc.py on a arch-python-linux-x86_64 named itd-ngpu-01 with 1 processor, by orp20 Mon Jun  3 11:47:13 2019
Using Petsc Development GIT revision: v3.4.2-23845-gd566698  GIT Date: 2018-12-05 06:21:01 +0100

                         Max       Max/Min     Avg       Total 
Time (sec):           6.195e+00     1.000   6.195e+00
Objects:              1.000e+00     1.000   1.000e+00
Flop:                 0.000e+00     0.000   0.000e+00  0.000e+00
Flop/sec:             0.000e+00     0.000   0.000e+00  0.000e+00
Memory:               1.352e+05     1.000   1.352e+05  1.352e+05
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 6.1946e+00 100.0%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------


      ##########################################################
      #                                                        #
      #                       WARNING!!!                       #
      #                                                        #
      #   This code was compiled with a debugging option.      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --with-fortran-bindings=0 --download-hdf5 --with-scalar-type=complex --download-exodusii --with-zlib-dir=/apps/zlib/1.2.8 --download-metis --download-netcdf --download-parmetis --download-chaco --download-pnetcdf --download-mumps --download-scalapack --with-cc=mpiicc --with-cxx=mpiicpc --with-fc=mpiifort --with-shared-libraries=1 --download-fblaslapack=1
-----------------------------------------
Libraries compiled on 2018-12-18 17:00:46 on itd-ngpu-02 
Machine characteristics: Linux-2.6.32-696.18.7.el6.x86_64-x86_64-with-redhat-6.9-Carbon
Using PETSc directory: /beegfs/scratch/user/s/orp20/firedrake-complex/petsc
Using PETSc arch: arch-python-linux-x86_64
-----------------------------------------

Using C compiler: mpiicc  -fPIC  -wd1572 -g  
Using Fortran compiler: mpiifort  -fPIC -g    
-----------------------------------------

Using include paths: -I/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/include -I/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/arch-python-linux-x86_64/include -I/apps/zlib/1.2.8/include
-----------------------------------------

Using C linker: mpiicc
Using Fortran linker: mpiifort
Using libraries: -Wl,-rpath,/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/arch-python-linux-x86_64/lib -L/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/arch-python-linux-x86_64/lib -lpetsc -Wl,-rpath,/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/arch-python-linux-x86_64/lib -L/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/arch-python-linux-x86_64/lib -Wl,-rpath,/apps/zlib/1.2.8/lib -L/apps/zlib/1.2.8/lib -Wl,-rpath,/apps/intel/parallel_studio_xe_2018/impi/2018.0.128/intel64/lib/debug_mt -L/apps/intel/parallel_studio_xe_2018/impi/2018.0.128/intel64/lib/debug_mt -Wl,-rpath,/apps/intel/parallel_studio_xe_2018/impi/2018.0.128/intel64/lib -L/apps/intel/parallel_studio_xe_2018/impi/2018.0.128/intel64/lib -Wl,-rpath,/apps/intel/parallel_studio_2015/composer_xe_2015.0.090/mkl/lib/intel64 -L/apps/intel/parallel_studio_2015/composer_xe_2015.0.090/mkl/lib/intel64 -Wl,-rpath,/apps/intel/parallel_studio_2015/composer_xe_2015.0.090/compiler/lib/intel64 -L/apps/intel/parallel_studio_2015/composer_xe_2015.0.090/compiler/lib/intel64 -Wl,-rpath,/cm/shared/apps/slurm/17.11.7/lib64/slurm -L/cm/shared/apps/slurm/17.11.7/lib64/slurm -Wl,-rpath,/cm/shared/apps/slurm/17.11.7/lib64 -L/cm/shared/apps/slurm/17.11.7/lib64 -Wl,-rpath,/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/compiler/lib/intel64 -L/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/compiler/lib/intel64 -Wl,-rpath,/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/ipp/lib/intel64 -L/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/ipp/lib/intel64 -Wl,-rpath,/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/tbb/lib/intel64/gcc4.4 -L/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/tbb/lib/intel64/gcc4.4 -Wl,-rpath,/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/tbb/lib/intel64/gcc4.4 -L/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/tbb/lib/intel64/gcc4.4 -Wl,-rpath,/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/compiler/lib/intel64 -L/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/compiler/lib/intel64 -Wl,-rpath,/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/mkl/lib/intel64 -L/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/mkl/lib/intel64 -Wl,-rpath,/usr/lib/gcc/x86_64-redhat-linux/4.4.7 -L/usr/lib/gcc/x86_64-redhat-linux/4.4.7 -Wl,-rpath,/opt/intel/mpi-rt/2017.0.0/intel64/lib/debug_mt -Wl,-rpath,/opt/intel/mpi-rt/2017.0.0/intel64/lib -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lflapack -lfblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lz -lX11 -lstdc++ -ldl -lmpifort -lmpi -lmpigi -lrt -lpthread -lifport -lifcore -limf -lsvml -lm -lipgo -lirc -lgcc_s -lirc_s -lstdc++ -ldl
-----------------------------------------



      ##########################################################
      #                                                        #
      #                       WARNING!!!                       #
      #                                                        #
      #   This code was compiled with a debugging option.      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


---------------------------------------------------------
Finished at Mon Jun  3 11:47:13 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23845-gd566698  GIT Date: 2018-12-05 06:21:01 +0100 Mon Jun  3 11:48:46 2019
run-nbpc-qmc.py on a arch-pyt, 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------



      ##########################################################
      #                                                        #
      #                       WARNING!!!                       #
      #                                                        #
      #   This code was compiled with a debugging option.      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


run-nbpc-qmc.py on a arch-python-linux-x86_64 named itd-ngpu-01 with 1 processor, by orp20 Mon Jun  3 11:48:53 2019
Using Petsc Development GIT revision: v3.4.2-23845-gd566698  GIT Date: 2018-12-05 06:21:01 +0100

                         Max       Max/Min     Avg       Total 
Time (sec):           7.064e+00     1.000   7.064e+00
Objects:              1.000e+00     1.000   1.000e+00
Flop:                 0.000e+00     0.000   0.000e+00  0.000e+00
Flop/sec:             0.000e+00     0.000   0.000e+00  0.000e+00
Memory:               1.352e+05     1.000   1.352e+05  1.352e+05
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 7.0635e+00 100.0%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------


      ##########################################################
      #                                                        #
      #                       WARNING!!!                       #
      #                                                        #
      #   This code was compiled with a debugging option.      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --with-fortran-bindings=0 --download-hdf5 --with-scalar-type=complex --download-exodusii --with-zlib-dir=/apps/zlib/1.2.8 --download-metis --download-netcdf --download-parmetis --download-chaco --download-pnetcdf --download-mumps --download-scalapack --with-cc=mpiicc --with-cxx=mpiicpc --with-fc=mpiifort --with-shared-libraries=1 --download-fblaslapack=1
-----------------------------------------
Libraries compiled on 2018-12-18 17:00:46 on itd-ngpu-02 
Machine characteristics: Linux-2.6.32-696.18.7.el6.x86_64-x86_64-with-redhat-6.9-Carbon
Using PETSc directory: /beegfs/scratch/user/s/orp20/firedrake-complex/petsc
Using PETSc arch: arch-python-linux-x86_64
-----------------------------------------

Using C compiler: mpiicc  -fPIC  -wd1572 -g  
Using Fortran compiler: mpiifort  -fPIC -g    
-----------------------------------------

Using include paths: -I/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/include -I/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/arch-python-linux-x86_64/include -I/apps/zlib/1.2.8/include
-----------------------------------------

Using C linker: mpiicc
Using Fortran linker: mpiifort
Using libraries: -Wl,-rpath,/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/arch-python-linux-x86_64/lib -L/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/arch-python-linux-x86_64/lib -lpetsc -Wl,-rpath,/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/arch-python-linux-x86_64/lib -L/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/arch-python-linux-x86_64/lib -Wl,-rpath,/apps/zlib/1.2.8/lib -L/apps/zlib/1.2.8/lib -Wl,-rpath,/apps/intel/parallel_studio_xe_2018/impi/2018.0.128/intel64/lib/debug_mt -L/apps/intel/parallel_studio_xe_2018/impi/2018.0.128/intel64/lib/debug_mt -Wl,-rpath,/apps/intel/parallel_studio_xe_2018/impi/2018.0.128/intel64/lib -L/apps/intel/parallel_studio_xe_2018/impi/2018.0.128/intel64/lib -Wl,-rpath,/apps/intel/parallel_studio_2015/composer_xe_2015.0.090/mkl/lib/intel64 -L/apps/intel/parallel_studio_2015/composer_xe_2015.0.090/mkl/lib/intel64 -Wl,-rpath,/apps/intel/parallel_studio_2015/composer_xe_2015.0.090/compiler/lib/intel64 -L/apps/intel/parallel_studio_2015/composer_xe_2015.0.090/compiler/lib/intel64 -Wl,-rpath,/cm/shared/apps/slurm/17.11.7/lib64/slurm -L/cm/shared/apps/slurm/17.11.7/lib64/slurm -Wl,-rpath,/cm/shared/apps/slurm/17.11.7/lib64 -L/cm/shared/apps/slurm/17.11.7/lib64 -Wl,-rpath,/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/compiler/lib/intel64 -L/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/compiler/lib/intel64 -Wl,-rpath,/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/ipp/lib/intel64 -L/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/ipp/lib/intel64 -Wl,-rpath,/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/tbb/lib/intel64/gcc4.4 -L/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/tbb/lib/intel64/gcc4.4 -Wl,-rpath,/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/tbb/lib/intel64/gcc4.4 -L/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/tbb/lib/intel64/gcc4.4 -Wl,-rpath,/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/compiler/lib/intel64 -L/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/compiler/lib/intel64 -Wl,-rpath,/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/mkl/lib/intel64 -L/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/mkl/lib/intel64 -Wl,-rpath,/usr/lib/gcc/x86_64-redhat-linux/4.4.7 -L/usr/lib/gcc/x86_64-redhat-linux/4.4.7 -Wl,-rpath,/opt/intel/mpi-rt/2017.0.0/intel64/lib/debug_mt -Wl,-rpath,/opt/intel/mpi-rt/2017.0.0/intel64/lib -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lflapack -lfblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lz -lX11 -lstdc++ -ldl -lmpifort -lmpi -lmpigi -lrt -lpthread -lifport -lifcore -limf -lsvml -lm -lipgo -lirc -lgcc_s -lirc_s -lstdc++ -ldl
-----------------------------------------



      ##########################################################
      #                                                        #
      #                       WARNING!!!                       #
      #                                                        #
      #   This code was compiled with a debugging option.      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


---------------------------------------------------------
Finished at Mon Jun  3 11:48:53 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23845-gd566698  GIT Date: 2018-12-05 06:21:01 +0100 Mon Jun  3 11:52:30 2019
run-nbpc-qmc.py on a arch-pyt, 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------



      ##########################################################
      #                                                        #
      #                       WARNING!!!                       #
      #                                                        #
      #   This code was compiled with a debugging option.      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


run-nbpc-qmc.py on a arch-python-linux-x86_64 named itd-ngpu-01 with 1 processor, by orp20 Mon Jun  3 11:52:59 2019
Using Petsc Development GIT revision: v3.4.2-23845-gd566698  GIT Date: 2018-12-05 06:21:01 +0100

                         Max       Max/Min     Avg       Total 
Time (sec):           2.898e+01     1.000   2.898e+01
Objects:              1.220e+02     1.000   1.220e+02
Flop:                 1.669e+08     1.000   1.669e+08  1.669e+08
Flop/sec:             5.761e+06     1.000   5.761e+06  5.761e+06
Memory:               4.062e+05     1.000   4.062e+05  4.062e+05
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 2.8975e+01 100.0%  1.6693e+08 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------


      ##########################################################
      #                                                        #
      #                       WARNING!!!                       #
      #                                                        #
      #   This code was compiled with a debugging option.      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSidedF         1 1.0 1.1802e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             1 1.0 7.8678e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              152 1.0 7.4315e-04 1.0 2.92e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   393
VecNorm              184 1.0 5.5337e-04 1.0 1.18e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   214
VecScale             184 1.0 4.1271e-02 1.0 5.96e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     1
VecCopy              224 1.0 7.6699e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                85 1.0 2.5415e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               64 1.0 2.5955e-02 1.0 4.15e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     2
VecMAXPY             184 1.0 8.0562e-04 1.0 3.92e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   487
VecNormalize         184 1.0 4.2415e-02 1.0 1.78e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     4
MatMult              152 1.0 1.2302e-03 1.0 5.55e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   451
MatSolve             184 1.0 3.7246e-03 1.0 1.79e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0   482
MatLUFactorSym         1 1.0 6.0549e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum         1 1.0 1.5923e-02 1.0 4.30e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     3
MatAssemblyBegin     131 1.0 5.6481e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd       131 1.0 2.7461e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            1 1.0 7.8669e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         1 1.0 1.0283e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        64 1.0 1.9598e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp                1 1.0 3.5436e-02 1.0 4.30e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     1
PCApply              184 1.0 4.2799e-03 1.0 1.79e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0   419
KSPSetUp              32 1.0 3.8326e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              32 1.0 1.7176e-01 1.0 3.27e+06 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0    19
KSPGMRESOrthog       152 1.0 3.0890e-03 1.0 5.85e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   189
SNESSolve             32 1.0 8.1444e+00 1.0 1.67e+08 1.0 0.0e+00 0.0e+00 0.0e+00 28100  0  0  0  28100  0  0  0    20
SNESFunctionEval      32 1.0 5.6205e+00 1.0 4.68e+07 1.0 0.0e+00 0.0e+00 0.0e+00 19 28  0  0  0  19 28  0  0  0     8
SNESJacobianEval      32 1.0 2.3384e+00 1.0 1.17e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8 70  0  0  0   8 70  0  0  0    50
DMPlexInterp           1 1.0 1.9723e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
DMPlexStratify         2 1.0 3.8288e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
CreateMesh             4 1.0 1.1520e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateFunctionSpace       1 1.0 3.9687e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
Mesh: reorder          1 1.0 2.2591e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        1 1.0 3.4643e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateSparsity         1 1.0 3.8015e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroInitial         2 1.0 1.2379e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopExecute       224 1.0 7.8973e+00 1.0 1.64e+08 1.0 0.0e+00 0.0e+00 0.0e+00 27 98  0  0  0  27 98  0  0  0    21
ParLoopset_1          64 1.0 4.7429e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednBegin     224 1.0 1.1754e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd       224 1.0 1.0905e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells         192 1.0 8.7568e-01 1.0 1.62e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 97  0  0  0   3 97  0  0  0   185
ParLoopExtFacets     192 1.0 2.2486e-02 1.0 1.24e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0    55
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
           Index Set    40             40        41120     0.
   IS L to G Mapping     1              1          996     0.
             Section    15             15        10800     0.
   Star Forest Graph    14             14        11328     0.
              Vector    23             23        66432     0.
              Matrix     6              6        66452     0.
      Preconditioner     1              1         1000     0.
       Krylov Solver     1              1        35000     0.
     DMKSP interface     1              1          656     0.
                SNES     1              1         1380     0.
              DMSNES     1              1          672     0.
      SNESLineSearch     1              1          992     0.
    Distributed Mesh     7              7        34536     0.
    GraphPartitioner     2              2         1224     0.
     Discrete System     7              7         6500     0.
========================================================================================================================
Average time to get PetscTime(): 1.19209e-07
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --with-fortran-bindings=0 --download-hdf5 --with-scalar-type=complex --download-exodusii --with-zlib-dir=/apps/zlib/1.2.8 --download-metis --download-netcdf --download-parmetis --download-chaco --download-pnetcdf --download-mumps --download-scalapack --with-cc=mpiicc --with-cxx=mpiicpc --with-fc=mpiifort --with-shared-libraries=1 --download-fblaslapack=1
-----------------------------------------
Libraries compiled on 2018-12-18 17:00:46 on itd-ngpu-02 
Machine characteristics: Linux-2.6.32-696.18.7.el6.x86_64-x86_64-with-redhat-6.9-Carbon
Using PETSc directory: /beegfs/scratch/user/s/orp20/firedrake-complex/petsc
Using PETSc arch: arch-python-linux-x86_64
-----------------------------------------

Using C compiler: mpiicc  -fPIC  -wd1572 -g  
Using Fortran compiler: mpiifort  -fPIC -g    
-----------------------------------------

Using include paths: -I/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/include -I/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/arch-python-linux-x86_64/include -I/apps/zlib/1.2.8/include
-----------------------------------------

Using C linker: mpiicc
Using Fortran linker: mpiifort
Using libraries: -Wl,-rpath,/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/arch-python-linux-x86_64/lib -L/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/arch-python-linux-x86_64/lib -lpetsc -Wl,-rpath,/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/arch-python-linux-x86_64/lib -L/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/arch-python-linux-x86_64/lib -Wl,-rpath,/apps/zlib/1.2.8/lib -L/apps/zlib/1.2.8/lib -Wl,-rpath,/apps/intel/parallel_studio_xe_2018/impi/2018.0.128/intel64/lib/debug_mt -L/apps/intel/parallel_studio_xe_2018/impi/2018.0.128/intel64/lib/debug_mt -Wl,-rpath,/apps/intel/parallel_studio_xe_2018/impi/2018.0.128/intel64/lib -L/apps/intel/parallel_studio_xe_2018/impi/2018.0.128/intel64/lib -Wl,-rpath,/apps/intel/parallel_studio_2015/composer_xe_2015.0.090/mkl/lib/intel64 -L/apps/intel/parallel_studio_2015/composer_xe_2015.0.090/mkl/lib/intel64 -Wl,-rpath,/apps/intel/parallel_studio_2015/composer_xe_2015.0.090/compiler/lib/intel64 -L/apps/intel/parallel_studio_2015/composer_xe_2015.0.090/compiler/lib/intel64 -Wl,-rpath,/cm/shared/apps/slurm/17.11.7/lib64/slurm -L/cm/shared/apps/slurm/17.11.7/lib64/slurm -Wl,-rpath,/cm/shared/apps/slurm/17.11.7/lib64 -L/cm/shared/apps/slurm/17.11.7/lib64 -Wl,-rpath,/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/compiler/lib/intel64 -L/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/compiler/lib/intel64 -Wl,-rpath,/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/ipp/lib/intel64 -L/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/ipp/lib/intel64 -Wl,-rpath,/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/tbb/lib/intel64/gcc4.4 -L/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/tbb/lib/intel64/gcc4.4 -Wl,-rpath,/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/tbb/lib/intel64/gcc4.4 -L/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/tbb/lib/intel64/gcc4.4 -Wl,-rpath,/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/compiler/lib/intel64 -L/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/compiler/lib/intel64 -Wl,-rpath,/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/mkl/lib/intel64 -L/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/mkl/lib/intel64 -Wl,-rpath,/usr/lib/gcc/x86_64-redhat-linux/4.4.7 -L/usr/lib/gcc/x86_64-redhat-linux/4.4.7 -Wl,-rpath,/opt/intel/mpi-rt/2017.0.0/intel64/lib/debug_mt -Wl,-rpath,/opt/intel/mpi-rt/2017.0.0/intel64/lib -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lflapack -lfblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lz -lX11 -lstdc++ -ldl -lmpifort -lmpi -lmpigi -lrt -lpthread -lifport -lifcore -limf -lsvml -lm -lipgo -lirc -lgcc_s -lirc_s -lstdc++ -ldl
-----------------------------------------



      ##########################################################
      #                                                        #
      #                       WARNING!!!                       #
      #                                                        #
      #   This code was compiled with a debugging option.      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


---------------------------------------------------------
Finished at Mon Jun  3 11:52:59 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23845-gd566698  GIT Date: 2018-12-05 06:21:01 +0100 Mon Jun  3 11:57:55 2019
run-nbpc-qmc.py on a arch-pyt, 1 proc. with options:
---------------------------------------------------------
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------



      ##########################################################
      #                                                        #
      #                       WARNING!!!                       #
      #                                                        #
      #   This code was compiled with a debugging option.      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


run-nbpc-qmc.py on a arch-python-linux-x86_64 named itd-ngpu-01 with 1 processor, by orp20 Mon Jun  3 11:58:04 2019
Using Petsc Development GIT revision: v3.4.2-23845-gd566698  GIT Date: 2018-12-05 06:21:01 +0100

                         Max       Max/Min     Avg       Total 
Time (sec):           9.248e+00     1.000   9.248e+00
Objects:              1.220e+02     1.000   1.220e+02
Flop:                 1.669e+08     1.000   1.669e+08  1.669e+08
Flop/sec:             1.805e+07     1.000   1.805e+07  1.805e+07
Memory:               4.062e+05     1.000   4.062e+05  4.062e+05
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 9.2480e+00 100.0%  1.6693e+08 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------


      ##########################################################
      #                                                        #
      #                       WARNING!!!                       #
      #                                                        #
      #   This code was compiled with a debugging option.      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSidedF         1 1.0 1.2493e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             1 1.0 6.9141e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              152 1.0 7.3624e-04 1.0 2.92e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   396
VecNorm              184 1.0 5.4383e-04 1.0 1.18e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   218
VecScale             184 1.0 1.2974e-02 1.0 5.96e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     5
VecCopy              224 1.0 7.1764e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                85 1.0 2.2960e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               64 1.0 5.1193e-03 1.0 4.15e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     8
VecMAXPY             184 1.0 7.7057e-04 1.0 3.92e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   509
VecNormalize         184 1.0 1.4102e-02 1.0 1.78e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0    13
MatMult              152 1.0 1.2414e-03 1.0 5.55e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   447
MatSolve             184 1.0 3.6180e-03 1.0 1.79e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0   496
MatLUFactorSym         1 1.0 8.7094e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum         1 1.0 3.9196e-04 1.0 4.30e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   110
MatAssemblyBegin     131 1.0 4.9448e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd       131 1.0 1.2536e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            1 1.0 4.9019e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         1 1.0 8.6284e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        64 1.0 1.9670e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCSetUp                1 1.0 3.3469e-03 1.0 4.30e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0    13
PCApply              184 1.0 4.1268e-03 1.0 1.79e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0   435
KSPSetUp              32 1.0 4.9829e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              32 1.0 4.7568e-02 1.0 3.27e+06 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0    69
KSPGMRESOrthog       152 1.0 2.7974e-03 1.0 5.85e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   209
SNESSolve             32 1.0 1.2536e+00 1.0 1.67e+08 1.0 0.0e+00 0.0e+00 0.0e+00 14100  0  0  0  14100  0  0  0   133
SNESFunctionEval      32 1.0 5.7826e-01 1.0 4.68e+07 1.0 0.0e+00 0.0e+00 0.0e+00  6 28  0  0  0   6 28  0  0  0    81
SNESJacobianEval      32 1.0 6.2122e-01 1.0 1.17e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7 70  0  0  0   7 70  0  0  0   188
DMPlexInterp           1 1.0 2.9812e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexStratify         2 1.0 7.9253e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateMesh             4 1.0 1.3819e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateFunctionSpace       1 1.0 1.7284e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0
Mesh: reorder          1 1.0 1.5500e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        1 1.0 5.0640e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateSparsity         1 1.0 5.2290e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroInitial         2 1.0 1.3261e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopExecute       224 1.0 1.1523e+00 1.0 1.64e+08 1.0 0.0e+00 0.0e+00 0.0e+00 12 98  0  0  0  12 98  0  0  0   142
ParLoopset_1          64 1.0 4.6601e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednBegin     224 1.0 1.0493e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopRednEnd       224 1.0 1.0166e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopCells         192 1.0 8.4892e-01 1.0 1.62e+08 1.0 0.0e+00 0.0e+00 0.0e+00  9 97  0  0  0   9 97  0  0  0   191
ParLoopExtFacets     192 1.0 1.7360e-02 1.0 1.24e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0    71
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Viewer     1              0            0     0.
           Index Set    40             40        41120     0.
   IS L to G Mapping     1              1          996     0.
             Section    15             15        10800     0.
   Star Forest Graph    14             14        11328     0.
              Vector    23             23        66432     0.
              Matrix     6              6        66452     0.
      Preconditioner     1              1         1000     0.
       Krylov Solver     1              1        35000     0.
     DMKSP interface     1              1          656     0.
                SNES     1              1         1380     0.
              DMSNES     1              1          672     0.
      SNESLineSearch     1              1          992     0.
    Distributed Mesh     7              7        34536     0.
    GraphPartitioner     2              2         1224     0.
     Discrete System     7              7         6500     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
#PETSc Option Table entries:
-history tmp.txt
-log_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 16 sizeof(PetscInt) 4
Configure options: --with-fortran-bindings=0 --download-hdf5 --with-scalar-type=complex --download-exodusii --with-zlib-dir=/apps/zlib/1.2.8 --download-metis --download-netcdf --download-parmetis --download-chaco --download-pnetcdf --download-mumps --download-scalapack --with-cc=mpiicc --with-cxx=mpiicpc --with-fc=mpiifort --with-shared-libraries=1 --download-fblaslapack=1
-----------------------------------------
Libraries compiled on 2018-12-18 17:00:46 on itd-ngpu-02 
Machine characteristics: Linux-2.6.32-696.18.7.el6.x86_64-x86_64-with-redhat-6.9-Carbon
Using PETSc directory: /beegfs/scratch/user/s/orp20/firedrake-complex/petsc
Using PETSc arch: arch-python-linux-x86_64
-----------------------------------------

Using C compiler: mpiicc  -fPIC  -wd1572 -g  
Using Fortran compiler: mpiifort  -fPIC -g    
-----------------------------------------

Using include paths: -I/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/include -I/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/arch-python-linux-x86_64/include -I/apps/zlib/1.2.8/include
-----------------------------------------

Using C linker: mpiicc
Using Fortran linker: mpiifort
Using libraries: -Wl,-rpath,/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/arch-python-linux-x86_64/lib -L/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/arch-python-linux-x86_64/lib -lpetsc -Wl,-rpath,/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/arch-python-linux-x86_64/lib -L/beegfs/scratch/user/s/orp20/firedrake-complex/petsc/arch-python-linux-x86_64/lib -Wl,-rpath,/apps/zlib/1.2.8/lib -L/apps/zlib/1.2.8/lib -Wl,-rpath,/apps/intel/parallel_studio_xe_2018/impi/2018.0.128/intel64/lib/debug_mt -L/apps/intel/parallel_studio_xe_2018/impi/2018.0.128/intel64/lib/debug_mt -Wl,-rpath,/apps/intel/parallel_studio_xe_2018/impi/2018.0.128/intel64/lib -L/apps/intel/parallel_studio_xe_2018/impi/2018.0.128/intel64/lib -Wl,-rpath,/apps/intel/parallel_studio_2015/composer_xe_2015.0.090/mkl/lib/intel64 -L/apps/intel/parallel_studio_2015/composer_xe_2015.0.090/mkl/lib/intel64 -Wl,-rpath,/apps/intel/parallel_studio_2015/composer_xe_2015.0.090/compiler/lib/intel64 -L/apps/intel/parallel_studio_2015/composer_xe_2015.0.090/compiler/lib/intel64 -Wl,-rpath,/cm/shared/apps/slurm/17.11.7/lib64/slurm -L/cm/shared/apps/slurm/17.11.7/lib64/slurm -Wl,-rpath,/cm/shared/apps/slurm/17.11.7/lib64 -L/cm/shared/apps/slurm/17.11.7/lib64 -Wl,-rpath,/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/compiler/lib/intel64 -L/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/compiler/lib/intel64 -Wl,-rpath,/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/ipp/lib/intel64 -L/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/ipp/lib/intel64 -Wl,-rpath,/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/tbb/lib/intel64/gcc4.4 -L/apps/intel/parallel_studio_xe_2018/compilers_and_libraries_2018.0.128/linux/tbb/lib/intel64/gcc4.4 -Wl,-rpath,/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/tbb/lib/intel64/gcc4.4 -L/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/tbb/lib/intel64/gcc4.4 -Wl,-rpath,/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/compiler/lib/intel64 -L/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/compiler/lib/intel64 -Wl,-rpath,/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/mkl/lib/intel64 -L/apps/intel/parallel_studio_2016/compilers_and_libraries_2016.3.210/linux/mkl/lib/intel64 -Wl,-rpath,/usr/lib/gcc/x86_64-redhat-linux/4.4.7 -L/usr/lib/gcc/x86_64-redhat-linux/4.4.7 -Wl,-rpath,/opt/intel/mpi-rt/2017.0.0/intel64/lib/debug_mt -Wl,-rpath,/opt/intel/mpi-rt/2017.0.0/intel64/lib -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lflapack -lfblas -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lparmetis -lmetis -lz -lX11 -lstdc++ -ldl -lmpifort -lmpi -lmpigi -lrt -lpthread -lifport -lifcore -limf -lsvml -lm -lipgo -lirc -lgcc_s -lirc_s -lstdc++ -ldl
-----------------------------------------



      ##########################################################
      #                                                        #
      #                       WARNING!!!                       #
      #                                                        #
      #   This code was compiled with a debugging option.      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


---------------------------------------------------------
Finished at Mon Jun  3 11:58:04 2019
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23845-gd566698  GIT Date: 2018-12-05 06:21:01 +0100 Mon Jun  3 12:00:19 2019
run-nbpc-qmc.py on a arch-pyt, 1 proc. with options:
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23845-gd566698  GIT Date: 2018-12-05 06:21:01 +0100 Mon Jun  3 12:02:05 2019
run-nbpc-qmc.py on a arch-pyt, 4 proc. with options:
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23845-gd566698  GIT Date: 2018-12-05 06:21:01 +0100 Mon Jun  3 12:55:05 2019
/beegfs/scratch/user/s/orp20/running-nbpc/nbpc-qmc-with-N-scaling/run-nbpc-qmc.py on a arch-pyt, 4 proc. with options:
---------------------------------------------------------
[0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: ---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23845-gd566698  GIT Date: 2018-12-05 06:21:01 +0100 Mon Jun  3 13:01:28 2019
/beegfs/scratch/user/s/orp20/running-nbpc/nbpc-qmc-with-N-scaling/run-nbpc-qmc.py on a arch-pyt, 1 proc. with options:
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23845-gd566698  GIT Date: 2018-12-05 06:21:01 +0100 Mon Jun  3 13:02:12 2019
run-nbpc-qmc.py on a arch-pyt, 2 proc. with options:
---------------------------------------------------------
---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23845-gd566698  GIT Date: 2018-12-05 06:21:01 +0100 Mon Jun  3 13:04:37 2019
run-nbpc-qmc.py on a arch-pyt, 4 proc. with options:
---------------------------------------------------------
[0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: ---------------------------------------------------------
Petsc Development GIT revision: v3.4.2-23845-gd566698  GIT Date: 2018-12-05 06:21:01 +0100 Thu Jun  6 10:36:27 2019
/beegfs/scratch/user/s/orp20/running-nbpc/nbpc-qmc-with-N-scaling/run-nbpc-qmc.py on a arch-pyt, 1 proc. with options:
---------------------------------------------------------
[0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: [0]PETSC ERROR: 